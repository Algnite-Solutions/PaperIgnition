#!/usr/bin/env python3
"""
åŸºäºæ•°æ®åº“çŠ¶æ€æ‰¹é‡ç”Ÿæˆè®ºæ–‡åšå®¢è„šæœ¬

é€»è¾‘ï¼š
1. è¿æ¥ Metadata æ•°æ®åº“
2. æŸ¥è¯¢ blog å­—æ®µä¸ºç©ºçš„ paper doc_id
3. ä» jsons ç›®å½•åŠ è½½å¯¹åº”çš„è®ºæ–‡æ•°æ®åˆ›å»º DocSet å¯¹è±¡
4. è°ƒç”¨ generate_blog.py çš„ run_batch_generation æ‰¹é‡ç”Ÿæˆåšå®¢
5. ä¿å­˜ç”Ÿæˆçš„åšå®¢åˆ° blogs ç›®å½•
6. è°ƒç”¨ API æ›´æ–°æ•°æ®åº“ä¸­çš„ blog å­—æ®µ

ä½¿ç”¨æ–¹æ³•ï¼š
    python bulk_generate.py
"""

import os
import sys
import asyncio
import asyncpg
from pathlib import Path
from typing import List
import logging
import yaml
import json
from tqdm import tqdm
import httpx

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))
sys.path.insert(0, str(project_root.parent / "AIgnite"))  # å‡è®¾AIgniteè·¯å¾„

# å¯¼å…¥ DocSet å’Œç”Ÿæˆå‡½æ•°
from AIgnite.data.docset import DocSet
from generate_blog import run_batch_generation, load_config as gb_load_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bulk_generate.log')
    ]
)

# ç¦ç”¨ç¬¬ä¸‰æ–¹åº“çš„ DEBUG æ—¥å¿—
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('httpcore').setLevel(logging.WARNING)
logging.getLogger('asyncio').setLevel(logging.WARNING)
logging.getLogger('asyncpg').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


class BulkBlogGenerator:
    def __init__(self, config_path: str = None, batch_size: int = 10):
        # --- é…ç½®åŠ è½½é€»è¾‘ ---
        if config_path:
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    full_config = yaml.safe_load(f)
                
                self.index_config = full_config.get('index_service', {})
                self.blog_config = full_config.get('blog_generation', {})
                
                # è·å–æ•°æ®åº“è¿æ¥ URL
                self.db_url = self.index_config.get('metadata_db', {}).get('db_url')
                if not self.db_url:
                    raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ index_service.metadata_db.db_url")
                
                # API URL
                self.api_host = self.index_config.get('host', "http://localhost:8002")
                self.update_blogs_endpoint = f"{self.api_host.rstrip('/')}/update_papers_blog/"
                
                # JSON æ–‡ä»¶ç›®å½•
                self.json_folder = full_config.get('PAPER_STORAGE', {}).get('json_folder', 
                    "/data3/guofang/peirongcan/PaperIgnition/orchestrator/jsons")
                
                # åšå®¢è¾“å‡ºç›®å½•
                self.output_path = self.blog_config.get('output_path', 
                    "/data3/guofang/peirongcan/PaperIgnition/orchestrator/blogs")
                
                logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                
            except Exception as e:
                logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼")
                self.db_url = "postgresql://postgres:11111@localhost:5432/paperignition"
                self.api_host = "http://localhost:8002"
                self.update_blogs_endpoint = f"{self.api_host.rstrip('/')}/update_papers_blog/"
                self.json_folder = "/data3/guofang/peirongcan/PaperIgnition/orchestrator/jsons"
                self.output_path = "/data3/guofang/peirongcan/PaperIgnition/orchestrator/blogs"
        else:
            logger.error("âŒ å¿…é¡»æä¾›é…ç½®æ–‡ä»¶è·¯å¾„")
            sys.exit(1)
        
        self.batch_size = batch_size
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_target_papers = 0
        self.loaded_papers = 0
        self.missing_json_files = 0
        self.successful_generations = 0
        self.failed_generations = 0
        self.successful_db_updates = 0
        self.failed_db_updates = 0
        
        logger.info(f"ğŸ—„ï¸  DB URL: {self.db_url.split('@')[-1]}")  # éšè—å¯†ç 
        logger.info(f"ğŸ“ JSONç›®å½•: {self.json_folder}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_path}")
        logger.info(f"ğŸ”§ API URL: {self.update_blogs_endpoint}")

    async def fetch_missing_blog_doc_ids(self) -> List[str]:
        """
        è¿æ¥æ•°æ®åº“ï¼ŒæŸ¥è¯¢æ‰€æœ‰ blog å­—æ®µä¸ºç©ºæˆ–NULLçš„ paper doc_id
        """
        logger.info("ğŸ” æ­£åœ¨è¿æ¥æ•°æ®åº“æŸ¥è¯¢ç¼ºå¤±åšå®¢çš„è®ºæ–‡...")
        conn = None
        try:
            conn = await asyncpg.connect(self.db_url)
            
            # æŸ¥è¯¢ blog ä¸º NULL æˆ– ç©ºå­—ç¬¦ä¸² æˆ– åªæœ‰ç©ºç™½å­—ç¬¦ çš„è®°å½•
            query = """
                SELECT doc_id 
                FROM papers 
                WHERE blog IS NULL 
                   OR trim(blog) = ''
            """
            
            rows = await conn.fetch(query)
            doc_ids = [row['doc_id'] for row in rows]
            
            logger.info(f"ğŸ“‹ æ•°æ®åº“ä¸­å…±æœ‰ {len(doc_ids)} ç¯‡è®ºæ–‡ç¼ºå°‘åšå®¢å†…å®¹")
            self.total_target_papers = len(doc_ids)
            return doc_ids
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
        finally:
            if conn:
                await conn.close()

    async def load_papers_from_doc_ids(self, doc_ids: List[str]) -> List[DocSet]:
        """
        æ ¹æ®doc_idåˆ—è¡¨ï¼Œä»JSONç›®å½•åŠ è½½è®ºæ–‡æ•°æ®åˆ›å»ºDocSetå¯¹è±¡
        """
        papers = []
        json_folder_path = Path(self.json_folder)
        
        if not json_folder_path.exists():
            logger.error(f"âŒ JSONç›®å½•ä¸å­˜åœ¨: {json_folder_path}")
            return []
        
        logger.info("ğŸ“‚ å¼€å§‹åŠ è½½è®ºæ–‡æ•°æ®...")
        
        with tqdm(total=len(doc_ids), desc="ğŸ“– åŠ è½½è®ºæ–‡", unit="ç¯‡", ncols=100) as pbar:
            for doc_id in doc_ids:
                json_file = json_folder_path / f"{doc_id}.json"
                
                if json_file.exists():
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        if data:
                            paper = DocSet(**data)
                            papers.append(paper)
                            self.loaded_papers += 1
                        else:
                            logger.warning(f"âš ï¸ JSONæ–‡ä»¶ä¸ºç©º: {json_file}")
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ åŠ è½½JSONå‡ºé”™ {json_file}: {e}")
                        self.missing_json_files += 1
                else:
                    logger.warning(f"âš ï¸ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
                    self.missing_json_files += 1
                
                pbar.update(1)
        
        logger.info(f"âœ… åŠ è½½å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡æ•°æ®")
        if self.missing_json_files > 0:
            logger.warning(f"âš ï¸  æœ‰ {self.missing_json_files} ç¯‡è®ºæ–‡ç¼ºå°‘JSONæ–‡ä»¶")
        
        return papers

    async def run_generation(self, papers: List[DocSet]) -> int:
        """
        è°ƒç”¨run_batch_generationç”Ÿæˆåšå®¢ï¼Œæ¯æ‰¹ç”Ÿæˆåç«‹å³æ›´æ–°æ•°æ®åº“
        åˆ†æ‰¹ç”Ÿæˆä»¥é¿å…å¹¶å‘è¿‡è½½ï¼Œè¿”å›æ€»æˆåŠŸç”Ÿæˆæ•°
        """
        if not papers:
            return 0
        
        total_successful = 0
        total_batches = (len(papers) + self.batch_size - 1) // self.batch_size
        
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æ‰¹ç”Ÿæˆ {len(papers)} ç¯‡åšå®¢ (æ¯æ‰¹ {self.batch_size} ç¯‡ï¼Œå…± {total_batches} æ‰¹)")
        
        with tqdm(total=total_batches, desc="ğŸ“ ç”Ÿæˆæ‰¹æ¬¡", unit="æ‰¹") as batch_pbar:
            for i in range(0, len(papers), self.batch_size):
                batch_papers = papers[i:i + self.batch_size]
                batch_start = i + 1
                batch_end = min(i + self.batch_size, len(papers))
                
                batch_successful = 0
                try:
                    logger.info(f"å¤„ç†ç¬¬ {batch_start}-{batch_end} ç¯‡ (æ‰¹æ¬¡ {i//self.batch_size + 1}/{total_batches})")
                    blogs = await run_batch_generation(batch_papers, output_path=self.output_path)
                    
                    if blogs:
                        # å‡è®¾æˆåŠŸï¼Œæ”¶é›†æœ¬æ‰¹ doc_ids å¹¶ç«‹å³æ›´æ–° DB
                        batch_doc_ids = [paper.doc_id for paper in batch_papers]
                        await self.update_to_database(batch_doc_ids)
                        batch_successful = len(batch_papers)
                        self.successful_generations += batch_successful
                        logger.info(f"âœ… æ‰¹æ¬¡ {i//self.batch_size + 1} ç”Ÿæˆå¹¶æ›´æ–° DB æˆåŠŸ: {batch_successful} ç¯‡")
                    else:
                        self.failed_generations += len(batch_papers)
                        logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i//self.batch_size + 1} ç”Ÿæˆå¤±è´¥: {len(batch_papers)} ç¯‡ (è·³è¿‡ DB æ›´æ–°)")
                    
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {i//self.batch_size + 1} ç”Ÿæˆå‡ºé”™: {e}")
                    self.failed_generations += len(batch_papers)
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i//self.batch_size + 1} è·³è¿‡ DB æ›´æ–°")
                
                total_successful += batch_successful
                batch_pbar.update(1)
                await asyncio.sleep(1.0)  # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…æœåŠ¡å™¨è¿‡è½½
        
        logger.info(f"âœ… æ•´ä½“ç”Ÿæˆå®Œæˆ: {total_successful} ç¯‡æˆåŠŸ (å·²æ¯æ‰¹æ›´æ–° DB)")
        return total_successful

    async def update_to_database(self, doc_ids: List[str]) -> bool:
        """
        è¯»å–ç”Ÿæˆçš„.mdæ–‡ä»¶ï¼Œè°ƒç”¨APIæ‰¹é‡æ›´æ–°æ•°æ®åº“ä¸­çš„blogå­—æ®µ
        """
        if not doc_ids:
            return False
        
        # æ”¶é›†è®ºæ–‡æ•°æ®ï¼špaper_id å’Œ blog_content
        papers_data = []
        blogs_dir_path = Path(self.output_path)
        
        if not blogs_dir_path.exists():
            logger.error(f"âŒ åšå®¢ç›®å½•ä¸å­˜åœ¨: {blogs_dir_path}")
            return False
        
        logger.info("ğŸ“‚ å¼€å§‹å‡†å¤‡DBæ›´æ–°æ•°æ®...")
        
        with tqdm(total=len(doc_ids), desc="ğŸ“– è¯»å–åšå®¢æ–‡ä»¶", unit="ç¯‡", ncols=100) as pbar:
            for doc_id in doc_ids:
                md_file = blogs_dir_path / f"{doc_id}.md"
                
                if md_file.exists():
                    try:
                        with open(md_file, 'r', encoding='utf-8') as f:
                            content = f.read().strip()
                        
                        if content:
                            papers_data.append({
                                "paper_id": doc_id,
                                "blog_content": content
                            })
                        else:
                            logger.warning(f"âš ï¸ åšå®¢æ–‡ä»¶ä¸ºç©º: {md_file}")
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ è¯»å–åšå®¢æ–‡ä»¶å‡ºé”™ {md_file}: {e}")
                else:
                    logger.warning(f"âš ï¸ åšå®¢æ–‡ä»¶ä¸å­˜åœ¨: {md_file}")
                
                pbar.update(1)
        
        if not papers_data:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç”Ÿæˆçš„åšå®¢æ–‡ä»¶ï¼Œæ— æ³•æ›´æ–°DB")
            return False
        
        logger.info(f"ğŸš€ å¼€å§‹é€šè¿‡ API æ›´æ–° {len(papers_data)} ç¯‡è®ºæ–‡çš„åšå®¢åˆ°æ•°æ®åº“...")
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            total_batches = (len(papers_data) + self.batch_size - 1) // self.batch_size
            
            with tqdm(total=total_batches, desc="ğŸ’¾ æäº¤DBæ›´æ–°", unit="æ‰¹æ¬¡") as pbar:
                for i in range(0, len(papers_data), self.batch_size):
                    batch = papers_data[i : i + self.batch_size]
                    request_data = {"papers": batch}
                    
                    try:
                        response = await client.put(
                            self.update_blogs_endpoint,
                            json=request_data,
                            timeout=120.0
                        )
                        
                        if response.status_code == 200:
                            result = response.json()
                            updated_count = result.get("updated_count", 0)
                            self.successful_db_updates += updated_count
                            logger.info(f"âœ… æ‰¹æ¬¡æ›´æ–°æˆåŠŸ: {updated_count}/{len(batch)} ç¯‡")
                        else:
                            failed_count = len(batch)
                            self.failed_db_updates += failed_count
                            logger.error(f"âŒ æ‰¹æ¬¡æ›´æ–°å¤±è´¥: HTTP {response.status_code} - {response.text}")
                        
                    except Exception as e:
                        failed_count = len(batch)
                        self.failed_db_updates += failed_count
                        logger.error(f"âŒ API è¯·æ±‚å‡ºé”™: {e}")
                    
                    pbar.update(1)
                    await asyncio.sleep(0.2)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        logger.info("âœ… DBæ›´æ–°å®Œæˆ")
        return True

    async def run(self, dry_run: bool = False):
        """
        ä¸»æµç¨‹
        """
        # 1. ä»æ•°æ®åº“è·å–ç›®æ ‡doc_ids
        doc_ids = await self.fetch_missing_blog_doc_ids()
        if not doc_ids:
            logger.info("âœ¨ æ•°æ®åº“ä¸­æ‰€æœ‰è®ºæ–‡å‡å·²æœ‰åšå®¢ï¼Œæ— éœ€ç”Ÿæˆ")
            return
        
        # 2. åŠ è½½è®ºæ–‡æ•°æ®
        papers = await self.load_papers_from_doc_ids(doc_ids)
        if not papers:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å¯ç”Ÿæˆçš„è®ºæ–‡æ•°æ®")
            return
        
        if dry_run:
            logger.info("ğŸ” Dry Run æ¨¡å¼ç»“æŸï¼Œä¸ç”Ÿæˆåšå®¢")
            return
        
        # 3. ç”Ÿæˆåšå®¢ (æ¯æ‰¹å†…å·²æ›´æ–° DB)
        total_generated = await self.run_generation(papers)
        
        # 4. æ— éœ€é¢å¤–æ›´æ–° (å·²åœ¨æ¯æ‰¹å†…å¤„ç†)
        if total_generated == 0:
            logger.warning("âš ï¸  æ— æˆåŠŸç”Ÿæˆçš„åšå®¢")
        
        # 5. æ‰“å°æ‘˜è¦
        self.print_summary()

    def print_summary(self):
        print("\n" + "="*60)
        print("ğŸ“Š ç”Ÿæˆä¸æ›´æ–°ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"ğŸ“‹ DBä¸­ç¼ºå°‘åšå®¢æ€»æ•°: {self.total_target_papers}")
        print(f"ğŸ“‚ æˆåŠŸåŠ è½½è®ºæ–‡: {self.loaded_papers}")
        print(f"ğŸ‘» ç¼ºå°‘JSONæ–‡ä»¶: {self.missing_json_files}")
        print("-" * 30)
        print(f"âœ… æˆåŠŸç”Ÿæˆåšå®¢:   {self.successful_generations}")
        print(f"âŒ ç”Ÿæˆå¤±è´¥:         {self.failed_generations}")
        print("-" * 30)
        print(f"âœ… æˆåŠŸæ›´æ–°DB:      {self.successful_db_updates}")
        print(f"âŒ DBæ›´æ–°å¤±è´¥:       {self.failed_db_updates}")
        print("="*60)


async def main():
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = "/data3/guofang/peirongcan/PaperIgnition/orchestrator/development_config.yaml"
    
    try:
        generator = BulkBlogGenerator(
            config_path=config_path,
            batch_size=10  # ç”Ÿæˆè¾ƒæ…¢ï¼Œæ‰¹æ¬¡å°
        )
        await generator.run(dry_run=False)
        
    except KeyboardInterrupt:
        logger.info("æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
