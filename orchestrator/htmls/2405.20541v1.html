<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zachary Ankner <sup class="ltx_sup" id="id17.16.id1"><span class="ltx_text ltx_font_italic" id="id17.16.id1.1">1,2</span></sup> â€† Cody Blakeney<sup class="ltx_sup" id="id18.17.id2"><span class="ltx_text ltx_font_italic" id="id18.17.id2.1">1</span></sup> â€† Kartik Sreenivasan<sup class="ltx_sup" id="id19.18.id3"><span class="ltx_text ltx_font_italic" id="id19.18.id3.1">1</span></sup> â€‰ 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.1">Max Marion<sup class="ltx_sup" id="id6.6.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.6.1.1.1">1</span></sup></span> â€†â€† <span class="ltx_text ltx_font_bold" id="id8.8.2">Matthew L. Leavitt<sup class="ltx_sup" id="id8.8.2.1"><span class="ltx_text ltx_font_medium" id="id8.8.2.1.1">3</span></sup></span> â€†â€† <span class="ltx_text ltx_font_bold" id="id11.11.4">Mansheej Paul<sup class="ltx_sup" id="id11.11.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.11.4.1.1">1</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.4.2"><span class="ltx_text ltx_font_medium" id="id11.11.4.2.1">1</span></sup></span>Databricks â€†â€†â€† <sup class="ltx_sup" id="id20.19.id4">2</sup>MIT â€†â€†â€† <sup class="ltx_sup" id="id21.20.id5">3</sup>DatologyAI
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.1">In this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models.
While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we investigate whether smaller models can be used for perplexity-based pruning and how pruning is affected by the domain composition of the data being pruned.
We demonstrate that for multiple dataset compositions, perplexity-based pruning of pretraining data can <em class="ltx_emph ltx_font_italic" id="id16.1.1">significantly</em> improve downstream task performance: pruning based on perplexities computed with a 125 million parameter model improves the average performance on downstream tasks of a 3 billion parameter model by up to 2.04 and achieves up to a <math alttext="1.45\times" class="ltx_math_unparsed" display="inline" id="id16.1.m1.1"><semantics id="id16.1.m1.1a"><mrow id="id16.1.m1.1b"><mn id="id16.1.m1.1.1">1.45</mn><mo id="id16.1.m1.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="id16.1.m1.1c">1.45\times</annotation><annotation encoding="application/x-llamapun" id="id16.1.m1.1d">1.45 Ã—</annotation></semantics></math> reduction in pretraining steps to reach commensurate baseline performance.
Furthermore, we demonstrate that such perplexity-based data pruning also yields downstream performance gains in the over-trained and data-constrained regimes.</p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">footnotetext: </span>
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="footnotex1.1" style="width:433.6pt;">
<span class="ltx_p" id="footnotex1.1.1">Correspondence to <a class="ltx_ref ltx_href" href="mailto:ankner@mit.edu" title="">ankner@mit.edu</a>.</span>
<span class="ltx_p" id="footnotex1.1.2">Code to be made public soon.</span>
</span></span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">A large focus of the machine learning community has been improving the performance of large language models (LLMs) while reducing their training costs.
In this work, we consider how to improve the quality of an LLM by improving the quality of its pretraining data.
Although there are many techniques to improve data quality, such as augmenting training samples with additional informationÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib27" title="">2024</a>; Korbak etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib24" title="">2023</a>)</cite>, in this work we focus on the predominant method of <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">data pruning</em>: intelligently selecting a high-quality subset of a larger dataset to train on.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Data pruning is commonly used for quality filtering of noisy text data.
Simple approaches include using symbolic rulesÂ <cite class="ltx_cite ltx_citemacro_citep">(Bane etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib4" title="">2022</a>; Raffel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib46" title="">2020</a>)</cite> or using simple classifiers to determine high-quality samplesÂ <cite class="ltx_cite ltx_citemacro_citep">(Wenzek etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib60" title="">2020</a>)</cite>.
However, in addition to basic quality filtering, more complex data pruning techniques are also applied to datasets to <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">further</em> improve their quality.
<cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib63" title="">2023b</a>)</cite> perform importance resampling where importance scores are calculated based on feature similarity to a target text.
<cite class="ltx_cite ltx_citemacro_citet">Tirumala etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib55" title="">2023</a>)</cite> prune datasets by deduplicating and diversifying data based on a pretrained language modelâ€™s embeddings of the text samples.
<cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib62" title="">2023a</a>)</cite> re-weight domain proportions based on learnability as determined by a smaller proxy model.
<cite class="ltx_cite ltx_citemacro_citet">Marion etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib29" title="">2023</a>)</cite> investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we thoroughly investigate the impact that data pruning based on sample perplexityÂ <cite class="ltx_cite ltx_citemacro_citep">(Marion etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib29" title="">2023</a>)</cite> has on LLM pretraining.
In particular, we focus on the interplay between pretraining dataset composition and pruning methodology.
We further evaluate perplexity pruning in the over-trained and data-constrained regimes.
We also investigate whether evaluating the quality of data interventions based on upstream test set perplexity is a sound methodology for gauging downstream performance.
To perform perplexity-based data pruning, we train a small language model on a random subset of the given pretraining dataset and then evaluate its perplexity on each sample in the dataset.
We then prune the dataset to only include samples within some range of perplexities (i.e., sub-sample to the highest or lowest perplexity samples).
We demonstrate that for two vastly different pretraining data compositions, a small language model can be used to effectively prune the pretraining dataset of a significantly larger model, leading to significant gains in the final modelâ€™s downstream performance.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our work differs from previous work on perplexity-based data pruning for LLM pretraining in three key ways: (i) our emphasis on downstream model quality evaluation, (ii) our exploration of different pretraining dataset domain compositions, and (iii) our analysis of pruning in non-standard training regimes.
While previous works evaluate the resulting LLMâ€™s quality based on upstream metrics such as perplexity on the test split of the pretraining dataset, we evaluate data pruningâ€™s impact based on downstream evaluation benchmarks (e.g. <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">mmlu</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib18" title="">2021</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">hellaswag</em><cite class="ltx_cite ltx_citemacro_citep">(Zellers etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib64" title="">2019</a>)</cite>, etc.).
Evaluating on more meaningful benchmarks enables us to make stronger, more rigorous conclusions about the impact of perplexity-based data pruning, as we find that some techniques that significantly improve downstream performance have no, or even adverse, effects on upstream performance.
This difference in metrics enables us to conclude that smaller models can prune the data for larger models, which was not observed in previous perplexity-basd pruning works.
Secondly, while previous work only investigates pruning on datasets composed of just one domain (CommonCrawl<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://data.commoncrawl.org/" title="">https://data.commoncrawl.org/</a></span></span></span>), we consider two datasets with different domain compositions: the PileÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib15" title="">2020</a>)</cite> and DolmaÂ <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib52" title="">2024</a>)</cite>.
The Pile is composed of many diverse curated domains, with only 15.61% of the data being derived from general web-scrapes, while Dolma is a web-scrape skewed dataset, with 81.31% of its data being derived from the CommonCrawl.
We find that successful pruning techniques vary greatly for different dataset compositions to the point that the best technique for one dataset composition may degrade performance for a different composition.
Finally, we also evaluate perplexity-based data pruning in the less standard regimes of over-training and data-constrained training.
This investigation provides a broader understanding for when practitioners should use perplexity pruning for their data.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Contributions</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">Our work makes the following contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We demonstrate that, across three datasets of varying domain compositions, a small reference model can effectively prune the pretraining dataset of a significantly larger language model (<math alttext="30\times" class="ltx_math_unparsed" display="inline" id="S1.I1.i1.p1.1.m1.1"><semantics id="S1.I1.i1.p1.1.m1.1a"><mrow id="S1.I1.i1.p1.1.m1.1b"><mn id="S1.I1.i1.p1.1.m1.1.1">30</mn><mo id="S1.I1.i1.p1.1.m1.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">30\times</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.p1.1.m1.1d">30 Ã—</annotation></semantics></math> greater parameters), providing both a significant increase in downstream performance and decrease in pretraining steps (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.T1" title="In 3.2 Perplexity-Based Data Pruning Improves Downstream Performance â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.F1" title="In 3.3 Perplexity-Based Data Pruning Improves Training Efficiency â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We show that data pruning techniques can be highly sensitive to the domain composition of the dataset, suggesting the need to evaluate multiple distinct dataset compositions when conducting data pruning research (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.T1" title="In 3.2 Perplexity-Based Data Pruning Improves Downstream Performance â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S7.T4" title="In 7.1 Finding the Best Selection Criteria â€£ 7 Full Data Pruning Settings Sweep â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We investigate perplexity-based data pruning in multiple non-standard settings demonstrating that it can still lead to gains when over-training and when data-constrained (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.SS4" title="3.4 Perplexity-Based Data Pruning for Over-Trained Models â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.SS5" title="3.5 Perplexity-Based Data Pruning for the Data Constrained Regime â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.5</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We find that test set perplexity can be a misleading metric for evaluating the efficacy of data pruning techniques, as interventions that result in significantly higher test set perplexity can still achieve better performance on downstream tasks (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.T3" title="In 3.6 Upstream Perplexity is not a Reliable Evaluation Metric for Data Pruning â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Perplexity-Based Data Pruning</h2>
<figure class="ltx_float ltx_algorithm" id="algorithm1">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing" id="algorithm1.33">
<div class="ltx_listingline" id="algorithm1.33.34">
</div>
<div class="ltx_listingline" id="algorithm1.5.5">
<span class="ltx_text" id="algorithm1.5.5.1"><span class="ltx_text ltx_font_bold" id="algorithm1.5.5.1.1">Input:</span> </span>Raw dataset <math alttext="D=\{x^{(i)}\}_{i=1}^{M}" class="ltx_Math" display="inline" id="algorithm1.1.1.m1.2"><semantics id="algorithm1.1.1.m1.2a"><mrow id="algorithm1.1.1.m1.2.2" xref="algorithm1.1.1.m1.2.2.cmml"><mi id="algorithm1.1.1.m1.2.2.3" xref="algorithm1.1.1.m1.2.2.3.cmml">D</mi><mo id="algorithm1.1.1.m1.2.2.2" xref="algorithm1.1.1.m1.2.2.2.cmml">=</mo><msubsup id="algorithm1.1.1.m1.2.2.1" xref="algorithm1.1.1.m1.2.2.1.cmml"><mrow id="algorithm1.1.1.m1.2.2.1.1.1.1" xref="algorithm1.1.1.m1.2.2.1.1.1.2.cmml"><mo id="algorithm1.1.1.m1.2.2.1.1.1.1.2" stretchy="false" xref="algorithm1.1.1.m1.2.2.1.1.1.2.cmml">{</mo><msup id="algorithm1.1.1.m1.2.2.1.1.1.1.1" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1.cmml"><mi id="algorithm1.1.1.m1.2.2.1.1.1.1.1.2" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1.2.cmml">x</mi><mrow id="algorithm1.1.1.m1.1.1.1.3" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1.cmml"><mo id="algorithm1.1.1.m1.1.1.1.3.1" stretchy="false" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1.cmml">(</mo><mi id="algorithm1.1.1.m1.1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.1.cmml">i</mi><mo id="algorithm1.1.1.m1.1.1.1.3.2" stretchy="false" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="algorithm1.1.1.m1.2.2.1.1.1.1.3" stretchy="false" xref="algorithm1.1.1.m1.2.2.1.1.1.2.cmml">}</mo></mrow><mrow id="algorithm1.1.1.m1.2.2.1.1.3" xref="algorithm1.1.1.m1.2.2.1.1.3.cmml"><mi id="algorithm1.1.1.m1.2.2.1.1.3.2" xref="algorithm1.1.1.m1.2.2.1.1.3.2.cmml">i</mi><mo id="algorithm1.1.1.m1.2.2.1.1.3.1" xref="algorithm1.1.1.m1.2.2.1.1.3.1.cmml">=</mo><mn id="algorithm1.1.1.m1.2.2.1.1.3.3" xref="algorithm1.1.1.m1.2.2.1.1.3.3.cmml">1</mn></mrow><mi id="algorithm1.1.1.m1.2.2.1.3" xref="algorithm1.1.1.m1.2.2.1.3.cmml">M</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.2b"><apply id="algorithm1.1.1.m1.2.2.cmml" xref="algorithm1.1.1.m1.2.2"><eq id="algorithm1.1.1.m1.2.2.2.cmml" xref="algorithm1.1.1.m1.2.2.2"></eq><ci id="algorithm1.1.1.m1.2.2.3.cmml" xref="algorithm1.1.1.m1.2.2.3">ğ·</ci><apply id="algorithm1.1.1.m1.2.2.1.cmml" xref="algorithm1.1.1.m1.2.2.1"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.2.2.1.2.cmml" xref="algorithm1.1.1.m1.2.2.1">superscript</csymbol><apply id="algorithm1.1.1.m1.2.2.1.1.cmml" xref="algorithm1.1.1.m1.2.2.1"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.2.2.1.1.2.cmml" xref="algorithm1.1.1.m1.2.2.1">subscript</csymbol><set id="algorithm1.1.1.m1.2.2.1.1.1.2.cmml" xref="algorithm1.1.1.m1.2.2.1.1.1.1"><apply id="algorithm1.1.1.m1.2.2.1.1.1.1.1.cmml" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1">superscript</csymbol><ci id="algorithm1.1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="algorithm1.1.1.m1.2.2.1.1.1.1.1.2">ğ‘¥</ci><ci id="algorithm1.1.1.m1.1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1.1">ğ‘–</ci></apply></set><apply id="algorithm1.1.1.m1.2.2.1.1.3.cmml" xref="algorithm1.1.1.m1.2.2.1.1.3"><eq id="algorithm1.1.1.m1.2.2.1.1.3.1.cmml" xref="algorithm1.1.1.m1.2.2.1.1.3.1"></eq><ci id="algorithm1.1.1.m1.2.2.1.1.3.2.cmml" xref="algorithm1.1.1.m1.2.2.1.1.3.2">ğ‘–</ci><cn id="algorithm1.1.1.m1.2.2.1.1.3.3.cmml" type="integer" xref="algorithm1.1.1.m1.2.2.1.1.3.3">1</cn></apply></apply><ci id="algorithm1.1.1.m1.2.2.1.3.cmml" xref="algorithm1.1.1.m1.2.2.1.3">ğ‘€</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.2c">D=\{x^{(i)}\}_{i=1}^{M}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.1.1.m1.2d">italic_D = { italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT</annotation></semantics></math>, where each <math alttext="x^{(i)}" class="ltx_Math" display="inline" id="algorithm1.2.2.m2.1"><semantics id="algorithm1.2.2.m2.1a"><msup id="algorithm1.2.2.m2.1.2" xref="algorithm1.2.2.m2.1.2.cmml"><mi id="algorithm1.2.2.m2.1.2.2" xref="algorithm1.2.2.m2.1.2.2.cmml">x</mi><mrow id="algorithm1.2.2.m2.1.1.1.3" xref="algorithm1.2.2.m2.1.2.cmml"><mo id="algorithm1.2.2.m2.1.1.1.3.1" stretchy="false" xref="algorithm1.2.2.m2.1.2.cmml">(</mo><mi id="algorithm1.2.2.m2.1.1.1.1" xref="algorithm1.2.2.m2.1.1.1.1.cmml">i</mi><mo id="algorithm1.2.2.m2.1.1.1.3.2" stretchy="false" xref="algorithm1.2.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.m2.1b"><apply id="algorithm1.2.2.m2.1.2.cmml" xref="algorithm1.2.2.m2.1.2"><csymbol cd="ambiguous" id="algorithm1.2.2.m2.1.2.1.cmml" xref="algorithm1.2.2.m2.1.2">superscript</csymbol><ci id="algorithm1.2.2.m2.1.2.2.cmml" xref="algorithm1.2.2.m2.1.2.2">ğ‘¥</ci><ci id="algorithm1.2.2.m2.1.1.1.1.cmml" xref="algorithm1.2.2.m2.1.1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.m2.1c">x^{(i)}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.2.2.m2.1d">italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> is a tokenized text sample; <span class="ltx_text ltx_font_typewriter" id="algorithm1.5.5.2">selection_criteria</span> <math alttext="\in\{\text{low, medium, high}\}" class="ltx_Math" display="inline" id="algorithm1.3.3.m3.1"><semantics id="algorithm1.3.3.m3.1a"><mrow id="algorithm1.3.3.m3.1.2" xref="algorithm1.3.3.m3.1.2.cmml"><mi id="algorithm1.3.3.m3.1.2.2" xref="algorithm1.3.3.m3.1.2.2.cmml"></mi><mo id="algorithm1.3.3.m3.1.2.1" xref="algorithm1.3.3.m3.1.2.1.cmml">âˆˆ</mo><mrow id="algorithm1.3.3.m3.1.2.3.2" xref="algorithm1.3.3.m3.1.2.3.1.cmml"><mo id="algorithm1.3.3.m3.1.2.3.2.1" stretchy="false" xref="algorithm1.3.3.m3.1.2.3.1.cmml">{</mo><mtext id="algorithm1.3.3.m3.1.1" xref="algorithm1.3.3.m3.1.1a.cmml">low, medium, high</mtext><mo id="algorithm1.3.3.m3.1.2.3.2.2" stretchy="false" xref="algorithm1.3.3.m3.1.2.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m3.1b"><apply id="algorithm1.3.3.m3.1.2.cmml" xref="algorithm1.3.3.m3.1.2"><in id="algorithm1.3.3.m3.1.2.1.cmml" xref="algorithm1.3.3.m3.1.2.1"></in><csymbol cd="latexml" id="algorithm1.3.3.m3.1.2.2.cmml" xref="algorithm1.3.3.m3.1.2.2">absent</csymbol><set id="algorithm1.3.3.m3.1.2.3.1.cmml" xref="algorithm1.3.3.m3.1.2.3.2"><ci id="algorithm1.3.3.m3.1.1a.cmml" xref="algorithm1.3.3.m3.1.1"><mtext id="algorithm1.3.3.m3.1.1.cmml" xref="algorithm1.3.3.m3.1.1">low, medium, high</mtext></ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m3.1c">\in\{\text{low, medium, high}\}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.3.3.m3.1d">âˆˆ { low, medium, high }</annotation></semantics></math>; selection rate <math alttext="r_{s}\in(0,1)" class="ltx_Math" display="inline" id="algorithm1.4.4.m4.2"><semantics id="algorithm1.4.4.m4.2a"><mrow id="algorithm1.4.4.m4.2.3" xref="algorithm1.4.4.m4.2.3.cmml"><msub id="algorithm1.4.4.m4.2.3.2" xref="algorithm1.4.4.m4.2.3.2.cmml"><mi id="algorithm1.4.4.m4.2.3.2.2" xref="algorithm1.4.4.m4.2.3.2.2.cmml">r</mi><mi id="algorithm1.4.4.m4.2.3.2.3" xref="algorithm1.4.4.m4.2.3.2.3.cmml">s</mi></msub><mo id="algorithm1.4.4.m4.2.3.1" xref="algorithm1.4.4.m4.2.3.1.cmml">âˆˆ</mo><mrow id="algorithm1.4.4.m4.2.3.3.2" xref="algorithm1.4.4.m4.2.3.3.1.cmml"><mo id="algorithm1.4.4.m4.2.3.3.2.1" stretchy="false" xref="algorithm1.4.4.m4.2.3.3.1.cmml">(</mo><mn id="algorithm1.4.4.m4.1.1" xref="algorithm1.4.4.m4.1.1.cmml">0</mn><mo id="algorithm1.4.4.m4.2.3.3.2.2" xref="algorithm1.4.4.m4.2.3.3.1.cmml">,</mo><mn id="algorithm1.4.4.m4.2.2" xref="algorithm1.4.4.m4.2.2.cmml">1</mn><mo id="algorithm1.4.4.m4.2.3.3.2.3" stretchy="false" xref="algorithm1.4.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.m4.2b"><apply id="algorithm1.4.4.m4.2.3.cmml" xref="algorithm1.4.4.m4.2.3"><in id="algorithm1.4.4.m4.2.3.1.cmml" xref="algorithm1.4.4.m4.2.3.1"></in><apply id="algorithm1.4.4.m4.2.3.2.cmml" xref="algorithm1.4.4.m4.2.3.2"><csymbol cd="ambiguous" id="algorithm1.4.4.m4.2.3.2.1.cmml" xref="algorithm1.4.4.m4.2.3.2">subscript</csymbol><ci id="algorithm1.4.4.m4.2.3.2.2.cmml" xref="algorithm1.4.4.m4.2.3.2.2">ğ‘Ÿ</ci><ci id="algorithm1.4.4.m4.2.3.2.3.cmml" xref="algorithm1.4.4.m4.2.3.2.3">ğ‘ </ci></apply><interval closure="open" id="algorithm1.4.4.m4.2.3.3.1.cmml" xref="algorithm1.4.4.m4.2.3.3.2"><cn id="algorithm1.4.4.m4.1.1.cmml" type="integer" xref="algorithm1.4.4.m4.1.1">0</cn><cn id="algorithm1.4.4.m4.2.2.cmml" type="integer" xref="algorithm1.4.4.m4.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.m4.2c">r_{s}\in(0,1)</annotation><annotation encoding="application/x-llamapun" id="algorithm1.4.4.m4.2d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT âˆˆ ( 0 , 1 )</annotation></semantics></math>; reference training split size <math alttext="R" class="ltx_Math" display="inline" id="algorithm1.5.5.m5.1"><semantics id="algorithm1.5.5.m5.1a"><mi id="algorithm1.5.5.m5.1.1" xref="algorithm1.5.5.m5.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m5.1b"><ci id="algorithm1.5.5.m5.1.1.cmml" xref="algorithm1.5.5.m5.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m5.1c">R</annotation><annotation encoding="application/x-llamapun" id="algorithm1.5.5.m5.1d">italic_R</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="algorithm1.6.6">
<span class="ltx_text" id="algorithm1.6.6.1"><span class="ltx_text ltx_font_bold" id="algorithm1.6.6.1.1">Output:</span> </span>Parameters of final model trained on the perplexity pruned dataset <math alttext="\theta_{\text{final}}^{*}" class="ltx_Math" display="inline" id="algorithm1.6.6.m1.1"><semantics id="algorithm1.6.6.m1.1a"><msubsup id="algorithm1.6.6.m1.1.1" xref="algorithm1.6.6.m1.1.1.cmml"><mi id="algorithm1.6.6.m1.1.1.2.2" xref="algorithm1.6.6.m1.1.1.2.2.cmml">Î¸</mi><mtext id="algorithm1.6.6.m1.1.1.2.3" xref="algorithm1.6.6.m1.1.1.2.3a.cmml">final</mtext><mo id="algorithm1.6.6.m1.1.1.3" xref="algorithm1.6.6.m1.1.1.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.m1.1b"><apply id="algorithm1.6.6.m1.1.1.cmml" xref="algorithm1.6.6.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.6.6.m1.1.1.1.cmml" xref="algorithm1.6.6.m1.1.1">superscript</csymbol><apply id="algorithm1.6.6.m1.1.1.2.cmml" xref="algorithm1.6.6.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.6.6.m1.1.1.2.1.cmml" xref="algorithm1.6.6.m1.1.1">subscript</csymbol><ci id="algorithm1.6.6.m1.1.1.2.2.cmml" xref="algorithm1.6.6.m1.1.1.2.2">ğœƒ</ci><ci id="algorithm1.6.6.m1.1.1.2.3a.cmml" xref="algorithm1.6.6.m1.1.1.2.3"><mtext id="algorithm1.6.6.m1.1.1.2.3.cmml" mathsize="70%" xref="algorithm1.6.6.m1.1.1.2.3">final</mtext></ci></apply><times id="algorithm1.6.6.m1.1.1.3.cmml" xref="algorithm1.6.6.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.m1.1c">\theta_{\text{final}}^{*}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.6.6.m1.1d">italic_Î¸ start_POSTSUBSCRIPT final end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math>.
</div>
<div class="ltx_listingline" id="algorithm1.33.35">
</div>
<div class="ltx_listingline" id="algorithm1.7.7">
<math alttext="D_{\text{ref}},D_{\text{train}}\leftarrow\texttt{random\_split}(D,R)" class="ltx_Math" display="inline" id="algorithm1.7.7.m1.4"><semantics id="algorithm1.7.7.m1.4a"><mrow id="algorithm1.7.7.m1.4.4" xref="algorithm1.7.7.m1.4.4.cmml"><mrow id="algorithm1.7.7.m1.4.4.2.2" xref="algorithm1.7.7.m1.4.4.2.3.cmml"><msub id="algorithm1.7.7.m1.3.3.1.1.1" xref="algorithm1.7.7.m1.3.3.1.1.1.cmml"><mi id="algorithm1.7.7.m1.3.3.1.1.1.2" xref="algorithm1.7.7.m1.3.3.1.1.1.2.cmml">D</mi><mtext id="algorithm1.7.7.m1.3.3.1.1.1.3" xref="algorithm1.7.7.m1.3.3.1.1.1.3a.cmml">ref</mtext></msub><mo id="algorithm1.7.7.m1.4.4.2.2.3" xref="algorithm1.7.7.m1.4.4.2.3.cmml">,</mo><msub id="algorithm1.7.7.m1.4.4.2.2.2" xref="algorithm1.7.7.m1.4.4.2.2.2.cmml"><mi id="algorithm1.7.7.m1.4.4.2.2.2.2" xref="algorithm1.7.7.m1.4.4.2.2.2.2.cmml">D</mi><mtext id="algorithm1.7.7.m1.4.4.2.2.2.3" xref="algorithm1.7.7.m1.4.4.2.2.2.3a.cmml">train</mtext></msub></mrow><mo id="algorithm1.7.7.m1.4.4.3" stretchy="false" xref="algorithm1.7.7.m1.4.4.3.cmml">â†</mo><mrow id="algorithm1.7.7.m1.4.4.4" xref="algorithm1.7.7.m1.4.4.4.cmml"><mtext class="ltx_mathvariant_monospace" id="algorithm1.7.7.m1.4.4.4.2" xref="algorithm1.7.7.m1.4.4.4.2a.cmml">random_split</mtext><mo id="algorithm1.7.7.m1.4.4.4.1" xref="algorithm1.7.7.m1.4.4.4.1.cmml">â¢</mo><mrow id="algorithm1.7.7.m1.4.4.4.3.2" xref="algorithm1.7.7.m1.4.4.4.3.1.cmml"><mo id="algorithm1.7.7.m1.4.4.4.3.2.1" stretchy="false" xref="algorithm1.7.7.m1.4.4.4.3.1.cmml">(</mo><mi id="algorithm1.7.7.m1.1.1" xref="algorithm1.7.7.m1.1.1.cmml">D</mi><mo id="algorithm1.7.7.m1.4.4.4.3.2.2" xref="algorithm1.7.7.m1.4.4.4.3.1.cmml">,</mo><mi id="algorithm1.7.7.m1.2.2" xref="algorithm1.7.7.m1.2.2.cmml">R</mi><mo id="algorithm1.7.7.m1.4.4.4.3.2.3" stretchy="false" xref="algorithm1.7.7.m1.4.4.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m1.4b"><apply id="algorithm1.7.7.m1.4.4.cmml" xref="algorithm1.7.7.m1.4.4"><ci id="algorithm1.7.7.m1.4.4.3.cmml" xref="algorithm1.7.7.m1.4.4.3">â†</ci><list id="algorithm1.7.7.m1.4.4.2.3.cmml" xref="algorithm1.7.7.m1.4.4.2.2"><apply id="algorithm1.7.7.m1.3.3.1.1.1.cmml" xref="algorithm1.7.7.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.3.3.1.1.1.1.cmml" xref="algorithm1.7.7.m1.3.3.1.1.1">subscript</csymbol><ci id="algorithm1.7.7.m1.3.3.1.1.1.2.cmml" xref="algorithm1.7.7.m1.3.3.1.1.1.2">ğ·</ci><ci id="algorithm1.7.7.m1.3.3.1.1.1.3a.cmml" xref="algorithm1.7.7.m1.3.3.1.1.1.3"><mtext id="algorithm1.7.7.m1.3.3.1.1.1.3.cmml" mathsize="70%" xref="algorithm1.7.7.m1.3.3.1.1.1.3">ref</mtext></ci></apply><apply id="algorithm1.7.7.m1.4.4.2.2.2.cmml" xref="algorithm1.7.7.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.4.4.2.2.2.1.cmml" xref="algorithm1.7.7.m1.4.4.2.2.2">subscript</csymbol><ci id="algorithm1.7.7.m1.4.4.2.2.2.2.cmml" xref="algorithm1.7.7.m1.4.4.2.2.2.2">ğ·</ci><ci id="algorithm1.7.7.m1.4.4.2.2.2.3a.cmml" xref="algorithm1.7.7.m1.4.4.2.2.2.3"><mtext id="algorithm1.7.7.m1.4.4.2.2.2.3.cmml" mathsize="70%" xref="algorithm1.7.7.m1.4.4.2.2.2.3">train</mtext></ci></apply></list><apply id="algorithm1.7.7.m1.4.4.4.cmml" xref="algorithm1.7.7.m1.4.4.4"><times id="algorithm1.7.7.m1.4.4.4.1.cmml" xref="algorithm1.7.7.m1.4.4.4.1"></times><ci id="algorithm1.7.7.m1.4.4.4.2a.cmml" xref="algorithm1.7.7.m1.4.4.4.2"><mtext class="ltx_mathvariant_monospace" id="algorithm1.7.7.m1.4.4.4.2.cmml" xref="algorithm1.7.7.m1.4.4.4.2">random_split</mtext></ci><interval closure="open" id="algorithm1.7.7.m1.4.4.4.3.1.cmml" xref="algorithm1.7.7.m1.4.4.4.3.2"><ci id="algorithm1.7.7.m1.1.1.cmml" xref="algorithm1.7.7.m1.1.1">ğ·</ci><ci id="algorithm1.7.7.m1.2.2.cmml" xref="algorithm1.7.7.m1.2.2">ğ‘…</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m1.4c">D_{\text{ref}},D_{\text{train}}\leftarrow\texttt{random\_split}(D,R)</annotation><annotation encoding="application/x-llamapun" id="algorithm1.7.7.m1.4d">italic_D start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT â† random_split ( italic_D , italic_R )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.8.8">
<math alttext="\theta_{\text{ref}}\leftarrow" class="ltx_Math" display="inline" id="algorithm1.8.8.m1.1"><semantics id="algorithm1.8.8.m1.1a"><mrow id="algorithm1.8.8.m1.1.1" xref="algorithm1.8.8.m1.1.1.cmml"><msub id="algorithm1.8.8.m1.1.1.2" xref="algorithm1.8.8.m1.1.1.2.cmml"><mi id="algorithm1.8.8.m1.1.1.2.2" xref="algorithm1.8.8.m1.1.1.2.2.cmml">Î¸</mi><mtext id="algorithm1.8.8.m1.1.1.2.3" xref="algorithm1.8.8.m1.1.1.2.3a.cmml">ref</mtext></msub><mo id="algorithm1.8.8.m1.1.1.1" stretchy="false" xref="algorithm1.8.8.m1.1.1.1.cmml">â†</mo><mi id="algorithm1.8.8.m1.1.1.3" xref="algorithm1.8.8.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m1.1b"><apply id="algorithm1.8.8.m1.1.1.cmml" xref="algorithm1.8.8.m1.1.1"><ci id="algorithm1.8.8.m1.1.1.1.cmml" xref="algorithm1.8.8.m1.1.1.1">â†</ci><apply id="algorithm1.8.8.m1.1.1.2.cmml" xref="algorithm1.8.8.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.1.1.2.1.cmml" xref="algorithm1.8.8.m1.1.1.2">subscript</csymbol><ci id="algorithm1.8.8.m1.1.1.2.2.cmml" xref="algorithm1.8.8.m1.1.1.2.2">ğœƒ</ci><ci id="algorithm1.8.8.m1.1.1.2.3a.cmml" xref="algorithm1.8.8.m1.1.1.2.3"><mtext id="algorithm1.8.8.m1.1.1.2.3.cmml" mathsize="70%" xref="algorithm1.8.8.m1.1.1.2.3">ref</mtext></ci></apply><csymbol cd="latexml" id="algorithm1.8.8.m1.1.1.3.cmml" xref="algorithm1.8.8.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m1.1c">\theta_{\text{ref}}\leftarrow</annotation><annotation encoding="application/x-llamapun" id="algorithm1.8.8.m1.1d">italic_Î¸ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT â†</annotation></semantics></math> random parameter initialization
</div>
<div class="ltx_listingline" id="algorithm1.9.9">
<math alttext="\theta_{\text{ref}}^{*}\leftarrow\texttt{train}(\theta_{\text{ref}},D_{\text{%
ref}})" class="ltx_Math" display="inline" id="algorithm1.9.9.m1.2"><semantics id="algorithm1.9.9.m1.2a"><mrow id="algorithm1.9.9.m1.2.2" xref="algorithm1.9.9.m1.2.2.cmml"><msubsup id="algorithm1.9.9.m1.2.2.4" xref="algorithm1.9.9.m1.2.2.4.cmml"><mi id="algorithm1.9.9.m1.2.2.4.2.2" xref="algorithm1.9.9.m1.2.2.4.2.2.cmml">Î¸</mi><mtext id="algorithm1.9.9.m1.2.2.4.2.3" xref="algorithm1.9.9.m1.2.2.4.2.3a.cmml">ref</mtext><mo id="algorithm1.9.9.m1.2.2.4.3" xref="algorithm1.9.9.m1.2.2.4.3.cmml">âˆ—</mo></msubsup><mo id="algorithm1.9.9.m1.2.2.3" stretchy="false" xref="algorithm1.9.9.m1.2.2.3.cmml">â†</mo><mrow id="algorithm1.9.9.m1.2.2.2" xref="algorithm1.9.9.m1.2.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="algorithm1.9.9.m1.2.2.2.4" xref="algorithm1.9.9.m1.2.2.2.4a.cmml">train</mtext><mo id="algorithm1.9.9.m1.2.2.2.3" xref="algorithm1.9.9.m1.2.2.2.3.cmml">â¢</mo><mrow id="algorithm1.9.9.m1.2.2.2.2.2" xref="algorithm1.9.9.m1.2.2.2.2.3.cmml"><mo id="algorithm1.9.9.m1.2.2.2.2.2.3" stretchy="false" xref="algorithm1.9.9.m1.2.2.2.2.3.cmml">(</mo><msub id="algorithm1.9.9.m1.1.1.1.1.1.1" xref="algorithm1.9.9.m1.1.1.1.1.1.1.cmml"><mi id="algorithm1.9.9.m1.1.1.1.1.1.1.2" xref="algorithm1.9.9.m1.1.1.1.1.1.1.2.cmml">Î¸</mi><mtext id="algorithm1.9.9.m1.1.1.1.1.1.1.3" xref="algorithm1.9.9.m1.1.1.1.1.1.1.3a.cmml">ref</mtext></msub><mo id="algorithm1.9.9.m1.2.2.2.2.2.4" xref="algorithm1.9.9.m1.2.2.2.2.3.cmml">,</mo><msub id="algorithm1.9.9.m1.2.2.2.2.2.2" xref="algorithm1.9.9.m1.2.2.2.2.2.2.cmml"><mi id="algorithm1.9.9.m1.2.2.2.2.2.2.2" xref="algorithm1.9.9.m1.2.2.2.2.2.2.2.cmml">D</mi><mtext id="algorithm1.9.9.m1.2.2.2.2.2.2.3" xref="algorithm1.9.9.m1.2.2.2.2.2.2.3a.cmml">ref</mtext></msub><mo id="algorithm1.9.9.m1.2.2.2.2.2.5" stretchy="false" xref="algorithm1.9.9.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m1.2b"><apply id="algorithm1.9.9.m1.2.2.cmml" xref="algorithm1.9.9.m1.2.2"><ci id="algorithm1.9.9.m1.2.2.3.cmml" xref="algorithm1.9.9.m1.2.2.3">â†</ci><apply id="algorithm1.9.9.m1.2.2.4.cmml" xref="algorithm1.9.9.m1.2.2.4"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.2.2.4.1.cmml" xref="algorithm1.9.9.m1.2.2.4">superscript</csymbol><apply id="algorithm1.9.9.m1.2.2.4.2.cmml" xref="algorithm1.9.9.m1.2.2.4"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.2.2.4.2.1.cmml" xref="algorithm1.9.9.m1.2.2.4">subscript</csymbol><ci id="algorithm1.9.9.m1.2.2.4.2.2.cmml" xref="algorithm1.9.9.m1.2.2.4.2.2">ğœƒ</ci><ci id="algorithm1.9.9.m1.2.2.4.2.3a.cmml" xref="algorithm1.9.9.m1.2.2.4.2.3"><mtext id="algorithm1.9.9.m1.2.2.4.2.3.cmml" mathsize="70%" xref="algorithm1.9.9.m1.2.2.4.2.3">ref</mtext></ci></apply><times id="algorithm1.9.9.m1.2.2.4.3.cmml" xref="algorithm1.9.9.m1.2.2.4.3"></times></apply><apply id="algorithm1.9.9.m1.2.2.2.cmml" xref="algorithm1.9.9.m1.2.2.2"><times id="algorithm1.9.9.m1.2.2.2.3.cmml" xref="algorithm1.9.9.m1.2.2.2.3"></times><ci id="algorithm1.9.9.m1.2.2.2.4a.cmml" xref="algorithm1.9.9.m1.2.2.2.4"><mtext class="ltx_mathvariant_monospace" id="algorithm1.9.9.m1.2.2.2.4.cmml" xref="algorithm1.9.9.m1.2.2.2.4">train</mtext></ci><interval closure="open" id="algorithm1.9.9.m1.2.2.2.2.3.cmml" xref="algorithm1.9.9.m1.2.2.2.2.2"><apply id="algorithm1.9.9.m1.1.1.1.1.1.1.cmml" xref="algorithm1.9.9.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.1.1.1.1.1.1.1.cmml" xref="algorithm1.9.9.m1.1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.9.9.m1.1.1.1.1.1.1.2.cmml" xref="algorithm1.9.9.m1.1.1.1.1.1.1.2">ğœƒ</ci><ci id="algorithm1.9.9.m1.1.1.1.1.1.1.3a.cmml" xref="algorithm1.9.9.m1.1.1.1.1.1.1.3"><mtext id="algorithm1.9.9.m1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="algorithm1.9.9.m1.1.1.1.1.1.1.3">ref</mtext></ci></apply><apply id="algorithm1.9.9.m1.2.2.2.2.2.2.cmml" xref="algorithm1.9.9.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.2.2.2.2.2.2.1.cmml" xref="algorithm1.9.9.m1.2.2.2.2.2.2">subscript</csymbol><ci id="algorithm1.9.9.m1.2.2.2.2.2.2.2.cmml" xref="algorithm1.9.9.m1.2.2.2.2.2.2.2">ğ·</ci><ci id="algorithm1.9.9.m1.2.2.2.2.2.2.3a.cmml" xref="algorithm1.9.9.m1.2.2.2.2.2.2.3"><mtext id="algorithm1.9.9.m1.2.2.2.2.2.2.3.cmml" mathsize="70%" xref="algorithm1.9.9.m1.2.2.2.2.2.2.3">ref</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m1.2c">\theta_{\text{ref}}^{*}\leftarrow\texttt{train}(\theta_{\text{ref}},D_{\text{%
ref}})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.9.9.m1.2d">italic_Î¸ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT â† train ( italic_Î¸ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.10.10">
<span class="ltx_text ltx_font_typewriter" id="algorithm1.10.10.1">P</span> <math alttext="\leftarrow" class="ltx_Math" display="inline" id="algorithm1.10.10.m1.1"><semantics id="algorithm1.10.10.m1.1a"><mo id="algorithm1.10.10.m1.1.1" stretchy="false" xref="algorithm1.10.10.m1.1.1.cmml">â†</mo><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m1.1b"><ci id="algorithm1.10.10.m1.1.1.cmml" xref="algorithm1.10.10.m1.1.1">â†</ci></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m1.1c">\leftarrow</annotation><annotation encoding="application/x-llamapun" id="algorithm1.10.10.m1.1d">â†</annotation></semantics></math> {} 
</div>
<div class="ltx_listingline" id="algorithm1.11.11">
<span class="ltx_text ltx_font_bold" id="algorithm1.11.11.2">for</span>Â <em class="ltx_emph ltx_font_italic" id="algorithm1.11.11.1"><math alttext="x^{(i)}\in D_{\text{train}}" class="ltx_Math" display="inline" id="algorithm1.11.11.1.m1.1"><semantics id="algorithm1.11.11.1.m1.1a"><mrow id="algorithm1.11.11.1.m1.1.2" xref="algorithm1.11.11.1.m1.1.2.cmml"><msup id="algorithm1.11.11.1.m1.1.2.2" xref="algorithm1.11.11.1.m1.1.2.2.cmml"><mi id="algorithm1.11.11.1.m1.1.2.2.2" xref="algorithm1.11.11.1.m1.1.2.2.2.cmml">x</mi><mrow id="algorithm1.11.11.1.m1.1.1.1.3" xref="algorithm1.11.11.1.m1.1.2.2.cmml"><mo id="algorithm1.11.11.1.m1.1.1.1.3.1" stretchy="false" xref="algorithm1.11.11.1.m1.1.2.2.cmml">(</mo><mi id="algorithm1.11.11.1.m1.1.1.1.1" xref="algorithm1.11.11.1.m1.1.1.1.1.cmml">i</mi><mo id="algorithm1.11.11.1.m1.1.1.1.3.2" stretchy="false" xref="algorithm1.11.11.1.m1.1.2.2.cmml">)</mo></mrow></msup><mo id="algorithm1.11.11.1.m1.1.2.1" xref="algorithm1.11.11.1.m1.1.2.1.cmml">âˆˆ</mo><msub id="algorithm1.11.11.1.m1.1.2.3" xref="algorithm1.11.11.1.m1.1.2.3.cmml"><mi id="algorithm1.11.11.1.m1.1.2.3.2" xref="algorithm1.11.11.1.m1.1.2.3.2.cmml">D</mi><mtext class="ltx_mathvariant_italic" id="algorithm1.11.11.1.m1.1.2.3.3" xref="algorithm1.11.11.1.m1.1.2.3.3b.cmml"><em class="ltx_emph" id="algorithm1.11.11.1.m1.1.2.3.3.1nest" style="font-size:70%;">train</em></mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.1.m1.1b"><apply id="algorithm1.11.11.1.m1.1.2.cmml" xref="algorithm1.11.11.1.m1.1.2"><in id="algorithm1.11.11.1.m1.1.2.1.cmml" xref="algorithm1.11.11.1.m1.1.2.1"></in><apply id="algorithm1.11.11.1.m1.1.2.2.cmml" xref="algorithm1.11.11.1.m1.1.2.2"><csymbol cd="ambiguous" id="algorithm1.11.11.1.m1.1.2.2.1.cmml" xref="algorithm1.11.11.1.m1.1.2.2">superscript</csymbol><ci id="algorithm1.11.11.1.m1.1.2.2.2.cmml" xref="algorithm1.11.11.1.m1.1.2.2.2">ğ‘¥</ci><ci id="algorithm1.11.11.1.m1.1.1.1.1.cmml" xref="algorithm1.11.11.1.m1.1.1.1.1">ğ‘–</ci></apply><apply id="algorithm1.11.11.1.m1.1.2.3.cmml" xref="algorithm1.11.11.1.m1.1.2.3"><csymbol cd="ambiguous" id="algorithm1.11.11.1.m1.1.2.3.1.cmml" xref="algorithm1.11.11.1.m1.1.2.3">subscript</csymbol><ci id="algorithm1.11.11.1.m1.1.2.3.2.cmml" xref="algorithm1.11.11.1.m1.1.2.3.2">ğ·</ci><ci id="algorithm1.11.11.1.m1.1.2.3.3b.cmml" xref="algorithm1.11.11.1.m1.1.2.3.3"><mtext class="ltx_mathvariant_italic" id="algorithm1.11.11.1.m1.1.2.3.3.cmml" mathsize="70%" xref="algorithm1.11.11.1.m1.1.2.3.3"><em class="ltx_emph" id="algorithm1.11.11.1.m1.1.2.3.3.1anest" style="font-size:70%;">train</em></mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.1.m1.1c">x^{(i)}\in D_{\text{train}}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.11.11.1.m1.1d">italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT âˆˆ italic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT</annotation></semantics></math></em>Â <span class="ltx_text ltx_font_bold" id="algorithm1.11.11.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.12.12">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.12.12.1">NLL<math alttext="{}_{x^{(i)}}=\frac{1}{|x^{(i)}|}\sum_{t_{j}\in x^{(i)}}-\log{P(t_{j}|t_{&lt;j};%
\theta_{\text{ref}})}" class="ltx_math_unparsed" display="inline" id="algorithm1.12.12.1.m1.4"><semantics id="algorithm1.12.12.1.m1.4a"><mrow id="algorithm1.12.12.1.m1.4b"><mmultiscripts id="algorithm1.12.12.1.m1.4.5"><mo id="algorithm1.12.12.1.m1.4.5.2">=</mo><mprescripts id="algorithm1.12.12.1.m1.4.5a"></mprescripts><msup id="algorithm1.12.12.1.m1.1.1.1"><mi id="algorithm1.12.12.1.m1.1.1.1.3">x</mi><mrow id="algorithm1.12.12.1.m1.1.1.1.1.1.3"><mo id="algorithm1.12.12.1.m1.1.1.1.1.1.3.1" stretchy="false">(</mo><mi id="algorithm1.12.12.1.m1.1.1.1.1.1.1">i</mi><mo id="algorithm1.12.12.1.m1.1.1.1.1.1.3.2" stretchy="false">)</mo></mrow></msup><mrow id="algorithm1.12.12.1.m1.4.5b"></mrow></mmultiscripts><mfrac id="algorithm1.12.12.1.m1.3.3"><mn id="algorithm1.12.12.1.m1.3.3.4">1</mn><mrow id="algorithm1.12.12.1.m1.3.3.2.2"><mo id="algorithm1.12.12.1.m1.3.3.2.2.2" stretchy="false">|</mo><msup id="algorithm1.12.12.1.m1.3.3.2.2.1"><mi id="algorithm1.12.12.1.m1.3.3.2.2.1.2">x</mi><mrow id="algorithm1.12.12.1.m1.2.2.1.1.1.3"><mo id="algorithm1.12.12.1.m1.2.2.1.1.1.3.1" stretchy="false">(</mo><mi id="algorithm1.12.12.1.m1.2.2.1.1.1.1">i</mi><mo id="algorithm1.12.12.1.m1.2.2.1.1.1.3.2" stretchy="false">)</mo></mrow></msup><mo id="algorithm1.12.12.1.m1.3.3.2.2.3" stretchy="false">|</mo></mrow></mfrac><msub id="algorithm1.12.12.1.m1.4.6"><mo id="algorithm1.12.12.1.m1.4.6.2" rspace="0em">âˆ‘</mo><mrow id="algorithm1.12.12.1.m1.4.4.1"><msub id="algorithm1.12.12.1.m1.4.4.1.3"><mi id="algorithm1.12.12.1.m1.4.4.1.3.2">t</mi><mi id="algorithm1.12.12.1.m1.4.4.1.3.3">j</mi></msub><mo id="algorithm1.12.12.1.m1.4.4.1.2">âˆˆ</mo><msup id="algorithm1.12.12.1.m1.4.4.1.4"><mi id="algorithm1.12.12.1.m1.4.4.1.4.2">x</mi><mrow id="algorithm1.12.12.1.m1.4.4.1.1.1.3"><mo id="algorithm1.12.12.1.m1.4.4.1.1.1.3.1" stretchy="false">(</mo><mi id="algorithm1.12.12.1.m1.4.4.1.1.1.1">i</mi><mo id="algorithm1.12.12.1.m1.4.4.1.1.1.3.2" stretchy="false">)</mo></mrow></msup></mrow></msub><mo id="algorithm1.12.12.1.m1.4.7" lspace="0em">âˆ’</mo><mi id="algorithm1.12.12.1.m1.4.8">log</mi><mi id="algorithm1.12.12.1.m1.4.9">P</mi><mrow id="algorithm1.12.12.1.m1.4.10"><mo id="algorithm1.12.12.1.m1.4.10.1" stretchy="false">(</mo><msub id="algorithm1.12.12.1.m1.4.10.2"><mi id="algorithm1.12.12.1.m1.4.10.2.2">t</mi><mi id="algorithm1.12.12.1.m1.4.10.2.3">j</mi></msub><mo fence="false" id="algorithm1.12.12.1.m1.4.10.3" rspace="0.167em" stretchy="false">|</mo><msub id="algorithm1.12.12.1.m1.4.10.4"><mi id="algorithm1.12.12.1.m1.4.10.4.2">t</mi><mrow id="algorithm1.12.12.1.m1.4.10.4.3"><mi id="algorithm1.12.12.1.m1.4.10.4.3.2"></mi><mo id="algorithm1.12.12.1.m1.4.10.4.3.1">&lt;</mo><mi id="algorithm1.12.12.1.m1.4.10.4.3.3">j</mi></mrow></msub><mo id="algorithm1.12.12.1.m1.4.10.5">;</mo><msub id="algorithm1.12.12.1.m1.4.10.6"><mi id="algorithm1.12.12.1.m1.4.10.6.2">Î¸</mi><mtext id="algorithm1.12.12.1.m1.4.10.6.3">ref</mtext></msub><mo id="algorithm1.12.12.1.m1.4.10.7" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="algorithm1.12.12.1.m1.4c">{}_{x^{(i)}}=\frac{1}{|x^{(i)}|}\sum_{t_{j}\in x^{(i)}}-\log{P(t_{j}|t_{&lt;j};%
\theta_{\text{ref}})}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.12.12.1.m1.4d">start_FLOATSUBSCRIPT italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_FLOATSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG | italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT | end_ARG âˆ‘ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT âˆˆ italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT - roman_log italic_P ( italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_t start_POSTSUBSCRIPT &lt; italic_j end_POSTSUBSCRIPT ; italic_Î¸ start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT )</annotation></semantics></math></span>
</div>
<div class="ltx_listingline" id="algorithm1.13.13">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.13.13.1">PPLX<math alttext="{}_{x^{(i)}}=2^{\texttt{NLL}_{x^{(i)}}}" class="ltx_math_unparsed" display="inline" id="algorithm1.13.13.1.m1.2"><semantics id="algorithm1.13.13.1.m1.2a"><mrow id="algorithm1.13.13.1.m1.2b"><mmultiscripts id="algorithm1.13.13.1.m1.2.3"><mo id="algorithm1.13.13.1.m1.2.3.2">=</mo><mprescripts id="algorithm1.13.13.1.m1.2.3a"></mprescripts><msup id="algorithm1.13.13.1.m1.1.1.1"><mi id="algorithm1.13.13.1.m1.1.1.1.3">x</mi><mrow id="algorithm1.13.13.1.m1.1.1.1.1.1.3"><mo id="algorithm1.13.13.1.m1.1.1.1.1.1.3.1" stretchy="false">(</mo><mi id="algorithm1.13.13.1.m1.1.1.1.1.1.1">i</mi><mo id="algorithm1.13.13.1.m1.1.1.1.1.1.3.2" stretchy="false">)</mo></mrow></msup><mrow id="algorithm1.13.13.1.m1.2.3b"></mrow></mmultiscripts><msup id="algorithm1.13.13.1.m1.2.4"><mn id="algorithm1.13.13.1.m1.2.4.2">2</mn><msub id="algorithm1.13.13.1.m1.2.2.1"><mtext class="ltx_mathvariant_monospace" id="algorithm1.13.13.1.m1.2.2.1.3">NLL</mtext><msup id="algorithm1.13.13.1.m1.2.2.1.1.1"><mi id="algorithm1.13.13.1.m1.2.2.1.1.1.3">x</mi><mrow id="algorithm1.13.13.1.m1.2.2.1.1.1.1.1.3"><mo id="algorithm1.13.13.1.m1.2.2.1.1.1.1.1.3.1" stretchy="false">(</mo><mi id="algorithm1.13.13.1.m1.2.2.1.1.1.1.1.1">i</mi><mo id="algorithm1.13.13.1.m1.2.2.1.1.1.1.1.3.2" stretchy="false">)</mo></mrow></msup></msub></msup></mrow><annotation encoding="application/x-tex" id="algorithm1.13.13.1.m1.2c">{}_{x^{(i)}}=2^{\texttt{NLL}_{x^{(i)}}}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.13.13.1.m1.2d">start_FLOATSUBSCRIPT italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_FLOATSUBSCRIPT = 2 start_POSTSUPERSCRIPT NLL start_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></span>
</div>
<div class="ltx_listingline" id="algorithm1.15.15">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.15.15.2">P</span>[<math alttext="x^{(i)}" class="ltx_Math" display="inline" id="algorithm1.14.14.m1.1"><semantics id="algorithm1.14.14.m1.1a"><msup id="algorithm1.14.14.m1.1.2" xref="algorithm1.14.14.m1.1.2.cmml"><mi id="algorithm1.14.14.m1.1.2.2" xref="algorithm1.14.14.m1.1.2.2.cmml">x</mi><mrow id="algorithm1.14.14.m1.1.1.1.3" xref="algorithm1.14.14.m1.1.2.cmml"><mo id="algorithm1.14.14.m1.1.1.1.3.1" stretchy="false" xref="algorithm1.14.14.m1.1.2.cmml">(</mo><mi id="algorithm1.14.14.m1.1.1.1.1" xref="algorithm1.14.14.m1.1.1.1.1.cmml">i</mi><mo id="algorithm1.14.14.m1.1.1.1.3.2" stretchy="false" xref="algorithm1.14.14.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="algorithm1.14.14.m1.1b"><apply id="algorithm1.14.14.m1.1.2.cmml" xref="algorithm1.14.14.m1.1.2"><csymbol cd="ambiguous" id="algorithm1.14.14.m1.1.2.1.cmml" xref="algorithm1.14.14.m1.1.2">superscript</csymbol><ci id="algorithm1.14.14.m1.1.2.2.cmml" xref="algorithm1.14.14.m1.1.2.2">ğ‘¥</ci><ci id="algorithm1.14.14.m1.1.1.1.1.cmml" xref="algorithm1.14.14.m1.1.1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.14.14.m1.1c">x^{(i)}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.14.14.m1.1d">italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math>] = <span class="ltx_text ltx_font_typewriter" id="algorithm1.15.15.1">PPLX<math alttext="{}_{x^{(i)}}" class="ltx_Math" display="inline" id="algorithm1.15.15.1.m1.1"><semantics id="algorithm1.15.15.1.m1.1a"><msub id="algorithm1.15.15.1.m1.1.1" xref="algorithm1.15.15.1.m1.1.1.cmml"><mi id="algorithm1.15.15.1.m1.1.1a" xref="algorithm1.15.15.1.m1.1.1.cmml"></mi><msup id="algorithm1.15.15.1.m1.1.1.1" xref="algorithm1.15.15.1.m1.1.1.1.cmml"><mi id="algorithm1.15.15.1.m1.1.1.1.3" xref="algorithm1.15.15.1.m1.1.1.1.3.cmml">x</mi><mrow id="algorithm1.15.15.1.m1.1.1.1.1.1.3" xref="algorithm1.15.15.1.m1.1.1.1.cmml"><mo id="algorithm1.15.15.1.m1.1.1.1.1.1.3.1" stretchy="false" xref="algorithm1.15.15.1.m1.1.1.1.cmml">(</mo><mi id="algorithm1.15.15.1.m1.1.1.1.1.1.1" xref="algorithm1.15.15.1.m1.1.1.1.1.1.1.cmml">i</mi><mo id="algorithm1.15.15.1.m1.1.1.1.1.1.3.2" stretchy="false" xref="algorithm1.15.15.1.m1.1.1.1.cmml">)</mo></mrow></msup></msub><annotation-xml encoding="MathML-Content" id="algorithm1.15.15.1.m1.1b"><apply id="algorithm1.15.15.1.m1.1.1.cmml" xref="algorithm1.15.15.1.m1.1.1"><apply id="algorithm1.15.15.1.m1.1.1.1.cmml" xref="algorithm1.15.15.1.m1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.15.15.1.m1.1.1.1.2.cmml" xref="algorithm1.15.15.1.m1.1.1.1">superscript</csymbol><ci id="algorithm1.15.15.1.m1.1.1.1.3.cmml" xref="algorithm1.15.15.1.m1.1.1.1.3">ğ‘¥</ci><ci id="algorithm1.15.15.1.m1.1.1.1.1.1.1.cmml" xref="algorithm1.15.15.1.m1.1.1.1.1.1.1">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.15.15.1.m1.1c">{}_{x^{(i)}}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.15.15.1.m1.1d">start_FLOATSUBSCRIPT italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_FLOATSUBSCRIPT</annotation></semantics></math></span>
</div>
<div class="ltx_listingline" id="algorithm1.33.36">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div class="ltx_listingline" id="algorithm1.33.37"> end for
</div>
<div class="ltx_listingline" id="algorithm1.16.16">
<span class="ltx_text ltx_font_bold" id="algorithm1.16.16.2">if</span>Â <em class="ltx_emph ltx_font_italic" id="algorithm1.16.16.1"><math alttext='\texttt{selection\_criteria}==\text{"low"}' class="ltx_math_unparsed" display="inline" id="algorithm1.16.16.1.m1.1"><semantics id="algorithm1.16.16.1.m1.1a"><mrow id="algorithm1.16.16.1.m1.1b"><mtext class="ltx_mathvariant_monospace" id="algorithm1.16.16.1.m1.1.1"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.16.16.1.m1.1.1.1nest">selection_criteria</em></mtext><mo id="algorithm1.16.16.1.m1.1.2" rspace="0em">=</mo><mo id="algorithm1.16.16.1.m1.1.3" lspace="0em">=</mo><mtext class="ltx_mathvariant_italic" id="algorithm1.16.16.1.m1.1.4">"low"</mtext></mrow><annotation encoding="application/x-tex" id="algorithm1.16.16.1.m1.1c">\texttt{selection\_criteria}==\text{"low"}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.16.16.1.m1.1d">selection_criteria = = "low"</annotation></semantics></math></em>Â <span class="ltx_text ltx_font_bold" id="algorithm1.16.16.3">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.17.17">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.17.17.1">min_percentile</span> <math alttext="\leftarrow 0.0" class="ltx_Math" display="inline" id="algorithm1.17.17.m1.1"><semantics id="algorithm1.17.17.m1.1a"><mrow id="algorithm1.17.17.m1.1.1" xref="algorithm1.17.17.m1.1.1.cmml"><mi id="algorithm1.17.17.m1.1.1.2" xref="algorithm1.17.17.m1.1.1.2.cmml"></mi><mo id="algorithm1.17.17.m1.1.1.1" stretchy="false" xref="algorithm1.17.17.m1.1.1.1.cmml">â†</mo><mn id="algorithm1.17.17.m1.1.1.3" xref="algorithm1.17.17.m1.1.1.3.cmml">0.0</mn></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.17.17.m1.1b"><apply id="algorithm1.17.17.m1.1.1.cmml" xref="algorithm1.17.17.m1.1.1"><ci id="algorithm1.17.17.m1.1.1.1.cmml" xref="algorithm1.17.17.m1.1.1.1">â†</ci><csymbol cd="latexml" id="algorithm1.17.17.m1.1.1.2.cmml" xref="algorithm1.17.17.m1.1.1.2">absent</csymbol><cn id="algorithm1.17.17.m1.1.1.3.cmml" type="float" xref="algorithm1.17.17.m1.1.1.3">0.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.17.17.m1.1c">\leftarrow 0.0</annotation><annotation encoding="application/x-llamapun" id="algorithm1.17.17.m1.1d">â† 0.0</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.18.18">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.18.18.1">max_percentile</span> <math alttext="\leftarrow r_{s}" class="ltx_Math" display="inline" id="algorithm1.18.18.m1.1"><semantics id="algorithm1.18.18.m1.1a"><mrow id="algorithm1.18.18.m1.1.1" xref="algorithm1.18.18.m1.1.1.cmml"><mi id="algorithm1.18.18.m1.1.1.2" xref="algorithm1.18.18.m1.1.1.2.cmml"></mi><mo id="algorithm1.18.18.m1.1.1.1" stretchy="false" xref="algorithm1.18.18.m1.1.1.1.cmml">â†</mo><msub id="algorithm1.18.18.m1.1.1.3" xref="algorithm1.18.18.m1.1.1.3.cmml"><mi id="algorithm1.18.18.m1.1.1.3.2" xref="algorithm1.18.18.m1.1.1.3.2.cmml">r</mi><mi id="algorithm1.18.18.m1.1.1.3.3" xref="algorithm1.18.18.m1.1.1.3.3.cmml">s</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.18.18.m1.1b"><apply id="algorithm1.18.18.m1.1.1.cmml" xref="algorithm1.18.18.m1.1.1"><ci id="algorithm1.18.18.m1.1.1.1.cmml" xref="algorithm1.18.18.m1.1.1.1">â†</ci><csymbol cd="latexml" id="algorithm1.18.18.m1.1.1.2.cmml" xref="algorithm1.18.18.m1.1.1.2">absent</csymbol><apply id="algorithm1.18.18.m1.1.1.3.cmml" xref="algorithm1.18.18.m1.1.1.3"><csymbol cd="ambiguous" id="algorithm1.18.18.m1.1.1.3.1.cmml" xref="algorithm1.18.18.m1.1.1.3">subscript</csymbol><ci id="algorithm1.18.18.m1.1.1.3.2.cmml" xref="algorithm1.18.18.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="algorithm1.18.18.m1.1.1.3.3.cmml" xref="algorithm1.18.18.m1.1.1.3.3">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.18.18.m1.1c">\leftarrow r_{s}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.18.18.m1.1d">â† italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.33.38">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div class="ltx_listingline" id="algorithm1.33.39"> end if
</div>
<div class="ltx_listingline" id="algorithm1.19.19">
<span class="ltx_text ltx_font_bold" id="algorithm1.19.19.2">else if</span>Â <em class="ltx_emph ltx_font_italic" id="algorithm1.19.19.1"><math alttext='\texttt{selection\_criteria}==\text{"medium"}' class="ltx_math_unparsed" display="inline" id="algorithm1.19.19.1.m1.1"><semantics id="algorithm1.19.19.1.m1.1a"><mrow id="algorithm1.19.19.1.m1.1b"><mtext class="ltx_mathvariant_monospace" id="algorithm1.19.19.1.m1.1.1"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.19.19.1.m1.1.1.1nest">selection_criteria</em></mtext><mo id="algorithm1.19.19.1.m1.1.2" rspace="0em">=</mo><mo id="algorithm1.19.19.1.m1.1.3" lspace="0em">=</mo><mtext class="ltx_mathvariant_italic" id="algorithm1.19.19.1.m1.1.4">"medium"</mtext></mrow><annotation encoding="application/x-tex" id="algorithm1.19.19.1.m1.1c">\texttt{selection\_criteria}==\text{"medium"}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.19.19.1.m1.1d">selection_criteria = = "medium"</annotation></semantics></math></em>Â <span class="ltx_text ltx_font_bold" id="algorithm1.19.19.3">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.20.20">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.20.20.1">min_percentile</span> <math alttext="\leftarrow 0.5-\frac{r_{s}}{2}" class="ltx_Math" display="inline" id="algorithm1.20.20.m1.1"><semantics id="algorithm1.20.20.m1.1a"><mrow id="algorithm1.20.20.m1.1.1" xref="algorithm1.20.20.m1.1.1.cmml"><mi id="algorithm1.20.20.m1.1.1.2" xref="algorithm1.20.20.m1.1.1.2.cmml"></mi><mo id="algorithm1.20.20.m1.1.1.1" stretchy="false" xref="algorithm1.20.20.m1.1.1.1.cmml">â†</mo><mrow id="algorithm1.20.20.m1.1.1.3" xref="algorithm1.20.20.m1.1.1.3.cmml"><mn id="algorithm1.20.20.m1.1.1.3.2" xref="algorithm1.20.20.m1.1.1.3.2.cmml">0.5</mn><mo id="algorithm1.20.20.m1.1.1.3.1" xref="algorithm1.20.20.m1.1.1.3.1.cmml">âˆ’</mo><mfrac id="algorithm1.20.20.m1.1.1.3.3" xref="algorithm1.20.20.m1.1.1.3.3.cmml"><msub id="algorithm1.20.20.m1.1.1.3.3.2" xref="algorithm1.20.20.m1.1.1.3.3.2.cmml"><mi id="algorithm1.20.20.m1.1.1.3.3.2.2" xref="algorithm1.20.20.m1.1.1.3.3.2.2.cmml">r</mi><mi id="algorithm1.20.20.m1.1.1.3.3.2.3" xref="algorithm1.20.20.m1.1.1.3.3.2.3.cmml">s</mi></msub><mn id="algorithm1.20.20.m1.1.1.3.3.3" xref="algorithm1.20.20.m1.1.1.3.3.3.cmml">2</mn></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.20.20.m1.1b"><apply id="algorithm1.20.20.m1.1.1.cmml" xref="algorithm1.20.20.m1.1.1"><ci id="algorithm1.20.20.m1.1.1.1.cmml" xref="algorithm1.20.20.m1.1.1.1">â†</ci><csymbol cd="latexml" id="algorithm1.20.20.m1.1.1.2.cmml" xref="algorithm1.20.20.m1.1.1.2">absent</csymbol><apply id="algorithm1.20.20.m1.1.1.3.cmml" xref="algorithm1.20.20.m1.1.1.3"><minus id="algorithm1.20.20.m1.1.1.3.1.cmml" xref="algorithm1.20.20.m1.1.1.3.1"></minus><cn id="algorithm1.20.20.m1.1.1.3.2.cmml" type="float" xref="algorithm1.20.20.m1.1.1.3.2">0.5</cn><apply id="algorithm1.20.20.m1.1.1.3.3.cmml" xref="algorithm1.20.20.m1.1.1.3.3"><divide id="algorithm1.20.20.m1.1.1.3.3.1.cmml" xref="algorithm1.20.20.m1.1.1.3.3"></divide><apply id="algorithm1.20.20.m1.1.1.3.3.2.cmml" xref="algorithm1.20.20.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="algorithm1.20.20.m1.1.1.3.3.2.1.cmml" xref="algorithm1.20.20.m1.1.1.3.3.2">subscript</csymbol><ci id="algorithm1.20.20.m1.1.1.3.3.2.2.cmml" xref="algorithm1.20.20.m1.1.1.3.3.2.2">ğ‘Ÿ</ci><ci id="algorithm1.20.20.m1.1.1.3.3.2.3.cmml" xref="algorithm1.20.20.m1.1.1.3.3.2.3">ğ‘ </ci></apply><cn id="algorithm1.20.20.m1.1.1.3.3.3.cmml" type="integer" xref="algorithm1.20.20.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.20.20.m1.1c">\leftarrow 0.5-\frac{r_{s}}{2}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.20.20.m1.1d">â† 0.5 - divide start_ARG italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.21.21">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.21.21.1">max_percentile</span> <math alttext="\leftarrow 0.5+\frac{r_{s}}{2}" class="ltx_Math" display="inline" id="algorithm1.21.21.m1.1"><semantics id="algorithm1.21.21.m1.1a"><mrow id="algorithm1.21.21.m1.1.1" xref="algorithm1.21.21.m1.1.1.cmml"><mi id="algorithm1.21.21.m1.1.1.2" xref="algorithm1.21.21.m1.1.1.2.cmml"></mi><mo id="algorithm1.21.21.m1.1.1.1" stretchy="false" xref="algorithm1.21.21.m1.1.1.1.cmml">â†</mo><mrow id="algorithm1.21.21.m1.1.1.3" xref="algorithm1.21.21.m1.1.1.3.cmml"><mn id="algorithm1.21.21.m1.1.1.3.2" xref="algorithm1.21.21.m1.1.1.3.2.cmml">0.5</mn><mo id="algorithm1.21.21.m1.1.1.3.1" xref="algorithm1.21.21.m1.1.1.3.1.cmml">+</mo><mfrac id="algorithm1.21.21.m1.1.1.3.3" xref="algorithm1.21.21.m1.1.1.3.3.cmml"><msub id="algorithm1.21.21.m1.1.1.3.3.2" xref="algorithm1.21.21.m1.1.1.3.3.2.cmml"><mi id="algorithm1.21.21.m1.1.1.3.3.2.2" xref="algorithm1.21.21.m1.1.1.3.3.2.2.cmml">r</mi><mi id="algorithm1.21.21.m1.1.1.3.3.2.3" xref="algorithm1.21.21.m1.1.1.3.3.2.3.cmml">s</mi></msub><mn id="algorithm1.21.21.m1.1.1.3.3.3" xref="algorithm1.21.21.m1.1.1.3.3.3.cmml">2</mn></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.21.21.m1.1b"><apply id="algorithm1.21.21.m1.1.1.cmml" xref="algorithm1.21.21.m1.1.1"><ci id="algorithm1.21.21.m1.1.1.1.cmml" xref="algorithm1.21.21.m1.1.1.1">â†</ci><csymbol cd="latexml" id="algorithm1.21.21.m1.1.1.2.cmml" xref="algorithm1.21.21.m1.1.1.2">absent</csymbol><apply id="algorithm1.21.21.m1.1.1.3.cmml" xref="algorithm1.21.21.m1.1.1.3"><plus id="algorithm1.21.21.m1.1.1.3.1.cmml" xref="algorithm1.21.21.m1.1.1.3.1"></plus><cn id="algorithm1.21.21.m1.1.1.3.2.cmml" type="float" xref="algorithm1.21.21.m1.1.1.3.2">0.5</cn><apply id="algorithm1.21.21.m1.1.1.3.3.cmml" xref="algorithm1.21.21.m1.1.1.3.3"><divide id="algorithm1.21.21.m1.1.1.3.3.1.cmml" xref="algorithm1.21.21.m1.1.1.3.3"></divide><apply id="algorithm1.21.21.m1.1.1.3.3.2.cmml" xref="algorithm1.21.21.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="algorithm1.21.21.m1.1.1.3.3.2.1.cmml" xref="algorithm1.21.21.m1.1.1.3.3.2">subscript</csymbol><ci id="algorithm1.21.21.m1.1.1.3.3.2.2.cmml" xref="algorithm1.21.21.m1.1.1.3.3.2.2">ğ‘Ÿ</ci><ci id="algorithm1.21.21.m1.1.1.3.3.2.3.cmml" xref="algorithm1.21.21.m1.1.1.3.3.2.3">ğ‘ </ci></apply><cn id="algorithm1.21.21.m1.1.1.3.3.3.cmml" type="integer" xref="algorithm1.21.21.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.21.21.m1.1c">\leftarrow 0.5+\frac{r_{s}}{2}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.21.21.m1.1d">â† 0.5 + divide start_ARG italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.33.40">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div class="ltx_listingline" id="algorithm1.33.41"> end if
</div>
<div class="ltx_listingline" id="algorithm1.22.22">
<span class="ltx_text ltx_font_bold" id="algorithm1.22.22.2">else if</span>Â <em class="ltx_emph ltx_font_italic" id="algorithm1.22.22.1"><math alttext='\texttt{selection\_criteria}==\text{"high"}' class="ltx_math_unparsed" display="inline" id="algorithm1.22.22.1.m1.1"><semantics id="algorithm1.22.22.1.m1.1a"><mrow id="algorithm1.22.22.1.m1.1b"><mtext class="ltx_mathvariant_monospace" id="algorithm1.22.22.1.m1.1.1"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.22.22.1.m1.1.1.1nest">selection_criteria</em></mtext><mo id="algorithm1.22.22.1.m1.1.2" rspace="0em">=</mo><mo id="algorithm1.22.22.1.m1.1.3" lspace="0em">=</mo><mtext class="ltx_mathvariant_italic" id="algorithm1.22.22.1.m1.1.4">"high"</mtext></mrow><annotation encoding="application/x-tex" id="algorithm1.22.22.1.m1.1c">\texttt{selection\_criteria}==\text{"high"}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.22.22.1.m1.1d">selection_criteria = = "high"</annotation></semantics></math></em>Â <span class="ltx_text ltx_font_bold" id="algorithm1.22.22.3">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.23.23">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.23.23.1">min_percentile</span> <math alttext="\leftarrow 1-r_{s}" class="ltx_Math" display="inline" id="algorithm1.23.23.m1.1"><semantics id="algorithm1.23.23.m1.1a"><mrow id="algorithm1.23.23.m1.1.1" xref="algorithm1.23.23.m1.1.1.cmml"><mi id="algorithm1.23.23.m1.1.1.2" xref="algorithm1.23.23.m1.1.1.2.cmml"></mi><mo id="algorithm1.23.23.m1.1.1.1" stretchy="false" xref="algorithm1.23.23.m1.1.1.1.cmml">â†</mo><mrow id="algorithm1.23.23.m1.1.1.3" xref="algorithm1.23.23.m1.1.1.3.cmml"><mn id="algorithm1.23.23.m1.1.1.3.2" xref="algorithm1.23.23.m1.1.1.3.2.cmml">1</mn><mo id="algorithm1.23.23.m1.1.1.3.1" xref="algorithm1.23.23.m1.1.1.3.1.cmml">âˆ’</mo><msub id="algorithm1.23.23.m1.1.1.3.3" xref="algorithm1.23.23.m1.1.1.3.3.cmml"><mi id="algorithm1.23.23.m1.1.1.3.3.2" xref="algorithm1.23.23.m1.1.1.3.3.2.cmml">r</mi><mi id="algorithm1.23.23.m1.1.1.3.3.3" xref="algorithm1.23.23.m1.1.1.3.3.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.23.23.m1.1b"><apply id="algorithm1.23.23.m1.1.1.cmml" xref="algorithm1.23.23.m1.1.1"><ci id="algorithm1.23.23.m1.1.1.1.cmml" xref="algorithm1.23.23.m1.1.1.1">â†</ci><csymbol cd="latexml" id="algorithm1.23.23.m1.1.1.2.cmml" xref="algorithm1.23.23.m1.1.1.2">absent</csymbol><apply id="algorithm1.23.23.m1.1.1.3.cmml" xref="algorithm1.23.23.m1.1.1.3"><minus id="algorithm1.23.23.m1.1.1.3.1.cmml" xref="algorithm1.23.23.m1.1.1.3.1"></minus><cn id="algorithm1.23.23.m1.1.1.3.2.cmml" type="integer" xref="algorithm1.23.23.m1.1.1.3.2">1</cn><apply id="algorithm1.23.23.m1.1.1.3.3.cmml" xref="algorithm1.23.23.m1.1.1.3.3"><csymbol cd="ambiguous" id="algorithm1.23.23.m1.1.1.3.3.1.cmml" xref="algorithm1.23.23.m1.1.1.3.3">subscript</csymbol><ci id="algorithm1.23.23.m1.1.1.3.3.2.cmml" xref="algorithm1.23.23.m1.1.1.3.3.2">ğ‘Ÿ</ci><ci id="algorithm1.23.23.m1.1.1.3.3.3.cmml" xref="algorithm1.23.23.m1.1.1.3.3.3">ğ‘ </ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.23.23.m1.1c">\leftarrow 1-r_{s}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.23.23.m1.1d">â† 1 - italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.24.24">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_typewriter" id="algorithm1.24.24.1">max_percentile</span> <math alttext="\leftarrow 1.0" class="ltx_Math" display="inline" id="algorithm1.24.24.m1.1"><semantics id="algorithm1.24.24.m1.1a"><mrow id="algorithm1.24.24.m1.1.1" xref="algorithm1.24.24.m1.1.1.cmml"><mi id="algorithm1.24.24.m1.1.1.2" xref="algorithm1.24.24.m1.1.1.2.cmml"></mi><mo id="algorithm1.24.24.m1.1.1.1" stretchy="false" xref="algorithm1.24.24.m1.1.1.1.cmml">â†</mo><mn id="algorithm1.24.24.m1.1.1.3" xref="algorithm1.24.24.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.24.24.m1.1b"><apply id="algorithm1.24.24.m1.1.1.cmml" xref="algorithm1.24.24.m1.1.1"><ci id="algorithm1.24.24.m1.1.1.1.cmml" xref="algorithm1.24.24.m1.1.1.1">â†</ci><csymbol cd="latexml" id="algorithm1.24.24.m1.1.1.2.cmml" xref="algorithm1.24.24.m1.1.1.2">absent</csymbol><cn id="algorithm1.24.24.m1.1.1.3.cmml" type="float" xref="algorithm1.24.24.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.24.24.m1.1c">\leftarrow 1.0</annotation><annotation encoding="application/x-llamapun" id="algorithm1.24.24.m1.1d">â† 1.0</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.33.42">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div class="ltx_listingline" id="algorithm1.33.43"> end if
</div>
<div class="ltx_listingline" id="algorithm1.25.25">
<math alttext="\hat{F}_{P}\leftarrow" class="ltx_Math" display="inline" id="algorithm1.25.25.m1.1"><semantics id="algorithm1.25.25.m1.1a"><mrow id="algorithm1.25.25.m1.1.1" xref="algorithm1.25.25.m1.1.1.cmml"><msub id="algorithm1.25.25.m1.1.1.2" xref="algorithm1.25.25.m1.1.1.2.cmml"><mover accent="true" id="algorithm1.25.25.m1.1.1.2.2" xref="algorithm1.25.25.m1.1.1.2.2.cmml"><mi id="algorithm1.25.25.m1.1.1.2.2.2" xref="algorithm1.25.25.m1.1.1.2.2.2.cmml">F</mi><mo id="algorithm1.25.25.m1.1.1.2.2.1" xref="algorithm1.25.25.m1.1.1.2.2.1.cmml">^</mo></mover><mi id="algorithm1.25.25.m1.1.1.2.3" xref="algorithm1.25.25.m1.1.1.2.3.cmml">P</mi></msub><mo id="algorithm1.25.25.m1.1.1.1" stretchy="false" xref="algorithm1.25.25.m1.1.1.1.cmml">â†</mo><mi id="algorithm1.25.25.m1.1.1.3" xref="algorithm1.25.25.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.25.25.m1.1b"><apply id="algorithm1.25.25.m1.1.1.cmml" xref="algorithm1.25.25.m1.1.1"><ci id="algorithm1.25.25.m1.1.1.1.cmml" xref="algorithm1.25.25.m1.1.1.1">â†</ci><apply id="algorithm1.25.25.m1.1.1.2.cmml" xref="algorithm1.25.25.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.25.25.m1.1.1.2.1.cmml" xref="algorithm1.25.25.m1.1.1.2">subscript</csymbol><apply id="algorithm1.25.25.m1.1.1.2.2.cmml" xref="algorithm1.25.25.m1.1.1.2.2"><ci id="algorithm1.25.25.m1.1.1.2.2.1.cmml" xref="algorithm1.25.25.m1.1.1.2.2.1">^</ci><ci id="algorithm1.25.25.m1.1.1.2.2.2.cmml" xref="algorithm1.25.25.m1.1.1.2.2.2">ğ¹</ci></apply><ci id="algorithm1.25.25.m1.1.1.2.3.cmml" xref="algorithm1.25.25.m1.1.1.2.3">ğ‘ƒ</ci></apply><csymbol cd="latexml" id="algorithm1.25.25.m1.1.1.3.cmml" xref="algorithm1.25.25.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.25.25.m1.1c">\hat{F}_{P}\leftarrow</annotation><annotation encoding="application/x-llamapun" id="algorithm1.25.25.m1.1d">over^ start_ARG italic_F end_ARG start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT â†</annotation></semantics></math> empirical CDF of <span class="ltx_text ltx_font_typewriter" id="algorithm1.25.25.1">P.values()</span>
</div>
<div class="ltx_listingline" id="algorithm1.26.26">
<math alttext="D_{\text{pruned}}\leftarrow" class="ltx_Math" display="inline" id="algorithm1.26.26.m1.1"><semantics id="algorithm1.26.26.m1.1a"><mrow id="algorithm1.26.26.m1.1.1" xref="algorithm1.26.26.m1.1.1.cmml"><msub id="algorithm1.26.26.m1.1.1.2" xref="algorithm1.26.26.m1.1.1.2.cmml"><mi id="algorithm1.26.26.m1.1.1.2.2" xref="algorithm1.26.26.m1.1.1.2.2.cmml">D</mi><mtext id="algorithm1.26.26.m1.1.1.2.3" xref="algorithm1.26.26.m1.1.1.2.3a.cmml">pruned</mtext></msub><mo id="algorithm1.26.26.m1.1.1.1" stretchy="false" xref="algorithm1.26.26.m1.1.1.1.cmml">â†</mo><mi id="algorithm1.26.26.m1.1.1.3" xref="algorithm1.26.26.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.26.26.m1.1b"><apply id="algorithm1.26.26.m1.1.1.cmml" xref="algorithm1.26.26.m1.1.1"><ci id="algorithm1.26.26.m1.1.1.1.cmml" xref="algorithm1.26.26.m1.1.1.1">â†</ci><apply id="algorithm1.26.26.m1.1.1.2.cmml" xref="algorithm1.26.26.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.26.26.m1.1.1.2.1.cmml" xref="algorithm1.26.26.m1.1.1.2">subscript</csymbol><ci id="algorithm1.26.26.m1.1.1.2.2.cmml" xref="algorithm1.26.26.m1.1.1.2.2">ğ·</ci><ci id="algorithm1.26.26.m1.1.1.2.3a.cmml" xref="algorithm1.26.26.m1.1.1.2.3"><mtext id="algorithm1.26.26.m1.1.1.2.3.cmml" mathsize="70%" xref="algorithm1.26.26.m1.1.1.2.3">pruned</mtext></ci></apply><csymbol cd="latexml" id="algorithm1.26.26.m1.1.1.3.cmml" xref="algorithm1.26.26.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.26.26.m1.1c">D_{\text{pruned}}\leftarrow</annotation><annotation encoding="application/x-llamapun" id="algorithm1.26.26.m1.1d">italic_D start_POSTSUBSCRIPT pruned end_POSTSUBSCRIPT â†</annotation></semantics></math> [] 
</div>
<div class="ltx_listingline" id="algorithm1.27.27">
<span class="ltx_text ltx_font_bold" id="algorithm1.27.27.2">for</span>Â <em class="ltx_emph ltx_font_italic" id="algorithm1.27.27.1"><math alttext="x^{(i)},\texttt{PPLX}_{x^{(i)}}\in\texttt{P}" class="ltx_Math" display="inline" id="algorithm1.27.27.1.m1.4"><semantics id="algorithm1.27.27.1.m1.4a"><mrow id="algorithm1.27.27.1.m1.4.4" xref="algorithm1.27.27.1.m1.4.4.cmml"><mrow id="algorithm1.27.27.1.m1.4.4.2.2" xref="algorithm1.27.27.1.m1.4.4.2.3.cmml"><msup id="algorithm1.27.27.1.m1.3.3.1.1.1" xref="algorithm1.27.27.1.m1.3.3.1.1.1.cmml"><mi id="algorithm1.27.27.1.m1.3.3.1.1.1.2" xref="algorithm1.27.27.1.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="algorithm1.27.27.1.m1.1.1.1.3" xref="algorithm1.27.27.1.m1.3.3.1.1.1.cmml"><mo id="algorithm1.27.27.1.m1.1.1.1.3.1" stretchy="false" xref="algorithm1.27.27.1.m1.3.3.1.1.1.cmml">(</mo><mi id="algorithm1.27.27.1.m1.1.1.1.1" xref="algorithm1.27.27.1.m1.1.1.1.1.cmml">i</mi><mo id="algorithm1.27.27.1.m1.1.1.1.3.2" stretchy="false" xref="algorithm1.27.27.1.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="algorithm1.27.27.1.m1.4.4.2.2.3" xref="algorithm1.27.27.1.m1.4.4.2.3.cmml">,</mo><msub id="algorithm1.27.27.1.m1.4.4.2.2.2" xref="algorithm1.27.27.1.m1.4.4.2.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="algorithm1.27.27.1.m1.4.4.2.2.2.2" xref="algorithm1.27.27.1.m1.4.4.2.2.2.2b.cmml"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.27.27.1.m1.4.4.2.2.2.2.1nest">PPLX</em></mtext><msup id="algorithm1.27.27.1.m1.2.2.1" xref="algorithm1.27.27.1.m1.2.2.1.cmml"><mi id="algorithm1.27.27.1.m1.2.2.1.3" xref="algorithm1.27.27.1.m1.2.2.1.3.cmml">x</mi><mrow id="algorithm1.27.27.1.m1.2.2.1.1.1.3" xref="algorithm1.27.27.1.m1.2.2.1.cmml"><mo id="algorithm1.27.27.1.m1.2.2.1.1.1.3.1" stretchy="false" xref="algorithm1.27.27.1.m1.2.2.1.cmml">(</mo><mi id="algorithm1.27.27.1.m1.2.2.1.1.1.1" xref="algorithm1.27.27.1.m1.2.2.1.1.1.1.cmml">i</mi><mo id="algorithm1.27.27.1.m1.2.2.1.1.1.3.2" stretchy="false" xref="algorithm1.27.27.1.m1.2.2.1.cmml">)</mo></mrow></msup></msub></mrow><mo id="algorithm1.27.27.1.m1.4.4.3" xref="algorithm1.27.27.1.m1.4.4.3.cmml">âˆˆ</mo><mtext class="ltx_mathvariant_monospace" id="algorithm1.27.27.1.m1.4.4.4" xref="algorithm1.27.27.1.m1.4.4.4b.cmml"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.27.27.1.m1.4.4.4.1nest">P</em></mtext></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.27.27.1.m1.4b"><apply id="algorithm1.27.27.1.m1.4.4.cmml" xref="algorithm1.27.27.1.m1.4.4"><in id="algorithm1.27.27.1.m1.4.4.3.cmml" xref="algorithm1.27.27.1.m1.4.4.3"></in><list id="algorithm1.27.27.1.m1.4.4.2.3.cmml" xref="algorithm1.27.27.1.m1.4.4.2.2"><apply id="algorithm1.27.27.1.m1.3.3.1.1.1.cmml" xref="algorithm1.27.27.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="algorithm1.27.27.1.m1.3.3.1.1.1.1.cmml" xref="algorithm1.27.27.1.m1.3.3.1.1.1">superscript</csymbol><ci id="algorithm1.27.27.1.m1.3.3.1.1.1.2.cmml" xref="algorithm1.27.27.1.m1.3.3.1.1.1.2">ğ‘¥</ci><ci id="algorithm1.27.27.1.m1.1.1.1.1.cmml" xref="algorithm1.27.27.1.m1.1.1.1.1">ğ‘–</ci></apply><apply id="algorithm1.27.27.1.m1.4.4.2.2.2.cmml" xref="algorithm1.27.27.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="algorithm1.27.27.1.m1.4.4.2.2.2.1.cmml" xref="algorithm1.27.27.1.m1.4.4.2.2.2">subscript</csymbol><ci id="algorithm1.27.27.1.m1.4.4.2.2.2.2b.cmml" xref="algorithm1.27.27.1.m1.4.4.2.2.2.2"><mtext class="ltx_mathvariant_monospace" id="algorithm1.27.27.1.m1.4.4.2.2.2.2.cmml" xref="algorithm1.27.27.1.m1.4.4.2.2.2.2"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.27.27.1.m1.4.4.2.2.2.2.1anest">PPLX</em></mtext></ci><apply id="algorithm1.27.27.1.m1.2.2.1.cmml" xref="algorithm1.27.27.1.m1.2.2.1"><csymbol cd="ambiguous" id="algorithm1.27.27.1.m1.2.2.1.2.cmml" xref="algorithm1.27.27.1.m1.2.2.1">superscript</csymbol><ci id="algorithm1.27.27.1.m1.2.2.1.3.cmml" xref="algorithm1.27.27.1.m1.2.2.1.3">ğ‘¥</ci><ci id="algorithm1.27.27.1.m1.2.2.1.1.1.1.cmml" xref="algorithm1.27.27.1.m1.2.2.1.1.1.1">ğ‘–</ci></apply></apply></list><ci id="algorithm1.27.27.1.m1.4.4.4b.cmml" xref="algorithm1.27.27.1.m1.4.4.4"><mtext class="ltx_mathvariant_monospace" id="algorithm1.27.27.1.m1.4.4.4.cmml" xref="algorithm1.27.27.1.m1.4.4.4"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.27.27.1.m1.4.4.4.1anest">P</em></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.27.27.1.m1.4c">x^{(i)},\texttt{PPLX}_{x^{(i)}}\in\texttt{P}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.27.27.1.m1.4d">italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , PPLX start_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT âˆˆ P</annotation></semantics></math></em>Â <span class="ltx_text ltx_font_bold" id="algorithm1.27.27.3">do</span>
</div>
<div class="ltx_listingline" id="algorithm1.28.28">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<span class="ltx_text ltx_font_bold" id="algorithm1.28.28.2">if</span>Â <em class="ltx_emph ltx_font_italic" id="algorithm1.28.28.1"><math alttext="\texttt{min\_percentile}&lt;\hat{F}_{P}(\texttt{PPLX}_{x^{(i)}})&lt;\texttt{max\_percentile}" class="ltx_Math" display="inline" id="algorithm1.28.28.1.m1.2"><semantics id="algorithm1.28.28.1.m1.2a"><mrow id="algorithm1.28.28.1.m1.2.2" xref="algorithm1.28.28.1.m1.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="algorithm1.28.28.1.m1.2.2.3" xref="algorithm1.28.28.1.m1.2.2.3b.cmml"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.28.28.1.m1.2.2.3.1nest">min_percentile</em></mtext><mo id="algorithm1.28.28.1.m1.2.2.4" xref="algorithm1.28.28.1.m1.2.2.4.cmml">&lt;</mo><mrow id="algorithm1.28.28.1.m1.2.2.1" xref="algorithm1.28.28.1.m1.2.2.1.cmml"><msub id="algorithm1.28.28.1.m1.2.2.1.3" xref="algorithm1.28.28.1.m1.2.2.1.3.cmml"><mover accent="true" id="algorithm1.28.28.1.m1.2.2.1.3.2" xref="algorithm1.28.28.1.m1.2.2.1.3.2.cmml"><mi id="algorithm1.28.28.1.m1.2.2.1.3.2.2" xref="algorithm1.28.28.1.m1.2.2.1.3.2.2.cmml">F</mi><mo id="algorithm1.28.28.1.m1.2.2.1.3.2.1" xref="algorithm1.28.28.1.m1.2.2.1.3.2.1.cmml">^</mo></mover><mi id="algorithm1.28.28.1.m1.2.2.1.3.3" xref="algorithm1.28.28.1.m1.2.2.1.3.3.cmml">P</mi></msub><mo id="algorithm1.28.28.1.m1.2.2.1.2" xref="algorithm1.28.28.1.m1.2.2.1.2.cmml">â¢</mo><mrow id="algorithm1.28.28.1.m1.2.2.1.1.1" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.cmml"><mo id="algorithm1.28.28.1.m1.2.2.1.1.1.2" stretchy="false" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.cmml">(</mo><msub id="algorithm1.28.28.1.m1.2.2.1.1.1.1" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="algorithm1.28.28.1.m1.2.2.1.1.1.1.2" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.2b.cmml"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.28.28.1.m1.2.2.1.1.1.1.2.1nest">PPLX</em></mtext><msup id="algorithm1.28.28.1.m1.1.1.1" xref="algorithm1.28.28.1.m1.1.1.1.cmml"><mi id="algorithm1.28.28.1.m1.1.1.1.3" xref="algorithm1.28.28.1.m1.1.1.1.3.cmml">x</mi><mrow id="algorithm1.28.28.1.m1.1.1.1.1.1.3" xref="algorithm1.28.28.1.m1.1.1.1.cmml"><mo id="algorithm1.28.28.1.m1.1.1.1.1.1.3.1" stretchy="false" xref="algorithm1.28.28.1.m1.1.1.1.cmml">(</mo><mi id="algorithm1.28.28.1.m1.1.1.1.1.1.1" xref="algorithm1.28.28.1.m1.1.1.1.1.1.1.cmml">i</mi><mo id="algorithm1.28.28.1.m1.1.1.1.1.1.3.2" stretchy="false" xref="algorithm1.28.28.1.m1.1.1.1.cmml">)</mo></mrow></msup></msub><mo id="algorithm1.28.28.1.m1.2.2.1.1.1.3" stretchy="false" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="algorithm1.28.28.1.m1.2.2.5" xref="algorithm1.28.28.1.m1.2.2.5.cmml">&lt;</mo><mtext class="ltx_mathvariant_monospace" id="algorithm1.28.28.1.m1.2.2.6" xref="algorithm1.28.28.1.m1.2.2.6b.cmml"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.28.28.1.m1.2.2.6.1nest">max_percentile</em></mtext></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.28.28.1.m1.2b"><apply id="algorithm1.28.28.1.m1.2.2.cmml" xref="algorithm1.28.28.1.m1.2.2"><and id="algorithm1.28.28.1.m1.2.2a.cmml" xref="algorithm1.28.28.1.m1.2.2"></and><apply id="algorithm1.28.28.1.m1.2.2b.cmml" xref="algorithm1.28.28.1.m1.2.2"><lt id="algorithm1.28.28.1.m1.2.2.4.cmml" xref="algorithm1.28.28.1.m1.2.2.4"></lt><ci id="algorithm1.28.28.1.m1.2.2.3b.cmml" xref="algorithm1.28.28.1.m1.2.2.3"><mtext class="ltx_mathvariant_monospace" id="algorithm1.28.28.1.m1.2.2.3.cmml" xref="algorithm1.28.28.1.m1.2.2.3"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.28.28.1.m1.2.2.3.1anest">min_percentile</em></mtext></ci><apply id="algorithm1.28.28.1.m1.2.2.1.cmml" xref="algorithm1.28.28.1.m1.2.2.1"><times id="algorithm1.28.28.1.m1.2.2.1.2.cmml" xref="algorithm1.28.28.1.m1.2.2.1.2"></times><apply id="algorithm1.28.28.1.m1.2.2.1.3.cmml" xref="algorithm1.28.28.1.m1.2.2.1.3"><csymbol cd="ambiguous" id="algorithm1.28.28.1.m1.2.2.1.3.1.cmml" xref="algorithm1.28.28.1.m1.2.2.1.3">subscript</csymbol><apply id="algorithm1.28.28.1.m1.2.2.1.3.2.cmml" xref="algorithm1.28.28.1.m1.2.2.1.3.2"><ci id="algorithm1.28.28.1.m1.2.2.1.3.2.1.cmml" xref="algorithm1.28.28.1.m1.2.2.1.3.2.1">^</ci><ci id="algorithm1.28.28.1.m1.2.2.1.3.2.2.cmml" xref="algorithm1.28.28.1.m1.2.2.1.3.2.2">ğ¹</ci></apply><ci id="algorithm1.28.28.1.m1.2.2.1.3.3.cmml" xref="algorithm1.28.28.1.m1.2.2.1.3.3">ğ‘ƒ</ci></apply><apply id="algorithm1.28.28.1.m1.2.2.1.1.1.1.cmml" xref="algorithm1.28.28.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="algorithm1.28.28.1.m1.2.2.1.1.1.1.1.cmml" xref="algorithm1.28.28.1.m1.2.2.1.1.1">subscript</csymbol><ci id="algorithm1.28.28.1.m1.2.2.1.1.1.1.2b.cmml" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="algorithm1.28.28.1.m1.2.2.1.1.1.1.2.cmml" xref="algorithm1.28.28.1.m1.2.2.1.1.1.1.2"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.28.28.1.m1.2.2.1.1.1.1.2.1anest">PPLX</em></mtext></ci><apply id="algorithm1.28.28.1.m1.1.1.1.cmml" xref="algorithm1.28.28.1.m1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.28.28.1.m1.1.1.1.2.cmml" xref="algorithm1.28.28.1.m1.1.1.1">superscript</csymbol><ci id="algorithm1.28.28.1.m1.1.1.1.3.cmml" xref="algorithm1.28.28.1.m1.1.1.1.3">ğ‘¥</ci><ci id="algorithm1.28.28.1.m1.1.1.1.1.1.1.cmml" xref="algorithm1.28.28.1.m1.1.1.1.1.1.1">ğ‘–</ci></apply></apply></apply></apply><apply id="algorithm1.28.28.1.m1.2.2c.cmml" xref="algorithm1.28.28.1.m1.2.2"><lt id="algorithm1.28.28.1.m1.2.2.5.cmml" xref="algorithm1.28.28.1.m1.2.2.5"></lt><share href="https://arxiv.org/html/2405.20541v1#algorithm1.28.28.1.m1.2.2.1.cmml" id="algorithm1.28.28.1.m1.2.2d.cmml" xref="algorithm1.28.28.1.m1.2.2"></share><ci id="algorithm1.28.28.1.m1.2.2.6b.cmml" xref="algorithm1.28.28.1.m1.2.2.6"><mtext class="ltx_mathvariant_monospace" id="algorithm1.28.28.1.m1.2.2.6.cmml" xref="algorithm1.28.28.1.m1.2.2.6"><em class="ltx_emph ltx_font_typewriter" id="algorithm1.28.28.1.m1.2.2.6.1anest">max_percentile</em></mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.28.28.1.m1.2c">\texttt{min\_percentile}&lt;\hat{F}_{P}(\texttt{PPLX}_{x^{(i)}})&lt;\texttt{max\_percentile}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.28.28.1.m1.2d">min_percentile &lt; over^ start_ARG italic_F end_ARG start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( PPLX start_POSTSUBSCRIPT italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) &lt; max_percentile</annotation></semantics></math></em>Â <span class="ltx_text ltx_font_bold" id="algorithm1.28.28.3">then</span>
</div>
<div class="ltx_listingline" id="algorithm1.30.30">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
<math alttext="D_{\text{pruned}}" class="ltx_Math" display="inline" id="algorithm1.29.29.m1.1"><semantics id="algorithm1.29.29.m1.1a"><msub id="algorithm1.29.29.m1.1.1" xref="algorithm1.29.29.m1.1.1.cmml"><mi id="algorithm1.29.29.m1.1.1.2" xref="algorithm1.29.29.m1.1.1.2.cmml">D</mi><mtext id="algorithm1.29.29.m1.1.1.3" xref="algorithm1.29.29.m1.1.1.3a.cmml">pruned</mtext></msub><annotation-xml encoding="MathML-Content" id="algorithm1.29.29.m1.1b"><apply id="algorithm1.29.29.m1.1.1.cmml" xref="algorithm1.29.29.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.29.29.m1.1.1.1.cmml" xref="algorithm1.29.29.m1.1.1">subscript</csymbol><ci id="algorithm1.29.29.m1.1.1.2.cmml" xref="algorithm1.29.29.m1.1.1.2">ğ·</ci><ci id="algorithm1.29.29.m1.1.1.3a.cmml" xref="algorithm1.29.29.m1.1.1.3"><mtext id="algorithm1.29.29.m1.1.1.3.cmml" mathsize="70%" xref="algorithm1.29.29.m1.1.1.3">pruned</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.29.29.m1.1c">D_{\text{pruned}}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.29.29.m1.1d">italic_D start_POSTSUBSCRIPT pruned end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="algorithm1.30.30.1">.append(<math alttext="x^{(i)}" class="ltx_Math" display="inline" id="algorithm1.30.30.1.m1.1"><semantics id="algorithm1.30.30.1.m1.1a"><msup id="algorithm1.30.30.1.m1.1.2" xref="algorithm1.30.30.1.m1.1.2.cmml"><mi id="algorithm1.30.30.1.m1.1.2.2" xref="algorithm1.30.30.1.m1.1.2.2.cmml">x</mi><mrow id="algorithm1.30.30.1.m1.1.1.1.3" xref="algorithm1.30.30.1.m1.1.2.cmml"><mo id="algorithm1.30.30.1.m1.1.1.1.3.1" stretchy="false" xref="algorithm1.30.30.1.m1.1.2.cmml">(</mo><mi id="algorithm1.30.30.1.m1.1.1.1.1" xref="algorithm1.30.30.1.m1.1.1.1.1.cmml">i</mi><mo id="algorithm1.30.30.1.m1.1.1.1.3.2" stretchy="false" xref="algorithm1.30.30.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="algorithm1.30.30.1.m1.1b"><apply id="algorithm1.30.30.1.m1.1.2.cmml" xref="algorithm1.30.30.1.m1.1.2"><csymbol cd="ambiguous" id="algorithm1.30.30.1.m1.1.2.1.cmml" xref="algorithm1.30.30.1.m1.1.2">superscript</csymbol><ci id="algorithm1.30.30.1.m1.1.2.2.cmml" xref="algorithm1.30.30.1.m1.1.2.2">ğ‘¥</ci><ci id="algorithm1.30.30.1.m1.1.1.1.1.cmml" xref="algorithm1.30.30.1.m1.1.1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.30.30.1.m1.1c">x^{(i)}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.30.30.1.m1.1d">italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math>)</span>
</div>
<div class="ltx_listingline" id="algorithm1.33.44">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 

</div>
<div class="ltx_listingline" id="algorithm1.33.45">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â  end if
</div>
<div class="ltx_listingline" id="algorithm1.33.46">Â Â <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;">Â </span>Â Â Â 
</div>
<div class="ltx_listingline" id="algorithm1.33.47"> end for
</div>
<div class="ltx_listingline" id="algorithm1.31.31">
<math alttext="\theta_{\text{final}}\leftarrow" class="ltx_Math" display="inline" id="algorithm1.31.31.m1.1"><semantics id="algorithm1.31.31.m1.1a"><mrow id="algorithm1.31.31.m1.1.1" xref="algorithm1.31.31.m1.1.1.cmml"><msub id="algorithm1.31.31.m1.1.1.2" xref="algorithm1.31.31.m1.1.1.2.cmml"><mi id="algorithm1.31.31.m1.1.1.2.2" xref="algorithm1.31.31.m1.1.1.2.2.cmml">Î¸</mi><mtext id="algorithm1.31.31.m1.1.1.2.3" xref="algorithm1.31.31.m1.1.1.2.3a.cmml">final</mtext></msub><mo id="algorithm1.31.31.m1.1.1.1" stretchy="false" xref="algorithm1.31.31.m1.1.1.1.cmml">â†</mo><mi id="algorithm1.31.31.m1.1.1.3" xref="algorithm1.31.31.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.31.31.m1.1b"><apply id="algorithm1.31.31.m1.1.1.cmml" xref="algorithm1.31.31.m1.1.1"><ci id="algorithm1.31.31.m1.1.1.1.cmml" xref="algorithm1.31.31.m1.1.1.1">â†</ci><apply id="algorithm1.31.31.m1.1.1.2.cmml" xref="algorithm1.31.31.m1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.31.31.m1.1.1.2.1.cmml" xref="algorithm1.31.31.m1.1.1.2">subscript</csymbol><ci id="algorithm1.31.31.m1.1.1.2.2.cmml" xref="algorithm1.31.31.m1.1.1.2.2">ğœƒ</ci><ci id="algorithm1.31.31.m1.1.1.2.3a.cmml" xref="algorithm1.31.31.m1.1.1.2.3"><mtext id="algorithm1.31.31.m1.1.1.2.3.cmml" mathsize="70%" xref="algorithm1.31.31.m1.1.1.2.3">final</mtext></ci></apply><csymbol cd="latexml" id="algorithm1.31.31.m1.1.1.3.cmml" xref="algorithm1.31.31.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.31.31.m1.1c">\theta_{\text{final}}\leftarrow</annotation><annotation encoding="application/x-llamapun" id="algorithm1.31.31.m1.1d">italic_Î¸ start_POSTSUBSCRIPT final end_POSTSUBSCRIPT â†</annotation></semantics></math> random parameter initialization
</div>
<div class="ltx_listingline" id="algorithm1.32.32">
<math alttext="\theta_{\text{final}}^{*}\leftarrow\texttt{train}(\theta_{\text{final}},D_{%
\text{pruned}})" class="ltx_Math" display="inline" id="algorithm1.32.32.m1.2"><semantics id="algorithm1.32.32.m1.2a"><mrow id="algorithm1.32.32.m1.2.2" xref="algorithm1.32.32.m1.2.2.cmml"><msubsup id="algorithm1.32.32.m1.2.2.4" xref="algorithm1.32.32.m1.2.2.4.cmml"><mi id="algorithm1.32.32.m1.2.2.4.2.2" xref="algorithm1.32.32.m1.2.2.4.2.2.cmml">Î¸</mi><mtext id="algorithm1.32.32.m1.2.2.4.2.3" xref="algorithm1.32.32.m1.2.2.4.2.3a.cmml">final</mtext><mo id="algorithm1.32.32.m1.2.2.4.3" xref="algorithm1.32.32.m1.2.2.4.3.cmml">âˆ—</mo></msubsup><mo id="algorithm1.32.32.m1.2.2.3" stretchy="false" xref="algorithm1.32.32.m1.2.2.3.cmml">â†</mo><mrow id="algorithm1.32.32.m1.2.2.2" xref="algorithm1.32.32.m1.2.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="algorithm1.32.32.m1.2.2.2.4" xref="algorithm1.32.32.m1.2.2.2.4a.cmml">train</mtext><mo id="algorithm1.32.32.m1.2.2.2.3" xref="algorithm1.32.32.m1.2.2.2.3.cmml">â¢</mo><mrow id="algorithm1.32.32.m1.2.2.2.2.2" xref="algorithm1.32.32.m1.2.2.2.2.3.cmml"><mo id="algorithm1.32.32.m1.2.2.2.2.2.3" stretchy="false" xref="algorithm1.32.32.m1.2.2.2.2.3.cmml">(</mo><msub id="algorithm1.32.32.m1.1.1.1.1.1.1" xref="algorithm1.32.32.m1.1.1.1.1.1.1.cmml"><mi id="algorithm1.32.32.m1.1.1.1.1.1.1.2" xref="algorithm1.32.32.m1.1.1.1.1.1.1.2.cmml">Î¸</mi><mtext id="algorithm1.32.32.m1.1.1.1.1.1.1.3" xref="algorithm1.32.32.m1.1.1.1.1.1.1.3a.cmml">final</mtext></msub><mo id="algorithm1.32.32.m1.2.2.2.2.2.4" xref="algorithm1.32.32.m1.2.2.2.2.3.cmml">,</mo><msub id="algorithm1.32.32.m1.2.2.2.2.2.2" xref="algorithm1.32.32.m1.2.2.2.2.2.2.cmml"><mi id="algorithm1.32.32.m1.2.2.2.2.2.2.2" xref="algorithm1.32.32.m1.2.2.2.2.2.2.2.cmml">D</mi><mtext id="algorithm1.32.32.m1.2.2.2.2.2.2.3" xref="algorithm1.32.32.m1.2.2.2.2.2.2.3a.cmml">pruned</mtext></msub><mo id="algorithm1.32.32.m1.2.2.2.2.2.5" stretchy="false" xref="algorithm1.32.32.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.32.32.m1.2b"><apply id="algorithm1.32.32.m1.2.2.cmml" xref="algorithm1.32.32.m1.2.2"><ci id="algorithm1.32.32.m1.2.2.3.cmml" xref="algorithm1.32.32.m1.2.2.3">â†</ci><apply id="algorithm1.32.32.m1.2.2.4.cmml" xref="algorithm1.32.32.m1.2.2.4"><csymbol cd="ambiguous" id="algorithm1.32.32.m1.2.2.4.1.cmml" xref="algorithm1.32.32.m1.2.2.4">superscript</csymbol><apply id="algorithm1.32.32.m1.2.2.4.2.cmml" xref="algorithm1.32.32.m1.2.2.4"><csymbol cd="ambiguous" id="algorithm1.32.32.m1.2.2.4.2.1.cmml" xref="algorithm1.32.32.m1.2.2.4">subscript</csymbol><ci id="algorithm1.32.32.m1.2.2.4.2.2.cmml" xref="algorithm1.32.32.m1.2.2.4.2.2">ğœƒ</ci><ci id="algorithm1.32.32.m1.2.2.4.2.3a.cmml" xref="algorithm1.32.32.m1.2.2.4.2.3"><mtext id="algorithm1.32.32.m1.2.2.4.2.3.cmml" mathsize="70%" xref="algorithm1.32.32.m1.2.2.4.2.3">final</mtext></ci></apply><times id="algorithm1.32.32.m1.2.2.4.3.cmml" xref="algorithm1.32.32.m1.2.2.4.3"></times></apply><apply id="algorithm1.32.32.m1.2.2.2.cmml" xref="algorithm1.32.32.m1.2.2.2"><times id="algorithm1.32.32.m1.2.2.2.3.cmml" xref="algorithm1.32.32.m1.2.2.2.3"></times><ci id="algorithm1.32.32.m1.2.2.2.4a.cmml" xref="algorithm1.32.32.m1.2.2.2.4"><mtext class="ltx_mathvariant_monospace" id="algorithm1.32.32.m1.2.2.2.4.cmml" xref="algorithm1.32.32.m1.2.2.2.4">train</mtext></ci><interval closure="open" id="algorithm1.32.32.m1.2.2.2.2.3.cmml" xref="algorithm1.32.32.m1.2.2.2.2.2"><apply id="algorithm1.32.32.m1.1.1.1.1.1.1.cmml" xref="algorithm1.32.32.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.32.32.m1.1.1.1.1.1.1.1.cmml" xref="algorithm1.32.32.m1.1.1.1.1.1.1">subscript</csymbol><ci id="algorithm1.32.32.m1.1.1.1.1.1.1.2.cmml" xref="algorithm1.32.32.m1.1.1.1.1.1.1.2">ğœƒ</ci><ci id="algorithm1.32.32.m1.1.1.1.1.1.1.3a.cmml" xref="algorithm1.32.32.m1.1.1.1.1.1.1.3"><mtext id="algorithm1.32.32.m1.1.1.1.1.1.1.3.cmml" mathsize="70%" xref="algorithm1.32.32.m1.1.1.1.1.1.1.3">final</mtext></ci></apply><apply id="algorithm1.32.32.m1.2.2.2.2.2.2.cmml" xref="algorithm1.32.32.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.32.32.m1.2.2.2.2.2.2.1.cmml" xref="algorithm1.32.32.m1.2.2.2.2.2.2">subscript</csymbol><ci id="algorithm1.32.32.m1.2.2.2.2.2.2.2.cmml" xref="algorithm1.32.32.m1.2.2.2.2.2.2.2">ğ·</ci><ci id="algorithm1.32.32.m1.2.2.2.2.2.2.3a.cmml" xref="algorithm1.32.32.m1.2.2.2.2.2.2.3"><mtext id="algorithm1.32.32.m1.2.2.2.2.2.2.3.cmml" mathsize="70%" xref="algorithm1.32.32.m1.2.2.2.2.2.2.3">pruned</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.32.32.m1.2c">\theta_{\text{final}}^{*}\leftarrow\texttt{train}(\theta_{\text{final}},D_{%
\text{pruned}})</annotation><annotation encoding="application/x-llamapun" id="algorithm1.32.32.m1.2d">italic_Î¸ start_POSTSUBSCRIPT final end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT â† train ( italic_Î¸ start_POSTSUBSCRIPT final end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT pruned end_POSTSUBSCRIPT )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algorithm1.33.33">
<span class="ltx_text ltx_font_bold" id="algorithm1.33.33.2">return</span> <em class="ltx_emph ltx_font_italic" id="algorithm1.33.33.1"><math alttext="\theta_{\text{final}}^{*}" class="ltx_Math" display="inline" id="algorithm1.33.33.1.m1.1"><semantics id="algorithm1.33.33.1.m1.1a"><msubsup id="algorithm1.33.33.1.m1.1.1" xref="algorithm1.33.33.1.m1.1.1.cmml"><mi id="algorithm1.33.33.1.m1.1.1.2.2" xref="algorithm1.33.33.1.m1.1.1.2.2.cmml">Î¸</mi><mtext class="ltx_mathvariant_italic" id="algorithm1.33.33.1.m1.1.1.2.3" xref="algorithm1.33.33.1.m1.1.1.2.3b.cmml"><em class="ltx_emph" id="algorithm1.33.33.1.m1.1.1.2.3.1nest" style="font-size:70%;">final</em></mtext><mo id="algorithm1.33.33.1.m1.1.1.3" xref="algorithm1.33.33.1.m1.1.1.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="algorithm1.33.33.1.m1.1b"><apply id="algorithm1.33.33.1.m1.1.1.cmml" xref="algorithm1.33.33.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.33.33.1.m1.1.1.1.cmml" xref="algorithm1.33.33.1.m1.1.1">superscript</csymbol><apply id="algorithm1.33.33.1.m1.1.1.2.cmml" xref="algorithm1.33.33.1.m1.1.1"><csymbol cd="ambiguous" id="algorithm1.33.33.1.m1.1.1.2.1.cmml" xref="algorithm1.33.33.1.m1.1.1">subscript</csymbol><ci id="algorithm1.33.33.1.m1.1.1.2.2.cmml" xref="algorithm1.33.33.1.m1.1.1.2.2">ğœƒ</ci><ci id="algorithm1.33.33.1.m1.1.1.2.3b.cmml" xref="algorithm1.33.33.1.m1.1.1.2.3"><mtext class="ltx_mathvariant_italic" id="algorithm1.33.33.1.m1.1.1.2.3.cmml" mathsize="70%" xref="algorithm1.33.33.1.m1.1.1.2.3"><em class="ltx_emph" id="algorithm1.33.33.1.m1.1.1.2.3.1anest" style="font-size:70%;">final</em></mtext></ci></apply><times id="algorithm1.33.33.1.m1.1.1.3.cmml" xref="algorithm1.33.33.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.33.33.1.m1.1c">\theta_{\text{final}}^{*}</annotation><annotation encoding="application/x-llamapun" id="algorithm1.33.33.1.m1.1d">italic_Î¸ start_POSTSUBSCRIPT final end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math></em>
</div>
<div class="ltx_listingline" id="algorithm1.33.48">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="algorithm1.35.1.1">AlgorithmÂ 1</span> </span>Psuedocode for performing perplexity-based data pruning.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.2">We start by training a reference model that will be used to calculate the perplexity of all samples in our dataset.
First, we partition the original dataset into two splits: one for training the reference model and one for training the final model.
After training the reference model on the standard next-token prediction objective, we compute the reference modelâ€™s perplexity on each of the samples in the final modelâ€™s training split.
We then prune the final modelâ€™s dataset split to a fraction of its original size, referred to as the <em class="ltx_emph ltx_font_italic" id="S2.p1.2.1">selection rate</em> (<math alttext="r_{s}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">r</mi><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ğ‘Ÿ</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">r_{s}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>), by selecting samples according to a <em class="ltx_emph ltx_font_italic" id="S2.p1.2.2">selection criteria</em> which can be one of low, medium, or high.
In low selection, samples with the lowest perplexity are selected.
In medium selection, we select samples whose perplexity is close to the median perplexity, that is, samples with perplexity in the <math alttext="[50-\frac{r_{s}}{2},50+\frac{r_{s}}{2}]" class="ltx_Math" display="inline" id="S2.p1.2.m2.2"><semantics id="S2.p1.2.m2.2a"><mrow id="S2.p1.2.m2.2.2.2" xref="S2.p1.2.m2.2.2.3.cmml"><mo id="S2.p1.2.m2.2.2.2.3" stretchy="false" xref="S2.p1.2.m2.2.2.3.cmml">[</mo><mrow id="S2.p1.2.m2.1.1.1.1" xref="S2.p1.2.m2.1.1.1.1.cmml"><mn id="S2.p1.2.m2.1.1.1.1.2" xref="S2.p1.2.m2.1.1.1.1.2.cmml">50</mn><mo id="S2.p1.2.m2.1.1.1.1.1" xref="S2.p1.2.m2.1.1.1.1.1.cmml">âˆ’</mo><mfrac id="S2.p1.2.m2.1.1.1.1.3" xref="S2.p1.2.m2.1.1.1.1.3.cmml"><msub id="S2.p1.2.m2.1.1.1.1.3.2" xref="S2.p1.2.m2.1.1.1.1.3.2.cmml"><mi id="S2.p1.2.m2.1.1.1.1.3.2.2" xref="S2.p1.2.m2.1.1.1.1.3.2.2.cmml">r</mi><mi id="S2.p1.2.m2.1.1.1.1.3.2.3" xref="S2.p1.2.m2.1.1.1.1.3.2.3.cmml">s</mi></msub><mn id="S2.p1.2.m2.1.1.1.1.3.3" xref="S2.p1.2.m2.1.1.1.1.3.3.cmml">2</mn></mfrac></mrow><mo id="S2.p1.2.m2.2.2.2.4" xref="S2.p1.2.m2.2.2.3.cmml">,</mo><mrow id="S2.p1.2.m2.2.2.2.2" xref="S2.p1.2.m2.2.2.2.2.cmml"><mn id="S2.p1.2.m2.2.2.2.2.2" xref="S2.p1.2.m2.2.2.2.2.2.cmml">50</mn><mo id="S2.p1.2.m2.2.2.2.2.1" xref="S2.p1.2.m2.2.2.2.2.1.cmml">+</mo><mfrac id="S2.p1.2.m2.2.2.2.2.3" xref="S2.p1.2.m2.2.2.2.2.3.cmml"><msub id="S2.p1.2.m2.2.2.2.2.3.2" xref="S2.p1.2.m2.2.2.2.2.3.2.cmml"><mi id="S2.p1.2.m2.2.2.2.2.3.2.2" xref="S2.p1.2.m2.2.2.2.2.3.2.2.cmml">r</mi><mi id="S2.p1.2.m2.2.2.2.2.3.2.3" xref="S2.p1.2.m2.2.2.2.2.3.2.3.cmml">s</mi></msub><mn id="S2.p1.2.m2.2.2.2.2.3.3" xref="S2.p1.2.m2.2.2.2.2.3.3.cmml">2</mn></mfrac></mrow><mo id="S2.p1.2.m2.2.2.2.5" stretchy="false" xref="S2.p1.2.m2.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.2b"><interval closure="closed" id="S2.p1.2.m2.2.2.3.cmml" xref="S2.p1.2.m2.2.2.2"><apply id="S2.p1.2.m2.1.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1.1"><minus id="S2.p1.2.m2.1.1.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1.1.1"></minus><cn id="S2.p1.2.m2.1.1.1.1.2.cmml" type="integer" xref="S2.p1.2.m2.1.1.1.1.2">50</cn><apply id="S2.p1.2.m2.1.1.1.1.3.cmml" xref="S2.p1.2.m2.1.1.1.1.3"><divide id="S2.p1.2.m2.1.1.1.1.3.1.cmml" xref="S2.p1.2.m2.1.1.1.1.3"></divide><apply id="S2.p1.2.m2.1.1.1.1.3.2.cmml" xref="S2.p1.2.m2.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.1.3.2.1.cmml" xref="S2.p1.2.m2.1.1.1.1.3.2">subscript</csymbol><ci id="S2.p1.2.m2.1.1.1.1.3.2.2.cmml" xref="S2.p1.2.m2.1.1.1.1.3.2.2">ğ‘Ÿ</ci><ci id="S2.p1.2.m2.1.1.1.1.3.2.3.cmml" xref="S2.p1.2.m2.1.1.1.1.3.2.3">ğ‘ </ci></apply><cn id="S2.p1.2.m2.1.1.1.1.3.3.cmml" type="integer" xref="S2.p1.2.m2.1.1.1.1.3.3">2</cn></apply></apply><apply id="S2.p1.2.m2.2.2.2.2.cmml" xref="S2.p1.2.m2.2.2.2.2"><plus id="S2.p1.2.m2.2.2.2.2.1.cmml" xref="S2.p1.2.m2.2.2.2.2.1"></plus><cn id="S2.p1.2.m2.2.2.2.2.2.cmml" type="integer" xref="S2.p1.2.m2.2.2.2.2.2">50</cn><apply id="S2.p1.2.m2.2.2.2.2.3.cmml" xref="S2.p1.2.m2.2.2.2.2.3"><divide id="S2.p1.2.m2.2.2.2.2.3.1.cmml" xref="S2.p1.2.m2.2.2.2.2.3"></divide><apply id="S2.p1.2.m2.2.2.2.2.3.2.cmml" xref="S2.p1.2.m2.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S2.p1.2.m2.2.2.2.2.3.2.1.cmml" xref="S2.p1.2.m2.2.2.2.2.3.2">subscript</csymbol><ci id="S2.p1.2.m2.2.2.2.2.3.2.2.cmml" xref="S2.p1.2.m2.2.2.2.2.3.2.2">ğ‘Ÿ</ci><ci id="S2.p1.2.m2.2.2.2.2.3.2.3.cmml" xref="S2.p1.2.m2.2.2.2.2.3.2.3">ğ‘ </ci></apply><cn id="S2.p1.2.m2.2.2.2.2.3.3.cmml" type="integer" xref="S2.p1.2.m2.2.2.2.2.3.3">2</cn></apply></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.2c">[50-\frac{r_{s}}{2},50+\frac{r_{s}}{2}]</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.2d">[ 50 - divide start_ARG italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG , 50 + divide start_ARG italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ]</annotation></semantics></math> percentiles of all perplexities.
In high selection, samples with the highest perplexity are selected.
After pruning our dataset, we train a final model using the standard next token prediction objective on the pruned version of the final model training split.
We present a pseudocode for pruning based on perplexity in AlgorithmÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#algorithm1" title="Algorithm 1 â€£ 2 Perplexity-Based Data Pruning â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">We consider the setting in which the reference model is significantly smaller than the final model.
While this assumption is not strictly necessary, we believe that it is the most practically relevant setup, as it best reflects a data pruning paradigm that would be used for the next generation of LLMs where the models being trained are larger than any existing models.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Setup</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Models.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">All models are based on the MPT family of transformer modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib59" title="">2017</a>; MosaicML, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib37" title="">2023c</a>)</cite>.
All reference models have 125 million parameters, and we consider final models with 1 billion and 3 billion parameters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Data.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">We consider two datasets in this work.
The PileÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib15" title="">2020</a>)</cite> is composed of 22 different domains that range from general web scrapes to legal text.
DolmaÂ <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib52" title="">2024</a>)</cite> is composed of 7 different domains and is derived mainly from general web scrapes.
We tokenize all datasets using the GPT-4 tokenizerÂ <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib39" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Training and hyperparameters.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">All reference models are trained for a fixed duration of 26 billion tokens. Unless otherwise specified, all final models are trained to Chinchilla optimalÂ <cite class="ltx_cite ltx_citemacro_citep">(Hoffmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib19" title="">2022</a>)</cite>, meaning that each final modelâ€™s training duration in tokens is 20 times its parameter count.
All models are trained using the decoupled Lion optimizerÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib7" title="">2024</a>)</cite> with a cosine learning rate schedule.
All reference models and 1B parameter models are trained with a maximum learning rate and weight decay of <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px3.p1.1.1">2e-4</span> and all 3B models are trained with a maximum learning rate and weight decay of <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px3.p1.1.2">1.6e-4</span>.
Training is conducted using llm-foundryÂ <cite class="ltx_cite ltx_citemacro_citep">(MosaicML, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib36" title="">2023b</a>)</cite> and using both Nvidia <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px3.p1.1.3">A100s</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS0.Px3.p1.1.4">H100s</span>.
We perform two trials for each experiment.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Evaluation.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.3">We evaluate models on 33 different downstream question-answering tasks using the MosaicML evaluation gauntletÂ <cite class="ltx_cite ltx_citemacro_citep">(MosaicML, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib35" title="">2023a</a>)</cite>.
Before averaging the accuracy across tasks, we normalize each task by the baseline of random guessing
<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Not to be confused, the random accuracy normalization we use is different from the normalized accuracy reported by the <a class="ltx_ref ltx_href" href="https://github.com/EleutherAI/lm-evaluation-harness/tree/main" title="">EleutherAI LM Evaluation Harness</a>, which normalizes based on the Byte-length of the response.</span></span></span>.
Specifically, we normalize the accuracy of each individual task as <math alttext="a_{n}=\frac{a_{m}-a_{r}}{1-a_{r}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px4.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml">n</mi></msub><mo id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mfrac id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mrow id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml"><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.3.cmml">m</mi></msub><mo id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.1.cmml">âˆ’</mo><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.3.cmml">r</mi></msub></mrow><mrow id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.cmml"><mn id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.2.cmml">1</mn><mo id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.1.cmml">âˆ’</mo><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.3.cmml">r</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1"><eq id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1"></eq><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.3">ğ‘›</ci></apply><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3"><divide id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3"></divide><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2"><minus id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.1"></minus><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.2.3">ğ‘š</ci></apply><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.2.3.3">ğ‘Ÿ</ci></apply></apply><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3"><minus id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.1"></minus><cn id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.2">1</cn><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.3.3.3">ğ‘Ÿ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.1.m1.1c">a_{n}=\frac{a_{m}-a_{r}}{1-a_{r}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.1.m1.1d">italic_a start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = divide start_ARG italic_a start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT - italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_ARG start_ARG 1 - italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>, where <math alttext="a_{m}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px4.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.2.m2.1c">a_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.2.m2.1d">italic_a start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> is the accuracy of the model and <math alttext="a_{r}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px4.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.3.m3.1c">a_{r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.3.m3.1d">italic_a start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math> is the expected accuracy of random guessing.
We report the average normalized accuracy for each task category as well as the average normalized accuracy across all task categories.
More details on tasks and task categories are listed inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S8" title="8 Detailed Evaluation Setup â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Perplexity-Based Data Pruning Improves Downstream Performance</h3>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Average normalized accuracy grouped by task category for both datasets and both final model sizes.
For all datasets and model sizes we find that training on perplexity pruned data outperforms the baseline.
Bold results are within one standard error of the highest score.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1">Pruning Method</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1"><span class="ltx_text" id="S3.T1.1.1.1.2.1.1.1" style="font-size:80%;">World Knowledge</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.3.1">
<span class="ltx_p" id="S3.T1.1.1.1.3.1.1"><span class="ltx_text" id="S3.T1.1.1.1.3.1.1.1" style="font-size:80%;">Common Sense Reasoning</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.4.1">
<span class="ltx_p" id="S3.T1.1.1.1.4.1.1"><span class="ltx_text" id="S3.T1.1.1.1.4.1.1.1" style="font-size:80%;">Language Understanding</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.5.1">
<span class="ltx_p" id="S3.T1.1.1.1.5.1.1"><span class="ltx_text" id="S3.T1.1.1.1.5.1.1.1" style="font-size:80%;">Symbolic Problem Solving</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.6.1">
<span class="ltx_p" id="S3.T1.1.1.1.6.1.1"><span class="ltx_text" id="S3.T1.1.1.1.6.1.1.1" style="font-size:80%;">Reading Comprehension</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.7.1">
<span class="ltx_p" id="S3.T1.1.1.1.7.1.1"><span class="ltx_text" id="S3.T1.1.1.1.7.1.1.1" style="font-size:80%;">Average</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.1"><span class="ltx_text" id="S3.T1.1.2.1.1.1" style="background-color:#FAFAFA;">1B Parameters Trained on Pile</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.2.1.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.2.1.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.2.1.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.2.1.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.2.1.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.2.1.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.1"><span class="ltx_text" id="S3.T1.1.3.2.1.1" style="background-color:#FAFAFA;">No Pruning (Baseline)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.3.2.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.3.2.2.1.1">15.51</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.3.2.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.3.2.3.1.1">10.31</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.3.2.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.3.2.4.1.1">28.11</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.3.2.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.3.2.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.2.5.1.1.1">3.53</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.3.2.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.3.2.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.2.6.1.1.1">11.16</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.3.2.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.3.2.7.1.1">13.73</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.1"><span class="ltx_text" id="S3.T1.1.4.3.1.1" style="background-color:#FAFAFA;">High Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.4.3.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.4.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.2.1.1.1">18.18</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.4.3.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.4.3.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.3.1.1.1">12.75</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.4.3.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.4.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.4.1.1.1">33.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.4.3.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.4.3.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.5.1.1.1">3.36</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.4.3.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.4.3.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.6.1.1.1">10.63</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.4.3.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.4.3.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.7.1.1.1">15.62</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.1"><span class="ltx_text" id="S3.T1.1.5.4.1.1" style="background-color:#F2F2F2;">3B Parameters Trained on Pile</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.5.4.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.5.4.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.5.4.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.5.4.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.5.4.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.5.4.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.1"><span class="ltx_text" id="S3.T1.1.6.5.1.1" style="background-color:#F2F2F2;">No Pruning (Baseline)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.6.5.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.6.5.2.1.1">21.82</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.6.5.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.6.5.3.1.1">13.09</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.6.5.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.6.5.4.1.1">39.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.6.5.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.6.5.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.6.5.5.1.1.1">4.88</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.6.5.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.6.5.6.1.1">14.28</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.6.5.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.6.5.7.1.1">18.63</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.6.1"><span class="ltx_text" id="S3.T1.1.7.6.1.1" style="background-color:#F2F2F2;">High Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.7.6.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.7.6.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.2.1.1.1">25.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.7.6.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.7.6.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.3.1.1.1">16.24</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.7.6.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.7.6.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.4.1.1.1">43.32</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.7.6.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.7.6.5.1.1">2.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.7.6.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.7.6.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.6.1.1.1">15.07</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.7.6.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.7.6.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.7.6.7.1.1.1">20.67</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.7" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.8.7.1"><span class="ltx_text" id="S3.T1.1.8.7.1.1" style="background-color:#FAFAFA;">1B Parameters Trained on Dolma</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.8.7.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.8.7.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.8.7.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.8.7.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.8.7.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S3.T1.1.8.7.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.8" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.8.1"><span class="ltx_text" id="S3.T1.1.9.8.1.1" style="background-color:#FAFAFA;">No Pruning (Baseline)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.9.8.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.9.8.2.1.1">16.48</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.9.8.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.9.8.3.1.1">12.32</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.9.8.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.9.8.4.1.1">28.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.9.8.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.9.8.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.9.8.5.1.1.1">3.58</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.9.8.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.9.8.6.1.1">7.95</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.9.8.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.9.8.7.1.1">13.84</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.9" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.9.1"><span class="ltx_text" id="S3.T1.1.10.9.1.1" style="background-color:#FAFAFA;">Medium Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.10.9.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.10.9.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.9.2.1.1.1">17.98</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.10.9.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.10.9.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.9.3.1.1.1">13.03</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.10.9.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.10.9.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.9.4.1.1.1">31.87</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.10.9.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.10.9.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.9.5.1.1.1">3.44</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.10.9.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.10.9.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.9.6.1.1.1">10.41</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.10.9.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S3.T1.1.10.9.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.10.9.7.1.1.1">15.35</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.10" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.10.1"><span class="ltx_text" id="S3.T1.1.11.10.1.1" style="background-color:#F2F2F2;">3B Parameters Trained on Dolma</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.11.10.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.11.10.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.11.10.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.11.10.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.11.10.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.11.10.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.11" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.11.1"><span class="ltx_text" id="S3.T1.1.12.11.1.1" style="background-color:#F2F2F2;">No Pruning (Baseline)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.12.11.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.12.11.2.1.1">23.56</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.12.11.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.12.11.3.1.1">14.29</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.12.11.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.12.11.4.1.1">39.57</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.12.11.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.12.11.5.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.5.1.1.1">4.4</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.12.11.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.12.11.6.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.12.11.6.1.1.1">14.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S3.T1.1.12.11.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.12.11.7.1.1">19.2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.12" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.13.12.1"><span class="ltx_text" id="S3.T1.1.13.12.1.1" style="background-color:#F2F2F2;">Medium Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S3.T1.1.13.12.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.13.12.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.12.2.1.1.1">24.19</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S3.T1.1.13.12.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.13.12.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.12.3.1.1.1">16.48</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S3.T1.1.13.12.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.13.12.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.12.4.1.1.1">41.8</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S3.T1.1.13.12.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.13.12.5.1.1">3.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S3.T1.1.13.12.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.13.12.6.1.1">13.19</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S3.T1.1.13.12.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S3.T1.1.13.12.7.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.13.12.7.1.1.1">19.79</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">If a certain range of perplexities is a good heuristic for data quality, training on that perplexity-pruned subset should improve downstream performance.
We sweep across pruning selection criteria and selection rates (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S7" title="7 Full Data Pruning Settings Sweep â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">7</span></a>) and find that the best settings are to select high-perplexity samples at a 50% rate for the Pile and to select medium-perplexity samples at a 50% rate for Dolma.
We compare the most performant pruning settings to baseline models trained on the original datasets without pruning inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.T1" title="In 3.2 Perplexity-Based Data Pruning Improves Downstream Performance â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.
Across all datasets and model sizes, models pretrained on the perplexity pruned version of the dataset significantly outperform the baseline model on average.
Specifically, perplexity-based data pruning outperforms the average downstream performance of no pruning for 1B models by 1.89 and 1.51 for the Pile and Dolma respectively, and improves the performance of 3B models by 2.04 and 0.59 for the Pile and Dolma respectively.
These results suggest that the perplexity of a small model provides a strong signal of data quality for a much larger model, as training on the data selected by the small model leads to significant downstream performance improvements.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Perplexity-Based Data Pruning Improves Training Efficiency</h3>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="631" id="S3.F1.1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Average normalized task accuracy evaluated intermittently throughout pretraining for each dataset and model size investigated. Perplexity-based data pruning leads to an improvement in performance for all intermediate training steps evaluated. </figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.4">Since perplexity-based data pruning improves the final performance of models, we also investigate how pruned data affects the training dynamics of models.
Specifically, we investigate whether training on perplexity pruned data enables models to achieve the same downstream performance as models trained on the unpruned data in training fewer steps.
We plot the average downstream performance of partially trained checkpoints from the 1B baseline and perplexity pruned models inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.F1" title="In 3.3 Perplexity-Based Data Pruning Improves Training Efficiency â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.
Perplexity pruning outperforms the baseline model for all intermediate pretraining durations evaluated.
Furthermore, perplexity pruned models reach the same average normalized accuracy as the baseline models in <math alttext="1.31\times" class="ltx_math_unparsed" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1b"><mn id="S3.SS3.p1.1.m1.1.1">1.31</mn><mo id="S3.SS3.p1.1.m1.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">1.31\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">1.31 Ã—</annotation></semantics></math> and <math alttext="1.45\times" class="ltx_math_unparsed" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1b"><mn id="S3.SS3.p1.2.m2.1.1">1.45</mn><mo id="S3.SS3.p1.2.m2.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">1.45\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">1.45 Ã—</annotation></semantics></math> fewer steps for Pile 1B and 3B respectively and in <math alttext="1.29\times" class="ltx_math_unparsed" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1b"><mn id="S3.SS3.p1.3.m3.1.1">1.29</mn><mo id="S3.SS3.p1.3.m3.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1.29\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">1.29 Ã—</annotation></semantics></math> and <math alttext="1.14\times" class="ltx_math_unparsed" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1b"><mn id="S3.SS3.p1.4.m4.1.1">1.14</mn><mo id="S3.SS3.p1.4.m4.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">1.14\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">1.14 Ã—</annotation></semantics></math> fewer steps for Dolma 1B and Dolma 3B respectively.
These results demonstrate that the resulting high-quality data from perplexity-based data pruning enables faster learning which can be leveraged to achieve the same downstream performance as training on unpruned data with fewer pretraining steps.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Perplexity-Based Data Pruning for Over-Trained Models</h3>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Downstream task performance for Chinchilla Optimal and <math alttext="5\times" class="ltx_math_unparsed" display="inline" id="S3.T2.2.m1.1"><semantics id="S3.T2.2.m1.1b"><mrow id="S3.T2.2.m1.1c"><mn id="S3.T2.2.m1.1.1">5</mn><mo id="S3.T2.2.m1.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.T2.2.m1.1d">5\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.m1.1e">5 Ã—</annotation></semantics></math> over-trained data budgets.
The â€œImprovement Over Baselineâ€ column refers to the gain observed from perplexity pruning as compared to the baseline trained in the same setting.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.12.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.12.10.11.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T2.12.10.11.1.1">Pruning Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.12.10.11.1.2">Average</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.12.10.11.1.3">Improvement Over Baseline</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.12.10.12.1" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.12.10.12.1.1"><span class="ltx_text" id="S3.T2.12.10.12.1.1.1" style="background-color:#FAFAFA;">1B Parameters Trained on High Perplexity Pile</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.12.10.12.1.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.12.10.12.1.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.2.2" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S3.T2.4.2.2.3"><span class="ltx_text" id="S3.T2.4.2.2.3.1" style="background-color:#FAFAFA;">Chinchilla Optimal</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.1.1"><math alttext="15.62" class="ltx_Math" display="inline" id="S3.T2.3.1.1.1.m1.1" style="background-color:#FAFAFA;"><semantics id="S3.T2.3.1.1.1.m1.1a"><mn id="S3.T2.3.1.1.1.m1.1.1" mathbackground="#FAFAFA" xref="S3.T2.3.1.1.1.m1.1.1.cmml">15.62</mn><annotation-xml encoding="MathML-Content" id="S3.T2.3.1.1.1.m1.1b"><cn id="S3.T2.3.1.1.1.m1.1.1.cmml" type="float" xref="S3.T2.3.1.1.1.m1.1.1">15.62</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.1.1.1.m1.1c">15.62</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.1.1.1.m1.1d">15.62</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.2.2.2"><math alttext="1.89" class="ltx_Math" display="inline" id="S3.T2.4.2.2.2.m1.1" style="background-color:#FAFAFA;"><semantics id="S3.T2.4.2.2.2.m1.1a"><mn id="S3.T2.4.2.2.2.m1.1.1" mathbackground="#FAFAFA" xref="S3.T2.4.2.2.2.m1.1.1.cmml">1.89</mn><annotation-xml encoding="MathML-Content" id="S3.T2.4.2.2.2.m1.1b"><cn id="S3.T2.4.2.2.2.m1.1.1.cmml" type="float" xref="S3.T2.4.2.2.2.m1.1.1">1.89</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.2.2.2.m1.1c">1.89</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.2.2.2.m1.1d">1.89</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.7.5.5" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S3.T2.5.3.3.1">
<math alttext="5\times" class="ltx_math_unparsed" display="inline" id="S3.T2.5.3.3.1.m1.1" style="background-color:#FAFAFA;"><semantics id="S3.T2.5.3.3.1.m1.1a"><mrow id="S3.T2.5.3.3.1.m1.1b"><mn id="S3.T2.5.3.3.1.m1.1.1" mathbackground="#FAFAFA">5</mn><mo id="S3.T2.5.3.3.1.m1.1.2" lspace="0.222em" mathbackground="#FAFAFA">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.T2.5.3.3.1.m1.1c">5\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.3.3.1.m1.1d">5 Ã—</annotation></semantics></math><span class="ltx_text" id="S3.T2.5.3.3.1.1" style="background-color:#FAFAFA;"> Over-Trained</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.6.4.4.2"><math alttext="18.83" class="ltx_Math" display="inline" id="S3.T2.6.4.4.2.m1.1" style="background-color:#FAFAFA;"><semantics id="S3.T2.6.4.4.2.m1.1a"><mn id="S3.T2.6.4.4.2.m1.1.1" mathbackground="#FAFAFA" xref="S3.T2.6.4.4.2.m1.1.1.cmml">18.83</mn><annotation-xml encoding="MathML-Content" id="S3.T2.6.4.4.2.m1.1b"><cn id="S3.T2.6.4.4.2.m1.1.1.cmml" type="float" xref="S3.T2.6.4.4.2.m1.1.1">18.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.4.4.2.m1.1c">18.83</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.4.4.2.m1.1d">18.83</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T2.7.5.5.3"><math alttext="1.74" class="ltx_Math" display="inline" id="S3.T2.7.5.5.3.m1.1" style="background-color:#FAFAFA;"><semantics id="S3.T2.7.5.5.3.m1.1a"><mn id="S3.T2.7.5.5.3.m1.1.1" mathbackground="#FAFAFA" xref="S3.T2.7.5.5.3.m1.1.1.cmml">1.74</mn><annotation-xml encoding="MathML-Content" id="S3.T2.7.5.5.3.m1.1b"><cn id="S3.T2.7.5.5.3.m1.1.1.cmml" type="float" xref="S3.T2.7.5.5.3.m1.1.1">1.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.5.5.3.m1.1c">1.74</annotation><annotation encoding="application/x-llamapun" id="S3.T2.7.5.5.3.m1.1d">1.74</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.10.13.2" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.12.10.13.2.1"><span class="ltx_text" id="S3.T2.12.10.13.2.1.1" style="background-color:#F2F2F2;">1B Parameters Trained on Medium Perplexity Dolma</span></td>
<td class="ltx_td ltx_border_t" id="S3.T2.12.10.13.2.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T2.12.10.13.2.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.9.7.7" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S3.T2.9.7.7.3"><span class="ltx_text" id="S3.T2.9.7.7.3.1" style="background-color:#F2F2F2;">Chinchilla Optimal</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.8.6.6.1"><math alttext="15.35" class="ltx_Math" display="inline" id="S3.T2.8.6.6.1.m1.1" style="background-color:#F2F2F2;"><semantics id="S3.T2.8.6.6.1.m1.1a"><mn id="S3.T2.8.6.6.1.m1.1.1" mathbackground="#F2F2F2" xref="S3.T2.8.6.6.1.m1.1.1.cmml">15.35</mn><annotation-xml encoding="MathML-Content" id="S3.T2.8.6.6.1.m1.1b"><cn id="S3.T2.8.6.6.1.m1.1.1.cmml" type="float" xref="S3.T2.8.6.6.1.m1.1.1">15.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.6.6.1.m1.1c">15.35</annotation><annotation encoding="application/x-llamapun" id="S3.T2.8.6.6.1.m1.1d">15.35</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S3.T2.9.7.7.2"><math alttext="1.51" class="ltx_Math" display="inline" id="S3.T2.9.7.7.2.m1.1" style="background-color:#F2F2F2;"><semantics id="S3.T2.9.7.7.2.m1.1a"><mn id="S3.T2.9.7.7.2.m1.1.1" mathbackground="#F2F2F2" xref="S3.T2.9.7.7.2.m1.1.1.cmml">1.51</mn><annotation-xml encoding="MathML-Content" id="S3.T2.9.7.7.2.m1.1b"><cn id="S3.T2.9.7.7.2.m1.1.1.cmml" type="float" xref="S3.T2.9.7.7.2.m1.1.1">1.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.7.7.2.m1.1c">1.51</annotation><annotation encoding="application/x-llamapun" id="S3.T2.9.7.7.2.m1.1d">1.51</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.10.10" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.10.8.8.1">
<math alttext="5\times" class="ltx_math_unparsed" display="inline" id="S3.T2.10.8.8.1.m1.1" style="background-color:#F2F2F2;"><semantics id="S3.T2.10.8.8.1.m1.1a"><mrow id="S3.T2.10.8.8.1.m1.1b"><mn id="S3.T2.10.8.8.1.m1.1.1" mathbackground="#F2F2F2">5</mn><mo id="S3.T2.10.8.8.1.m1.1.2" lspace="0.222em" mathbackground="#F2F2F2">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.T2.10.8.8.1.m1.1c">5\times</annotation><annotation encoding="application/x-llamapun" id="S3.T2.10.8.8.1.m1.1d">5 Ã—</annotation></semantics></math><span class="ltx_text" id="S3.T2.10.8.8.1.1" style="background-color:#F2F2F2;"> Over-Trained</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.11.9.9.2"><math alttext="18.67" class="ltx_Math" display="inline" id="S3.T2.11.9.9.2.m1.1" style="background-color:#F2F2F2;"><semantics id="S3.T2.11.9.9.2.m1.1a"><mn id="S3.T2.11.9.9.2.m1.1.1" mathbackground="#F2F2F2" xref="S3.T2.11.9.9.2.m1.1.1.cmml">18.67</mn><annotation-xml encoding="MathML-Content" id="S3.T2.11.9.9.2.m1.1b"><cn id="S3.T2.11.9.9.2.m1.1.1.cmml" type="float" xref="S3.T2.11.9.9.2.m1.1.1">18.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.9.9.2.m1.1c">18.67</annotation><annotation encoding="application/x-llamapun" id="S3.T2.11.9.9.2.m1.1d">18.67</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.12.10.10.3"><math alttext="0.84" class="ltx_Math" display="inline" id="S3.T2.12.10.10.3.m1.1" style="background-color:#F2F2F2;"><semantics id="S3.T2.12.10.10.3.m1.1a"><mn id="S3.T2.12.10.10.3.m1.1.1" mathbackground="#F2F2F2" xref="S3.T2.12.10.10.3.m1.1.1.cmml">0.84</mn><annotation-xml encoding="MathML-Content" id="S3.T2.12.10.10.3.m1.1b"><cn id="S3.T2.12.10.10.3.m1.1.1.cmml" type="float" xref="S3.T2.12.10.10.3.m1.1.1">0.84</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.10.10.3.m1.1c">0.84</annotation><annotation encoding="application/x-llamapun" id="S3.T2.12.10.10.3.m1.1d">0.84</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">A recent trend with LLMs has been to over-train models by training them on more tokens than the Chinchilla optimal number of tokensÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib57" title="">2023</a>; Gadre etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib13" title="">2024</a>)</cite>.
As our work targets the data component of LLM pretraining, we investigate the hypothesis that over-training would be more beneficial for models trained on perplexity pruned datasets as the data is of higher quality.
We test this hypothesis by training a 1B parameter model for 130B tokens, which is <math alttext="5\times" class="ltx_math_unparsed" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1b"><mn id="S3.SS4.p1.1.m1.1.1">5</mn><mo id="S3.SS4.p1.1.m1.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">5\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">5 Ã—</annotation></semantics></math> the Chinchilla optimal number of tokens.
We evaluate the downstream performance of each over-trained model inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.T2" title="In 3.4 Perplexity-Based Data Pruning for Over-Trained Models â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
The main observation is that while the absolute gain in average downstream normalized accuracy from perplexity-based data pruning on the Pile is similar for both compute optimal and over-trained models, the gain decreases for Dolma when over-training.
On the Pile, we find that the gain from perplexity pruned data is similar in the compute optimal regime and the over-trained regime: we see a gain in average performance of 1.89 when training compute optimal and a gain of 1.74 when over-training.
On Dolma, the gain from perplexity pruned data decreases in the over-trained regime: we see a gain of 1.51 when training for a compute optimal duration but this decreases to a gain of 0.84 when over-training.
These results show that while the higher quality data resulting from perplexity-based data pruning does still lead to an improvement in downstream performance in the over-trained regime, there is not a relative increase in downstream improvement over the baseline when over-training.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Perplexity-Based Data Pruning for the Data Constrained Regime</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.4">Our experiments so far were conducted in the setting where there exists a sufficient abundance of data such that even after pruning with the desired selection rate there are enough data points to fill the desired token budget without requiring any data to be repeated.
However, there are many training settings that do not fall under this data-abundant regime.
Consequently, we evaluate how perplexity-based data pruning performs when the number of tokens is constrained, and pruning induces a greater number of repetitions of the data.
For each dataset, we vary the available data such that training for a Chinchilla optimal number of tokens requires a different number of repetitions.
Specifically, we investigate data budgets that require {<span class="ltx_text ltx_font_typewriter" id="S3.SS5.p1.4.1">0.5,1,2,4,8</span>} repetitions to reach the Chinchilla optimal<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Repeat=0.5 means that the available number of tokens is twice the training budget, i.e. the data-abundant setting</span></span></span>.
As each number of repeats refers to the total number of tokens available, for all pruning experiments the number of repetitions after pruning is actually greater by a factor of <math alttext="\frac{1}{r_{s}}" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mfrac id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mn id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">1</mn><msub id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.3.2" xref="S3.SS5.p1.1.m1.1.1.3.2.cmml">r</mi><mi id="S3.SS5.p1.1.m1.1.1.3.3" xref="S3.SS5.p1.1.m1.1.1.3.3.cmml">s</mi></msub></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><divide id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"></divide><cn id="S3.SS5.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS5.p1.1.m1.1.1.2">1</cn><apply id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS5.p1.1.m1.1.1.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3.3">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\frac{1}{r_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">divide start_ARG 1 end_ARG start_ARG italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> since we prune the available tokens according to <math alttext="r_{s}" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">r</mi><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">ğ‘Ÿ</ci><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">r_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_r start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, the selection rate.
Since all models use a selection rate of <math alttext="0.5" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><mn id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><cn id="S3.SS5.p1.3.m3.1.1.cmml" type="float" xref="S3.SS5.p1.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">0.5</annotation></semantics></math>, the models trained on the pruned data see the data for <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S3.SS5.p1.4.m4.1"><semantics id="S3.SS5.p1.4.m4.1a"><mrow id="S3.SS5.p1.4.m4.1b"><mn id="S3.SS5.p1.4.m4.1.1">2</mn><mo id="S3.SS5.p1.4.m4.1.2" lspace="0.222em">Ã—</mo></mrow><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">2\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.4.m4.1d">2 Ã—</annotation></semantics></math> more repetitions.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="249" id="S3.F2.1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Downstream task performance as a function of available dataset size.
The number of repeats denotes the number of repeats over the raw dataset necessary to achieve the Chinchilla optimal number of tokens.
Training on perplexity pruned data leads to an improvement for up to two repeats on both the Pile Dolma.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">We plot the average downstream performance as a function of the number of repetitions inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.F2" title="In 3.5 Perplexity-Based Data Pruning for the Data Constrained Regime â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.
On both the Pile and Dolma, we find that training on perplexity pruned data yields an improvement for up to two repetitions.
These results suggest that perplexity-based data pruning can still provide performance gains for some degree of data constraint.
Furthermore, our results replicate the findings of <cite class="ltx_cite ltx_citemacro_citet">Muennighoff etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib38" title="">2023</a>)</cite> that more than four repetitions yields negligible gains.
Specifically, the baseline model without pruning maintains commensurate performance for up to four repetitions.
Similarly, models trained on perplexity-pruned data maintain commensurate performance for up to two repetitions through the base data, which corresponds to four repetitions after pruning.
That training on repeated perplexity-pruned data leads to diminishing gains after four repetitions post-pruning suggests that the higher quality data resulting from pruning does not change the point for which repeating data yields diminishing improvements in performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Upstream Perplexity is not a Reliable Evaluation Metric for Data Pruning</h3>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Performance as evaluated by perplexity on a test split of the original dataset as well as average normalized task accuracy for 1 billion parameter final models trained on the Pile.
The model trained on pruned data has worse pretraining test split perplexity even though it significantly improves average downstream normalized accuracy.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.2.2.2.3">Pruning Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1">Test Set Pplx. (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T3.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">â†“</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.2.2.2">Downstream Task Avg. (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.2.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.2.m1.1a"><mo id="S3.T3.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T3.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.2.m1.1d">â†‘</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.6.6.7.1" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.6.6.7.1.1"><span class="ltx_text" id="S3.T3.6.6.7.1.1.1" style="background-color:#FAFAFA;">1B Parameters Trained on Pile</span></th>
<td class="ltx_td ltx_border_t" id="S3.T3.6.6.7.1.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.6.6.7.1.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3.3" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.3.3.3.2"><span class="ltx_text" id="S3.T3.3.3.3.2.1" style="background-color:#FAFAFA;">No Pruning (Baseline)</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T3.3.3.3.3.1" style="background-color:#FAFAFA;">7.83</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3.1"><math alttext="13.73" class="ltx_Math" display="inline" id="S3.T3.3.3.3.1.m1.1" style="background-color:#FAFAFA;"><semantics id="S3.T3.3.3.3.1.m1.1a"><mn id="S3.T3.3.3.3.1.m1.1.1" mathbackground="#FAFAFA" xref="S3.T3.3.3.3.1.m1.1.1.cmml">13.73</mn><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.1.m1.1b"><cn id="S3.T3.3.3.3.1.m1.1.1.cmml" type="float" xref="S3.T3.3.3.3.1.m1.1.1">13.73</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.1.m1.1c">13.73</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.1.m1.1d">13.73</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.4" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.4.4.2"><span class="ltx_text" id="S3.T3.4.4.4.2.1" style="background-color:#FAFAFA;">High Perplexity Selected</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.3"><span class="ltx_text" id="S3.T3.4.4.4.3.1" style="background-color:#FAFAFA;">8.51</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4.1"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.T3.4.4.4.1.1" style="background-color:#FAFAFA;">15.62</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6.8.2" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.6.6.8.2.1"><span class="ltx_text" id="S3.T3.6.6.8.2.1.1" style="background-color:#F2F2F2;">1B Parameters Trained on Dolma</span></th>
<td class="ltx_td ltx_border_t" id="S3.T3.6.6.8.2.2"></td>
<td class="ltx_td ltx_border_t" id="S3.T3.6.6.8.2.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5.5" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.5.5.2"><span class="ltx_text" id="S3.T3.5.5.5.2.1" style="background-color:#F2F2F2;">No Pruning (Baseline)</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.5.5.3.1" style="background-color:#F2F2F2;">13.53</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.5.5.1"><math alttext="13.84" class="ltx_Math" display="inline" id="S3.T3.5.5.5.1.m1.1" style="background-color:#F2F2F2;"><semantics id="S3.T3.5.5.5.1.m1.1a"><mn id="S3.T3.5.5.5.1.m1.1.1" mathbackground="#F2F2F2" xref="S3.T3.5.5.5.1.m1.1.1.cmml">13.84</mn><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.1.m1.1b"><cn id="S3.T3.5.5.5.1.m1.1.1.cmml" type="float" xref="S3.T3.5.5.5.1.m1.1.1">13.84</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.1.m1.1c">13.84</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.5.1.m1.1d">13.84</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6.6" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.6.6.6.2"><span class="ltx_text" id="S3.T3.6.6.6.2.1" style="background-color:#F2F2F2;">Medium Perplexity Selected</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.6.3"><span class="ltx_text" id="S3.T3.6.6.6.3.1" style="background-color:#F2F2F2;">14.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.6.6.6.1"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S3.T3.6.6.6.1.1" style="background-color:#F2F2F2;">15.35</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">As previous works have used the perplexity of the model on a test split of the pretraining dataset as an approximation to downstream performance, we wanted to explore how well such perplexity-based evaluations agree with downstream performance for data intervention techniques.
Pruning performs an intervention on the dataset, making models trained on the pruned dataset biased estimators of the original data distribution.
Therefore, it is unlikely that the performance on the original data distribution is a fair evaluation of model quality.
We compare the test set perplexity and average downstream performance for 1 billion parameter models trained on the original and pruned version of the Pile and Dolma inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.T3" title="In 3.6 Upstream Perplexity is not a Reliable Evaluation Metric for Data Pruning â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>.
For both the Pile and Dolma, training on perplexity pruned data significantly worsens perplexity on a test split of the pretraining data, while the average downstream performance is significantly improved.
This result suggests that test set perplexity may not always be a sound metric for data pruning work and that researchers should instead directly evaluate on downstream benchmarks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Understanding the Effects of Perplexity-Based Pruning</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we investigate how data pruning works by exploring some of the properties of perplexity-based pruning.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>How Are Reference Perplexities Distributed</h3>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="249" id="S4.F3.1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of sample perplexities as evaluated by the reference model for the Pile and Dolma. We show both the original distribution over the full dataset without pruning as well as the distribution after applying the optimal perplexity-based data pruning technique for a given dataset.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In order to better understand how perplexity-based data pruning works, we investigate the distribution of the computed reference model perplexities for each dataset.
For each dataset, we randomly sample 10% of the calculated perplexities and perform kernel density estimation to estimate the distribution of log perplexities for a given dataset.
We repeat this procedure for the optimal pruned version of the dataset.
We plot the resulting estimates of the log perplexity distribution inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S4.F3" title="In 4.1 How Are Reference Perplexities Distributed â€£ 4 Understanding the Effects of Perplexity-Based Pruning â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>.
We find that the log perplexity distribution for the Pile is multimodal and asymmetric, while for Dolma and it is unimodal and symmetric.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>How Pruning Affects Domain Composition</h3>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="332" id="S4.F4.1.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Proportion of the total dataset each domain makes up before and after pruning.
For all datasets, pruning tends to select more samples from general web domains while leaving out samples from highly specific domains.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We can also interpret the effect that perplexity-based data pruning has on a dataset by examining how pruning affects each domainâ€™s proportion of the total dataset.
We plot the pre and post-pruning domain compositions for the Pile and Dolma inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S4.F4" title="In 4.2 How Pruning Affects Domain Composition â€£ 4 Understanding the Effects of Perplexity-Based Pruning â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>.
Interestingly, for all datasets pruning increases the proportion of data coming from web-scraped domains while decreasing the proportion of data coming from highly specific technical domains such as code or scientific papers.
This trend is more pronounced in the Pile, where the proportions of Pile-CC and OpenWebText2 nearly double, while the proportions of domains such as Pubmed Central, ArXiv, and Github are all reduced by at least a factor of three.
Future work should investigate how perplexity-based pruning affects a modelâ€™s performance on downstream tasks that are in the same category as the highly pruned domains.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Classical methods for pruning text data.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">In order to improve the quality of raw web scrapes, which often contain very noisy samples, pruning via quality filtering has become a common practice.
Simple rules-based methods have been employed to prune datasets by filtering out low-quality samples according to some hand-crafted heuristic such as whether the text contains prohibited words, is predominantly English, etc.Â <cite class="ltx_cite ltx_citemacro_citep">(Bane etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib4" title="">2022</a>; Raffel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib46" title="">2020</a>; Rae etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib45" title="">2022</a>; Penedo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib44" title="">2023</a>)</cite>.
N-gram perplexity-based methods, in which an n-gram model is first trained on a high quality, curated corpus and then used to score another corpus, have also been applied to filter text dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Moore &amp; Lewis, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib34" title="">2010</a>; Axelrod, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib3" title="">2017</a>; Gao, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib14" title="">2021</a>; LaurenÃ§on etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib25" title="">2022</a>; Muennighoff etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib38" title="">2023</a>)</cite>.
Although our method also uses perplexity to prune data, it does so in a very different manner.
In n-gram perplexity pruning, perplexity is used to estimate whether new text is in distribution as compared to the currated text the n-gram was trained on, while in our model-based perplexity pruning, the reference model is trained on the same distribution of text and the perplexity is more akin to an estimate of the difficulty of an example.
In this work, the datasets we leverage already have some basic rules-based pruning applied, and as such, the method we investigate is largely complementary to these existing techniques.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Neural network based methods for pruning text data.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Recently, there has been much interest in using neural networks to compute metrics that can be used to intelligently prune datasets.
A common technique in this family of methods is using a model to sample high-quality data from large datasets based on the sampleâ€™s similarity to a curated high-quality corpus that serves as a target distributionÂ <cite class="ltx_cite ltx_citemacro_citep">(Feng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib12" title="">2022</a>; Xie etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib63" title="">2023b</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Xie etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib62" title="">2023a</a>)</cite> also consider how to use a small reference model to prune pretraining data for a much larger model, by using a small reference model to learn the optimal weighting of domain proportions to maximize the "learnability" of the resulting dataset.
Pruning based on the difficulty or loss of a sample has previously been explored for text data, but the majority of such work focuses on curating data for finetuningÂ <cite class="ltx_cite ltx_citemacro_citep">(Swayamdipta etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib54" title="">2020</a>; Attendu &amp; Corbeil, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib2" title="">2023</a>; Coleman etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib10" title="">2020</a>; Mindermann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib32" title="">2022</a>; Mekala etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib30" title="">2024</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Marion etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib29" title="">2023</a>)</cite>, however, investigate multiple model-based sample difficulty heuristics for pruning pretraining text datasets.
Although we use the same method for pruning text pretraining datasets, our analysis differs substantially as we evaluate model quality based on downstream metrics and extend our analysis to multiple different dataset compositions which enables us to conclude that the reference model can be smaller than the final model.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Data pruning on vision tasks.</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">While data pruning is becoming more and more relevant with large amounts of text data, it has also been extensively applied in the vision domainÂ <cite class="ltx_cite ltx_citemacro_citep">(Paul etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib43" title="">2021</a>; Toneva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib56" title="">2018</a>; Park etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib42" title="">2023</a>)</cite>. These works often prune data points based on their loss or gradients during trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Killamsetty etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib23" title="">2021</a>; Mirzasoleiman etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib33" title="">2020</a>)</cite>. Model-based methods have also been leveraged for image data pruningÂ <cite class="ltx_cite ltx_citemacro_citep">(Fang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib11" title="">2024</a>; Schuhmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib51" title="">2021</a>)</cite>. Note that in the literature, data pruning is also sometimes referred to as coreset selectionÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib17" title="">2022</a>)</cite>. More recently, <cite class="ltx_cite ltx_citemacro_citet">Park etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib41" title="">2022</a>)</cite> show that, somewhat surprisingly, active learningÂ <cite class="ltx_cite ltx_citemacro_citep">(Castro &amp; Nowak, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib6" title="">2008</a>)</cite> based algorithms tend to outperform most data subset selection algorithms. In the context of contrastive learning, hard-negative mining has been effective as a data pruning methodÂ <cite class="ltx_cite ltx_citemacro_citep">(Kalantidis etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib22" title="">2020</a>; Robinson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib48" title="">2020</a>; Zhang &amp; Stratos, <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib65" title="">2021</a>)</cite>.
Recently, <cite class="ltx_cite ltx_citemacro_citet">Goyal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib16" title="">2024</a>)</cite> investigated scaling laws for training on pruned data in the context of vision models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we conduct an empirical investigation of the impact that perplexity-based data pruning has on model performance.
We demonstrate that small reference models can be used to prune the data of models with up to <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">30<math alttext="\times" class="ltx_Math" display="inline" id="S6.p1.1.1.m1.1"><semantics id="S6.p1.1.1.m1.1a"><mo id="S6.p1.1.1.m1.1.1" xref="S6.p1.1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.p1.1.1.m1.1b"><times id="S6.p1.1.1.m1.1.1.cmml" xref="S6.p1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.1.m1.1d">Ã—</annotation></semantics></math></em> more parameters, leading to both significant downstream performance improvements and increased training efficiency.
We then investigate perplexity-based data pruning in two non-standard settings: the over-trained and data-constrained regimes.
We find that for both settings, training on perplexity pruned data can outperform training on unpruned data, demonstrating that perplexity-based data pruning is a widely applicable and extensible technique.
We also investigate upstream metrics for evaluating data pruning techniques and provide an example where evaluating models based on their perplexity on the test split of the pretraining dataset does not align with evaluating based on downstream model performance.
Additionally, we demonstrate that optimal pruning techniques can vary greatly for different dataset compositions.
Although we do not present a predictive theory for how pruning parameters should be selected for different datasets, we demonstrate that the optimal pruning parameters for a 1 billion parameter model can successfully transfer to 3Â billion parameter models, potentially suggesting that empirically determining the optimal pruning parameters can be done cheaply.
Our work takes a key step towards establishing perplexity-based data pruning as a primary technique in the modern data researcherâ€™s toolkit.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>
<div class="ltx_para" id="S6.SS0.SSSx1.p1">
<p class="ltx_p" id="S6.SS0.SSSx1.p1.1">There are a few people who we would like to express our deepest gratitude for the assistance they provided.
Sean Owen helped us with his encyclopedic knowledge of PySpark.
Sam Havens and Daniel King both helped advise the early stages of this work.
Brett Larsen provided feedback on the presentation of our results.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini etÂ al. (2019)</span>
<span class="ltx_bibblock">
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">MathQA: Towards interpretable math word problem solving with operation-based formalisms.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.Â  2357â€“2367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1245</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1245" title="">https://aclanthology.org/N19-1245</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Attendu &amp; Corbeil (2023)</span>
<span class="ltx_bibblock">
Jean-michel Attendu and Jean-philippe Corbeil.

</span>
<span class="ltx_bibblock">NLU on data diets: Dynamic data subset selection for NLP classification tasks.

</span>
<span class="ltx_bibblock">In Nafise SadatÂ Moosavi, Iryna Gurevych, Yufang Hou, Gyuwan Kim, YoungÂ Jin Kim, Tal Schuster, and Ameeta Agrawal (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)</em>, pp.Â  129â€“146, Toronto, Canada (Hybrid), July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.sustainlp-1.9</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.sustainlp-1.9" title="">https://aclanthology.org/2023.sustainlp-1.9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Axelrod (2017)</span>
<span class="ltx_bibblock">
Amittai Axelrod.

</span>
<span class="ltx_bibblock">Cynical selection of language model training data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1709.02279</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bane etÂ al. (2022)</span>
<span class="ltx_bibblock">
Fred Bane, CeliaÂ Soler Uguet, Wiktor StribiÅ¼ew, and Anna Zaretskaya.

</span>
<span class="ltx_bibblock">A comparison of data filtering methods for neural machine translation.

</span>
<span class="ltx_bibblock">In Janice Campbell, Stephen Larocca, Jay Marciano, Konstantin Savenkov, and Alex Yanishevsky (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)</em>, pp.Â  313â€“325, Orlando, USA, September 2022. Association for Machine Translation in the Americas.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.amta-upg.22" title="">https://aclanthology.org/2022.amta-upg.22</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, etÂ al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 34, pp.Â  7432â€“7439, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castro &amp; Nowak (2008)</span>
<span class="ltx_bibblock">
Rui Castro and Robert Nowak.

</span>
<span class="ltx_bibblock">Active learning and sampling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Foundations and Applications of Sensor Management</em>, pp.Â  177â€“200. Springer, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xiangning Chen, Chen Liang, DaÂ Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, etÂ al.

</span>
<span class="ltx_bibblock">Symbolic discovery of optimization algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark etÂ al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BoolQ: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.Â  2924â€“2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1300</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1300" title="">https://aclanthology.org/N19-1300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark etÂ al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv:1803.05457v1</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coleman etÂ al. (2020)</span>
<span class="ltx_bibblock">
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Selection via proxy: Efficient data selection for deep learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HJg2b0VYDr" title="">https://openreview.net/forum?id=HJg2b0VYDr</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Alex Fang, AlbinÂ Madappally Jose, Amit Jain, Ludwig Schmidt, AlexanderÂ T Toshev, and Vaishaal Shankar.

</span>
<span class="ltx_bibblock">Data filtering networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=KAk6ngZ09F" title="">https://openreview.net/forum?id=KAk6ngZ09F</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yukun Feng, Patrick Xia, Benjamin VanÂ Durme, and JoÃ£o Sedoc.

</span>
<span class="ltx_bibblock">Automatic document selection for efficient encoder pretraining.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pp.Â  9522â€“9530, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.emnlp-main.647</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.647" title="">https://aclanthology.org/2022.emnlp-main.647</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre etÂ al. (2024)</span>
<span class="ltx_bibblock">
SamirÂ Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, etÂ al.

</span>
<span class="ltx_bibblock">Language models scale reliably with over-training and on downstream tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2403.08540</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao (2021)</span>
<span class="ltx_bibblock">
Leo Gao.

</span>
<span class="ltx_bibblock">An empirical exploration in quality filtering of text data, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2020)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2024)</span>
<span class="ltx_bibblock">
Sachin Goyal, Pratyush Maini, ZacharyÂ C Lipton, Aditi Raghunathan, and JÂ Zico Kolter.

</span>
<span class="ltx_bibblock">Scaling laws for data filteringâ€“data curation cannot be compute agnostic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2404.07177</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al. (2022)</span>
<span class="ltx_bibblock">
Chengcheng Guo, BoÂ Zhao, and Yanbing Bai.

</span>
<span class="ltx_bibblock">Deepcore: A comprehensive library for coreset selection in deep learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Conference on Database and Expert Systems Applications</em>, pp.Â  181â€“195. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks etÂ al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Conference on Learning Representations</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=d7KBjmI3GmQ" title="">https://openreview.net/forum?id=d7KBjmI3GmQ</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego deÂ LasÂ Casas, LisaÂ Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George vanÂ den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, KarÃ©n Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre.

</span>
<span class="ltx_bibblock">An empirical analysis of compute-optimal large language model training.

</span>
<span class="ltx_bibblock">In S.Â Koyejo, S.Â Mohamed, A.Â Agarwal, D.Â Belgrave, K.Â Cho, and A.Â Oh (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, volumeÂ 35, pp.Â  30016â€“30030. Curran Associates, Inc., 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jha etÂ al. (2023)</span>
<span class="ltx_bibblock">
Aditi Jha, Sam Havens, Jeremy Dohmann, Alexander Trott, and Jacob Portes.

</span>
<span class="ltx_bibblock">LIMIT: Less is more for instruction tuning across evaluation paradigms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=QxtL4Q1enz" title="">https://openreview.net/forum?id=QxtL4Q1enz</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.

</span>
<span class="ltx_bibblock">PubMedQA: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock">In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pp.Â  2567â€“2577, Hong Kong, China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D19-1259</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D19-1259" title="">https://aclanthology.org/D19-1259</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalantidis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yannis Kalantidis, MertÂ Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus.

</span>
<span class="ltx_bibblock">Hard negative mixing for contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>, 33:21798â€“21809, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Killamsetty etÂ al. (2021)</span>
<span class="ltx_bibblock">
Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer.

</span>
<span class="ltx_bibblock">Grad-match: Gradient matching based data subset selection for efficient deep model training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Conference on Machine Learning</em>, pp.Â  5464â€“5474. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korbak etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tomasz Korbak, Kejian Shi, Angelica Chen, RasikaÂ Vinayak Bhalerao, ChristopherÂ L. Buckley, Jason Phang, SamuelÂ R. Bowman, and Ethan Perez.

</span>
<span class="ltx_bibblock">Pretraining language models with human preferences.

</span>
<span class="ltx_bibblock">In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Proceedings of Machine Learning Research</em>, pp.Â  17506â€“17533. PMLR, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/korbak23a.html" title="">https://proceedings.mlr.press/v202/korbak23a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LaurenÃ§on etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hugo LaurenÃ§on, Lucile Saulnier, Thomas Wang, Christopher Akiki, AlbertÂ Villanova del Moral, TevenÂ Le Scao, LeandroÂ Von Werra, Chenghao Mou, EduardoÂ GonzÃ¡lez Ponferrada, Huu Nguyen, JÃ¶rg Frohberg, Mario Å aÅ¡ko, Quentin Lhoest, Angelina McMillan-Major, GÃ©rard Dupont, Stella Biderman, Anna Rogers, LoubnaÂ Ben allal, FrancescoÂ De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier deÂ la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, ManuelÂ Romero MuÃ±oz, Jian Zhu, DanielÂ Van Strien, Zaid Alyafeai, Khalid Almubarak, VuÂ Minh Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, PedroÂ Ortiz Suarez, Aaron Gokaslan, Shamik Bose, DavidÂ Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite.

</span>
<span class="ltx_bibblock">The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=UoEw6KigkUn" title="">https://openreview.net/forum?id=UoEw6KigkUn</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levesque etÂ al. (2012)</span>
<span class="ltx_bibblock">
Hector Levesque, Ernest Davis, and Leora Morgenstern.

</span>
<span class="ltx_bibblock">The winograd schema challenge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</em>. Citeseer, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis.

</span>
<span class="ltx_bibblock">Self-alignment with instruction backtranslation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=1oijHJBRsT" title="">https://openreview.net/forum?id=1oijHJBRsT</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.

</span>
<span class="ltx_bibblock">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning.

</span>
<span class="ltx_bibblock">In Christian Bessiere (ed.), <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</em>, pp.Â  3622â€“3628. ijcai.org, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.24963/IJCAI.2020/501</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.24963/ijcai.2020/501" title="">https://doi.org/10.24963/ijcai.2020/501</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marion etÂ al. (2023)</span>
<span class="ltx_bibblock">
Max Marion, Ahmet ÃœstÃ¼n, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker.

</span>
<span class="ltx_bibblock">When less is more: Investigating data pruning for pretraining LLMs at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">NeurIPS Workshop on Attributing Model Behavior at Scale</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=XUIYn3jo5T" title="">https://openreview.net/forum?id=XUIYn3jo5T</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mekala etÂ al. (2024)</span>
<span class="ltx_bibblock">
Dheeraj Mekala, Alex Nguyen, and Jingbo Shang.

</span>
<span class="ltx_bibblock">Smaller language models are capable of selecting instruction-tuning training data for larger language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2402.10430</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov etÂ al. (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Conference on Empirical Methods in Natural Language Processing</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:52183757" title="">https://api.semanticscholar.org/CorpusID:52183757</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mindermann etÂ al. (2022)</span>
<span class="ltx_bibblock">
SÃ¶ren Mindermann, JanÂ M Brauner, MuhammedÂ T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt HÃ¶ltgen, AidanÂ N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal.

</span>
<span class="ltx_bibblock">Prioritized training on points that are learnable, worth learning, and not yet learnt.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, LeÂ Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 39th International Conference on Machine Learning</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">Proceedings of Machine Learning Research</em>, pp.Â  15630â€“15649. PMLR, 17â€“23 Jul 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v162/mindermann22a.html" title="">https://proceedings.mlr.press/v162/mindermann22a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirzasoleiman etÂ al. (2020)</span>
<span class="ltx_bibblock">
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.

</span>
<span class="ltx_bibblock">Coresets for data-efficient training of machine learning models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Conference on Machine Learning</em>, pp.Â  6950â€“6960. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moore &amp; Lewis (2010)</span>
<span class="ltx_bibblock">
RobertÂ C. Moore and William Lewis.

</span>
<span class="ltx_bibblock">Intelligent selection of language model training data.

</span>
<span class="ltx_bibblock">In Jan HajiÄ, Sandra Carberry, Stephen Clark, and Joakim Nivre (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the ACL 2010 Conference Short Papers</em>, pp.Â  220â€“224, Uppsala, Sweden, July 2010. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P10-2041" title="">https://aclanthology.org/P10-2041</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML (2023a)</span>
<span class="ltx_bibblock">
MosaicML.

</span>
<span class="ltx_bibblock">Llm evaluation scores, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mosaicml.com/llm-evaluation" title="">https://www.mosaicml.com/llm-evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML (2023b)</span>
<span class="ltx_bibblock">
MosaicML.

</span>
<span class="ltx_bibblock">Llm foundry.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mosaicml/llm-foundry" title="">https://github.com/mosaicml/llm-foundry</a>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML (2023c)</span>
<span class="ltx_bibblock">
MosaicML.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023c.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.databricks.com/blog/mpt-7b" title="">https://www.databricks.com/blog/mpt-7b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff etÂ al. (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, AlexanderÂ M Rush, Boaz Barak, TevenÂ Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=j5BuTrEj35" title="">https://openreview.net/forum?id=j5BuTrEj35</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Tiktoken: A fast bpe tokeniser for use with openaiâ€™s models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/openai/tiktoken" title="">https://github.com/openai/tiktoken</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno etÂ al. (2016)</span>
<span class="ltx_bibblock">
Denis Paperno, GermÃ¡n Kruszewski, Angeliki Lazaridou, NgocÂ Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel FernÃ¡ndez.

</span>
<span class="ltx_bibblock">The LAMBADA dataset: Word prediction requiring a broad discourse context.

</span>
<span class="ltx_bibblock">In Katrin Erk and NoahÂ A. Smith (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.Â  1525â€“1534, Berlin, Germany, August 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P16-1144</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P16-1144" title="">https://aclanthology.org/P16-1144</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al. (2022)</span>
<span class="ltx_bibblock">
Dongmin Park, Dimitris Papailiopoulos, and Kangwook Lee.

</span>
<span class="ltx_bibblock">Active learning is a strong baseline for data subset selection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Has it Trained Yet? NeurIPS 2022 Workshop</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dongmin Park, Seola Choi, Doyoung Kim, Hwanjun Song, and Jae-Gil Lee.

</span>
<span class="ltx_bibblock">Robust data pruning under label noise via maximizing re-labeling accuracy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2311.01002</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mansheej Paul, Surya Ganguli, and GintareÂ Karolina Dziugaite.

</span>
<span class="ltx_bibblock">Deep learning on a data diet: Finding important examples early in training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in Neural Information Processing Systems</em>, 34:20596â€“20607, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo etÂ al. (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon LLM: Outperforming curated corpora with web data only.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=kM5eGcdCzq" title="">https://openreview.net/forum?id=kM5eGcdCzq</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae etÂ al. (2022)</span>
<span class="ltx_bibblock">
JackÂ W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George vanÂ den Driessche, LisaÂ Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, XiangÂ Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien deÂ MassonÂ dâ€™Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego deÂ LasÂ Casas, Aurelia Guy, Chris Jones,
James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, EdÂ Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis and insights from training gopher, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Journal of Machine Learning Research</em>, 21(140):1â€“67, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" title="">http://jmlr.org/papers/v21/20-074.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar etÂ al. (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In Jian Su, Kevin Duh, and Xavier Carreras (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, pp.Â  2383â€“2392, Austin, Texas, November 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D16-1264</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D16-1264" title="">https://aclanthology.org/D16-1264</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson etÂ al. (2020)</span>
<span class="ltx_bibblock">
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka.

</span>
<span class="ltx_bibblock">Contrastive learning with hard negative samples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2010.04592</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roemmele etÂ al. (2011)</span>
<span class="ltx_bibblock">
Melissa Roemmele, CosminÂ Adrian Bejan, and AndrewÂ S. Gordon.

</span>
<span class="ltx_bibblock">Choice of plausible alternatives: An evaluation of commonsense causal reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011</em>. AAAI, 2011.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418" title="">http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi etÂ al. (2020)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, RonanÂ Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">AAAI</em>, pp.Â  8732â€“8740, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aaai.org/ojs/index.php/AAAI/article/view/6399" title="">https://aaai.org/ojs/index.php/AAAI/article/view/6399</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann etÂ al. (2021)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2111.02114</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soldaini etÂ al. (2024)</span>
<span class="ltx_bibblock">
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, AnanyaÂ Harsh Jha, Sachin Kumar, LiÂ Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, MatthewÂ E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, NoahÂ A. Smith, Hannaneh Hajishirzi, IzÂ Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo.

</span>
<span class="ltx_bibblock">Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava etÂ al. (2023)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu AwalÂ Md Shoeb, Abubakar Abid, Adam Fisch, AdamÂ R. Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, AlexanderÂ W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, AnantharamanÂ S. Iyer, AndersÂ Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas StuhlmÃ¼ller, AndrewÂ M. Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla KarakaÅŸ, B.Â Ryan Roberts, BaoÂ Sheng Loe, Barret Zoph, BartÅ‚omiej Bojanowski, Batuhan Ã–zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, BillÂ Yuchen
Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, ChristopherÂ D Manning, Christopher Potts, Cindy Ramirez, ClaraÂ E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C.Â Daniel Freeman, Daniel Khashabi, Daniel Levy, DanielÂ MoseguÃ­ GonzÃ¡lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, DimitriÂ Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, EkinÂ Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele RodolÃ , Emma Lam, Eric Chu, Eric Tang,
Erkut Erdem, Ernie Chang, EthanÂ A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, EuniceÂ Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando MartÃ­nez-Plumed, Francesca HappÃ©, Francois Chollet, Frieda Rong, Gaurav Mishra, GentaÂ Indra Winata, Gerard deÂ Melo, GermÃ¡n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, GloriaÂ Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry FrancisÂ Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, HughÂ Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, JaimeÂ FernÃ¡ndez Fisac, JamesÂ B Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba
Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, JohnÂ U. Balis, Jonathan Batchelder, Jonathan Berant, JÃ¶rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, JoshuaÂ B. Tenenbaum, JoshuaÂ S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, KoryÂ Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, LiÂ Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-ColÃ³n, Luke Metz, LÃ¼tfiÂ Kerem Senel, Maarten Bosma, Maarten Sap, MaartjeÂ Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, MariaÂ Jose Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, MatthewÂ L
Leavitt, Matthias Hagen, MÃ¡tyÃ¡s Schubert, MedinaÂ Orduna Baitemirova, Melody Arnaud, Melvin McElrath, MichaelÂ Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, MichaÅ‚ SwÄ™drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, MoÂ Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, MukundÂ Varma T, Nanyun Peng, NathanÂ Andrew Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, NitishÂ Shirish Keskar, NivedithaÂ S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo AntonioÂ Moreno Casares, Parth Doshi, Pascale Fung, PaulÂ Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, PeterÂ W Chang, Peter Eckersley, PhuÂ Mon Htut, Pinyu Hwang, Piotr MiÅ‚kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu
Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, RachelÂ Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, RaphaÃ«l MilliÃ¨re, Rhythm Garg, Richard Barnes, RifÂ A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, RonanÂ Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, RyanÂ Andrew Chi, SeungjaeÂ Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, SaifÂ M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, SamuelÂ R. Bowman, SamuelÂ Stern Schoenholz, Sanghyun Han, Sanjeev Kwatra, SarahÂ A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, ShixiangÂ Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, ShyamolimaÂ Shammie Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, SnehaÂ Priscilla Makini, Soo-Hwan Lee, Spencer
Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, ThÃ©o Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, VinayÂ Venkatesh Ramasesh, vinayÂ uday prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, YuÂ Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, ZijieÂ J. Wang, Zirui Wang, and Ziyi Wu.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Transactions on Machine Learning Research</em>, 2023.

</span>
<span class="ltx_bibblock">ISSN 2835-8856.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=uyTL5Bvosj" title="">https://openreview.net/forum?id=uyTL5Bvosj</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Swayamdipta etÂ al. (2020)</span>
<span class="ltx_bibblock">
Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, NoahÂ A. Smith, and Yejin Choi.

</span>
<span class="ltx_bibblock">Dataset cartography: Mapping and diagnosing datasets with training dynamics.

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pp.Â  9275â€“9293, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.746</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.746" title="">https://aclanthology.org/2020.emnlp-main.746</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tirumala etÂ al. (2023)</span>
<span class="ltx_bibblock">
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and AriÂ S. Morcos.

</span>
<span class="ltx_bibblock">D4: Improving LLM pretraining via document de-duplication and diversification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=CG0L2PFrb1" title="">https://openreview.net/forum?id=CG0L2PFrb1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva etÂ al. (2018)</span>
<span class="ltx_bibblock">
Mariya Toneva, Alessandro Sordoni, Remi TachetÂ des Combes, Adam Trischler, Yoshua Bengio, and GeoffreyÂ J Gordon.

</span>
<span class="ltx_bibblock">An empirical study of example forgetting during deep neural network learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:1812.05159</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh &amp; Le (2019)</span>
<span class="ltx_bibblock">
TrieuÂ H. Trinh and QuocÂ V. Le.

</span>
<span class="ltx_bibblock">A simple method for commonsense reasoning, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, ÅÂ ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.Â Guyon, U.Â Von Luxburg, S.Â Bengio, H.Â Wallach, R.Â Fergus, S.Â Vishwanathan, and R.Â Garnett (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Advances in Neural Information Processing Systems</em>, volumeÂ 30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek etÂ al. (2020)</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco GuzmÃ¡n, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting high quality monolingual datasets from web crawl data.

</span>
<span class="ltx_bibblock">In Nicoletta Calzolari, FrÃ©dÃ©ric BÃ©chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, HÃ©lÃ¨ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pp.Â  4003â€“4012, Marseille, France, May 2020. European Language Resources Association.

</span>
<span class="ltx_bibblock">ISBN 979-10-95546-34-4.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.lrec-1.494" title="">https://aclanthology.org/2020.lrec-1.494</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolfe etÂ al. (2022)</span>
<span class="ltx_bibblock">
Thom Wolfe, Lewis Tunstall, and Patrick von Platen.

</span>
<span class="ltx_bibblock">Jeopardy dataset on hugging face hub.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/jeopardy" title="">https://huggingface.co/datasets/jeopardy</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2023a)</span>
<span class="ltx_bibblock">
SangÂ Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, QuocÂ V Le, Tengyu Ma, and AdamsÂ Wei Yu.

</span>
<span class="ltx_bibblock">Doremi: Optimizing data mixtures speeds up language model pretraining.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=lXuByUeHhd" title="">https://openreview.net/forum?id=lXuByUeHhd</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2023b)</span>
<span class="ltx_bibblock">
SangÂ Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.

</span>
<span class="ltx_bibblock">Data selection for language models via importance resampling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=uPSQv0leAu" title="">https://openreview.net/forum?id=uPSQv0leAu</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers etÂ al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Stratos (2021)</span>
<span class="ltx_bibblock">
Wenzheng Zhang and Karl Stratos.

</span>
<span class="ltx_bibblock">Understanding hard negatives in noise contrastive estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2104.06245</em>, 2021.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Full Data Pruning Settings Sweep</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this section, we report the results of sweeping over different perplexity-based pruning setting configurations.
In particular, for each dataset, we first sweep over the selection criteria to determine where from the distribution of perplexities samples should be selected.
Then, using the best selection criteria, we sweep the selection rate to determine how much we should prune.</p>
</div>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Setup.</h5>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.1">We use the same training and evaluation setup as detailed inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.SS1" title="3.1 Setup â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">3.1</span></a>.
We only perform the sweep over pruning settings for 1 billion parameter final models for computational budget reasons; however, we find that the best selection criteria at the 1 billion parameter scale also confers a performance improvement at the 3 billion parameter scale, as detailed inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S3.SS2" title="3.2 Perplexity-Based Data Pruning Improves Downstream Performance â€£ 3 Experiments â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Finding the Best Selection Criteria</h3>
<figure class="ltx_table" id="S7.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>
Results from sweeping different selection criteria.
We report the average normalized accuracy for each task grouping as well as across all tasks.
While high perplexity selection is optimal for the Pile, medium perplexity selection is optimal for Dolma.
Bold results are within one standard error of the highest normalized accuracy.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.1">Pruning Method</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.1.1.2.1">
<span class="ltx_p" id="S7.T4.1.1.1.2.1.1"><span class="ltx_text" id="S7.T4.1.1.1.2.1.1.1" style="font-size:80%;">World Knowledge</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.1.1.3.1">
<span class="ltx_p" id="S7.T4.1.1.1.3.1.1"><span class="ltx_text" id="S7.T4.1.1.1.3.1.1.1" style="font-size:80%;">Common Sense Reasoning</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.1.1.4.1">
<span class="ltx_p" id="S7.T4.1.1.1.4.1.1"><span class="ltx_text" id="S7.T4.1.1.1.4.1.1.1" style="font-size:80%;">Language Understanding</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.1.1.5.1">
<span class="ltx_p" id="S7.T4.1.1.1.5.1.1"><span class="ltx_text" id="S7.T4.1.1.1.5.1.1.1" style="font-size:80%;">Symbolic Problem Solving</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.1.1.6.1">
<span class="ltx_p" id="S7.T4.1.1.1.6.1.1"><span class="ltx_text" id="S7.T4.1.1.1.6.1.1.1" style="font-size:80%;">Reading Comprehension</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T4.1.1.1.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.1.1.7.1">
<span class="ltx_p" id="S7.T4.1.1.1.7.1.1"><span class="ltx_text" id="S7.T4.1.1.1.7.1.1.1" style="font-size:80%;">Average</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T4.1.2.1" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T4.1.2.1.1"><span class="ltx_text" id="S7.T4.1.2.1.1.1" style="background-color:#FAFAFA;">1B Parameters Trained on Pile</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.2.1.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.2.1.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.2.1.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.2.1.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.2.1.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.2.1.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.3.2" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.3.2.1"><span class="ltx_text" id="S7.T4.1.3.2.1.1" style="background-color:#FAFAFA;">No Pruning (Baseline)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.3.2.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.3.2.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.3.2.2.1.1">15.51</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.3.2.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.3.2.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.3.2.3.1.1">10.31</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.3.2.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.3.2.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.3.2.4.1.1">28.11</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.3.2.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.3.2.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.3.2.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.5.1.1.1">3.53</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.3.2.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.3.2.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.3.2.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.3.2.6.1.1.1">11.16</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.3.2.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.3.2.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.3.2.7.1.1">13.73</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.4.3" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.4.3.1"><span class="ltx_text" id="S7.T4.1.4.3.1.1" style="background-color:#FAFAFA;">Low Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.4.3.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.4.3.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.4.3.2.1.1">11.14</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.4.3.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.4.3.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.4.3.3.1.1">5.76</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.4.3.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.4.3.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.4.3.4.1.1">18.66</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.4.3.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.4.3.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.4.3.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.4.3.5.1.1.1">3.54</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.4.3.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.4.3.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.4.3.6.1.1">8.72</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.4.3.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.4.3.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.4.3.7.1.1">9.56</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.5.4" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.5.4.1"><span class="ltx_text" id="S7.T4.1.5.4.1.1" style="background-color:#FAFAFA;">Medium Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.5.4.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.5.4.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.5.4.2.1.1">16.12</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.5.4.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.5.4.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.5.4.3.1.1">9.01</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.5.4.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.5.4.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.5.4.4.1.1">28.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.5.4.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.5.4.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.5.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.5.4.5.1.1.1">3.41</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.5.4.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.5.4.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.5.4.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.5.4.6.1.1.1">10.86</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.5.4.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.5.4.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.5.4.7.1.1">13.5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.6.5" style="background-color:#FAFAFA;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.6.5.1"><span class="ltx_text" id="S7.T4.1.6.5.1.1" style="background-color:#FAFAFA;">High Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.6.5.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.6.5.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.6.5.2.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.6.5.2.1.1.1">18.18</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.6.5.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.6.5.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.6.5.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.6.5.3.1.1.1">12.75</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.6.5.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.6.5.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.6.5.4.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.6.5.4.1.1.1">33.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.6.5.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.6.5.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.6.5.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.6.5.5.1.1.1">3.36</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.6.5.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.6.5.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.6.5.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.6.5.6.1.1.1">10.63</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.6.5.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.6.5.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T4.1.6.5.7.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.6.5.7.1.1.1">15.62</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.7.6" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T4.1.7.6.1"><span class="ltx_text" id="S7.T4.1.7.6.1.1" style="background-color:#F2F2F2;">1B Parameters Trained on Dolma</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.7.6.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.7.6.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.7.6.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.7.6.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.7.6.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T4.1.7.6.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.8.7" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.8.7.1"><span class="ltx_text" id="S7.T4.1.8.7.1.1" style="background-color:#F2F2F2;">No Pruning (Baseline)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.8.7.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.8.7.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.8.7.2.1.1">16.48</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.8.7.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.8.7.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.8.7.3.1.1">12.32</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.8.7.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.8.7.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.8.7.4.1.1">28.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.8.7.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.8.7.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.8.7.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.8.7.5.1.1.1">3.58</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.8.7.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.8.7.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.8.7.6.1.1">7.95</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.8.7.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.8.7.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.8.7.7.1.1">13.84</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.9.8" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.9.8.1"><span class="ltx_text" id="S7.T4.1.9.8.1.1" style="background-color:#F2F2F2;">Low Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.9.8.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.9.8.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.9.8.2.1.1">16.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.9.8.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.9.8.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.9.8.3.1.1">10.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.9.8.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.9.8.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.9.8.4.1.1">27.28</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.9.8.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.9.8.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.9.8.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.9.8.5.1.1.1">3.45</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.9.8.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.9.8.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.9.8.6.1.1">7.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.9.8.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.9.8.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.9.8.7.1.1">12.96</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.10.9" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left" id="S7.T4.1.10.9.1"><span class="ltx_text" id="S7.T4.1.10.9.1.1" style="background-color:#F2F2F2;">Medium Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.10.9.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.10.9.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.10.9.2.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.10.9.2.1.1.1">17.98</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.10.9.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.10.9.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.10.9.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.10.9.3.1.1.1">13.03</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.10.9.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.10.9.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.10.9.4.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.10.9.4.1.1.1">31.87</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.10.9.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.10.9.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.10.9.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.10.9.5.1.1.1">3.44</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.10.9.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.10.9.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.10.9.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.10.9.6.1.1.1">10.41</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T4.1.10.9.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.10.9.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.10.9.7.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.10.9.7.1.1.1">15.35</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.11.10" style="background-color:#F2F2F2;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S7.T4.1.11.10.1"><span class="ltx_text" id="S7.T4.1.11.10.1.1" style="background-color:#F2F2F2;">High Perplexity Selected</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T4.1.11.10.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.11.10.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.11.10.2.1.1">16.65</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T4.1.11.10.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.11.10.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.11.10.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.11.10.3.1.1.1">13.12</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T4.1.11.10.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.11.10.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.11.10.4.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.11.10.4.1.1.1">31.14</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T4.1.11.10.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.11.10.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.11.10.5.1.1">3.15</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T4.1.11.10.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.11.10.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.11.10.6.1.1">8.55</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T4.1.11.10.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T4.1.11.10.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T4.1.11.10.7.1.1">14.52</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">For each dataset, we first sweep the selection criteria while keeping the selection rate fixed at 50%.
We report the performance of each selection criteria inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S7.T4" title="In 7.1 Finding the Best Selection Criteria â€£ 7 Full Data Pruning Settings Sweep â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>.
We find that on the Pile high perplexity selection works the best and on Dolma medium perplexity selection works the best, improving the average downstream performance by 1.89 and 1.51 respectively.
An important observation from the sweep is that the best selection criteria from one dataset does not transfer to another dataset and may actually degrade performance compared to the baseline.
Although medium-perplexity selection is the best method on Dolma, selecting medium-perplexity samples on the Pile leads to a decrease in the average downstream performance of 0.23 as compared to not performing pruning.
These results inform us that high and medium perplexity selection are the optimal selection criteria for the Pile and Dolma respectively, and that the optimal selection criteria does not necessarily transfer between datasets with different domain compositions.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Finding the Best Selection Rate</h3>
<figure class="ltx_table" id="S7.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>
Results from sweeping different selection rates.
We report the average normalized accuracy for each task grouping as well as across all tasks.
Bold results are within one standard error of the highest normalized accuracy.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S7.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S7.T5.1.1.1.1">Pruning Method</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T5.1.1.1.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.1.1.2.1">
<span class="ltx_p" id="S7.T5.1.1.1.2.1.1"><span class="ltx_text" id="S7.T5.1.1.1.2.1.1.1" style="font-size:80%;">World Knowledge</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T5.1.1.1.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.1.1.3.1">
<span class="ltx_p" id="S7.T5.1.1.1.3.1.1"><span class="ltx_text" id="S7.T5.1.1.1.3.1.1.1" style="font-size:80%;">Common Sense Reasoning</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T5.1.1.1.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.1.1.4.1">
<span class="ltx_p" id="S7.T5.1.1.1.4.1.1"><span class="ltx_text" id="S7.T5.1.1.1.4.1.1.1" style="font-size:80%;">Language Understanding</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T5.1.1.1.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.1.1.5.1">
<span class="ltx_p" id="S7.T5.1.1.1.5.1.1"><span class="ltx_text" id="S7.T5.1.1.1.5.1.1.1" style="font-size:80%;">Symbolic Problem Solving</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T5.1.1.1.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.1.1.6.1">
<span class="ltx_p" id="S7.T5.1.1.1.6.1.1"><span class="ltx_text" id="S7.T5.1.1.1.6.1.1.1" style="font-size:80%;">Reading Comprehension</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt" id="S7.T5.1.1.1.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.1.1.7.1">
<span class="ltx_p" id="S7.T5.1.1.1.7.1.1"><span class="ltx_text" id="S7.T5.1.1.1.7.1.1.1" style="font-size:80%;">Average</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T5.1.2.1" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.2.1.1"><span class="ltx_text" id="S7.T5.1.2.1.1.1" style="background-color:#FAFAFA;">1B Parameters Trained on Pile</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.2.1.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.2.1.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.2.1.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.2.1.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.2.1.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.2.1.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.3.2" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.3.2.1"><span class="ltx_text" id="S7.T5.1.3.2.1.1" style="background-color:#FAFAFA;">25% Selection Rate</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.3.2.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.2.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.3.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.3.2.2.1.1.1">18.21</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.3.2.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.2.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.3.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.3.2.3.1.1.1">12.88</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.3.2.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.2.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.3.2.4.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.3.2.4.1.1.1">34.44</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.3.2.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.2.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.3.2.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.3.2.5.1.1.1">3.73</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.3.2.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.2.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.3.2.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.3.2.6.1.1.1">9.44</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.3.2.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.2.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.3.2.7.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.3.2.7.1.1.1">15.74</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.4.3" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.4.3.1"><span class="ltx_text" id="S7.T5.1.4.3.1.1" style="background-color:#FAFAFA;">50% Selection Rate</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.4.3.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.3.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.4.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.4.3.2.1.1.1">18.18</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.4.3.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.3.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.4.3.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.4.3.3.1.1.1">12.75</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.4.3.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.3.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.4.3.4.1.1">33.2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.4.3.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.3.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.4.3.5.1.1">3.36</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.4.3.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.3.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.4.3.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.4.3.6.1.1.1">10.63</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.4.3.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.3.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.4.3.7.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.4.3.7.1.1.1">15.62</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.5.4" style="background-color:#FAFAFA;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.5.4.1"><span class="ltx_text" id="S7.T5.1.5.4.1.1" style="background-color:#FAFAFA;">75% Selection Rate</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.5.4.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.5.4.2.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.5.4.2.1.1">17.08</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.5.4.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.5.4.3.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.5.4.3.1.1">10.11</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.5.4.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.5.4.4.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.5.4.4.1.1">31.37</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.5.4.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.5.4.5.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.5.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.5.4.5.1.1.1">3.81</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.5.4.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.5.4.6.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.5.4.6.1.1">9.02</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.5.4.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.5.4.7.1" style="background-color:#FAFAFA;">
<span class="ltx_p" id="S7.T5.1.5.4.7.1.1">14.28</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.6.5" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.6.5.1"><span class="ltx_text" id="S7.T5.1.6.5.1.1" style="background-color:#F2F2F2;">1B Parameters Trained on Dolma</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.6.5.2" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.6.5.3" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.6.5.4" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.6.5.5" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.6.5.6" style="width:28.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S7.T5.1.6.5.7" style="width:28.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.7.6" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.7.6.1"><span class="ltx_text" id="S7.T5.1.7.6.1.1" style="background-color:#F2F2F2;">25% Selection Rate</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.7.6.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.6.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.7.6.2.1.1">17.94</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.7.6.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.6.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.7.6.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.7.6.3.1.1.1">12.16</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.7.6.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.6.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.7.6.4.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.7.6.4.1.1.1">31.63</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.7.6.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.6.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.7.6.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.7.6.5.1.1.1">3.58</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.7.6.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.6.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.7.6.6.1.1">8.91</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.7.6.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.6.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.7.6.7.1.1">14.85</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.8.7" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.8.7.1"><span class="ltx_text" id="S7.T5.1.8.7.1.1" style="background-color:#F2F2F2;">50% Selection Rate</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.8.7.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.7.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.8.7.2.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.8.7.2.1.1.1">17.98</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.8.7.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.7.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.8.7.3.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.8.7.3.1.1.1">13.03</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.8.7.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.7.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.8.7.4.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.8.7.4.1.1.1">31.87</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.8.7.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.7.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.8.7.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.8.7.5.1.1.1">3.44</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.8.7.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.7.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.8.7.6.1.1">10.41</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S7.T5.1.8.7.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.7.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.8.7.7.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.8.7.7.1.1.1">15.35</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.9.8" style="background-color:#F2F2F2;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S7.T5.1.9.8.1"><span class="ltx_text" id="S7.T5.1.9.8.1.1" style="background-color:#F2F2F2;">75% Selection Rate</span></th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T5.1.9.8.2" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.8.2.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.9.8.2.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.9.8.2.1.1.1">18.2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T5.1.9.8.3" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.8.3.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.9.8.3.1.1">11.78</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T5.1.9.8.4" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.8.4.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.9.8.4.1.1">29.96</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T5.1.9.8.5" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.8.5.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.9.8.5.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.9.8.5.1.1.1">3.32</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T5.1.9.8.6" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.8.6.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.9.8.6.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.9.8.6.1.1.1">10.82</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb" id="S7.T5.1.9.8.7" style="width:28.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.8.7.1" style="background-color:#F2F2F2;">
<span class="ltx_p" id="S7.T5.1.9.8.7.1.1">14.82</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Using the optimal selection criteria that we found for each dataset, we next investigate the best selection rate for each dataset.
We investigate three different selection rates: 25%, 50%, and 75%.
We present the results for each selection rate inÂ <a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#S7.T5" title="In 7.2 Finding the Best Selection Rate â€£ 7 Full Data Pruning Settings Sweep â€£ Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>.
On the Pile, we find that there is no significant difference in downstream performance for selection rates of 25% and 50%; on Dolma we find that a selection rate of 50% achieves the best average downstream performance.
For simplicity, we chose to conduct the rest of the experiments in the paper using a selection rate of 50% on both datasets.
Furthermore, we find that all the selection rates tested outperform the baseline of no data pruning as measured by average downstream performance.
This suggests that the selection criteria has a greater impact on the performance of a pruning configuration than the selection rate.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Detailed Evaluation Setup</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Jha etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib20" title="">2023</a>)</cite> also use the MosaicML evaluation gauntlet to perform evaluations in their work.
As such, with explicit permission from the authors, we exactly reproduce their text describing the tasks and tasks categories in the evaluation gauntlet. The following is from Section D of their paper:</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">The <span class="ltx_text ltx_font_bold" id="S8.p2.1.1">World Knowledge</span> category includes the following datasets:</p>
<ul class="ltx_itemize" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I1.i1.p1">
<p class="ltx_p" id="S8.I1.i1.p1.1">Jeopardy (2,117 questions that are a custom subset of the dataset originally obtained from <cite class="ltx_cite ltx_citemacro_cite">Wolfe etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib61" title="">2022</a>)</cite>)</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I1.i2.p1">
<p class="ltx_p" id="S8.I1.i2.p1.1">MMLU (14,042 four-choice multiple choice questions distributed across 57 categories <cite class="ltx_cite ltx_citemacro_cite">Hendrycks etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib18" title="">2021</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I1.i3.p1">
<p class="ltx_p" id="S8.I1.i3.p1.1">BIG-bench wikidata (20,321 questions regarding factual information pulled from wikipedia)Â <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I1.i4.p1">
<p class="ltx_p" id="S8.I1.i4.p1.1">ARC easy (2,376 easy multiple choice middle school science questions) <cite class="ltx_cite ltx_citemacro_cite">Clark etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib9" title="">2018</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I1.i5.p1">
<p class="ltx_p" id="S8.I1.i5.p1.1">ARC challenge (1,172 hard multiple choice science questions) <cite class="ltx_cite ltx_citemacro_cite">Clark etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib9" title="">2018</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I1.i6.p1">
<p class="ltx_p" id="S8.I1.i6.p1.1">BIG-bench: misconceptions (219 true or false questions regarding common misconceptions) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.1">The <span class="ltx_text ltx_font_bold" id="S8.p3.1.1">Commonsense Reasoning </span>category loosely assesses a modelâ€™s ability to do basic reasoning tasks that require commonsense knowledge of objects, their properties, and their behavior. It includes the following datasets:</p>
<ul class="ltx_itemize" id="S8.I2">
<li class="ltx_item" id="S8.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I2.i1.p1">
<p class="ltx_p" id="S8.I2.i1.p1.1">BIG-bench Strategy QA (2,289 very eclectic yes/no questions on a wide range of commonsense subjects e.g â€œCan fish get Tonsilitis?â€)<cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I2.i2.p1">
<p class="ltx_p" id="S8.I2.i2.p1.1">BIG-bench Strange Stories (174 short stories followed by questions about the characters)<cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I2.i3.p1">
<p class="ltx_p" id="S8.I2.i3.p1.1">BIG-bench Novel Concepts (32 find-the-common-concept problems)<cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I2.i4.p1">
<p class="ltx_p" id="S8.I2.i4.p1.1">COPA (100 cause/effect multiple choice questions) <cite class="ltx_cite ltx_citemacro_cite">Roemmele etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib49" title="">2011</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I2.i5.p1">
<p class="ltx_p" id="S8.I2.i5.p1.1">PIQA (1,838 commonsense physical intuition 2-choice questions) <cite class="ltx_cite ltx_citemacro_cite">Bisk etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib5" title="">2020</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I2.i6.p1">
<p class="ltx_p" id="S8.I2.i6.p1.1">OpenBook QA (500 questions that rely on basic physical and scientific intuition about common objects and entities) <cite class="ltx_cite ltx_citemacro_cite">Mihaylov etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib31" title="">2018</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.1"><span class="ltx_text ltx_font_bold" id="S8.p4.1.1">Language Understanding</span> tasks evaluate the modelâ€™s ability to understand the structure and properties of languages, and include the following datasets:</p>
<ul class="ltx_itemize" id="S8.I3">
<li class="ltx_item" id="S8.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i1.p1">
<p class="ltx_p" id="S8.I3.i1.p1.1">LAMBADA (6,153 passages take from books - we use the formatting adopted by OpenAIâ€™s version)<cite class="ltx_cite ltx_citemacro_cite">Paperno etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib40" title="">2016</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i2.p1">
<p class="ltx_p" id="S8.I3.i2.p1.1">HellaSwag (10,042 multiple choice scenarios in which the model is prompted with a scenario and choose the most likely conclusion to the scenario from four possible options)<cite class="ltx_cite ltx_citemacro_cite">Zellers etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib64" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i3.p1">
<p class="ltx_p" id="S8.I3.i3.p1.1">Winograd Schema Challenge (273 scenarios in which the model must use semantics to correctly resolve the anaphora in a sentence. The Eval Gauntlet uses the partial evaluation technique technique introduced in <cite class="ltx_cite ltx_citemacro_cite">Trinh &amp; Le (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib58" title="">2019</a>)</cite>) <cite class="ltx_cite ltx_citemacro_cite">Levesque etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib26" title="">2012</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i4.p1">
<p class="ltx_p" id="S8.I3.i4.p1.1">Winogrande (1,267 scenarios in which two possible beginnings of a sentence are presented along with a single ending) <cite class="ltx_cite ltx_citemacro_cite">Sakaguchi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib50" title="">2020</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i5.p1">
<p class="ltx_p" id="S8.I3.i5.p1.1">BIG-bench language identification (10,000 questions on multiple choice language identification) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i6.p1">
<p class="ltx_p" id="S8.I3.i6.p1.1">BIG-bench conceptual combinations (103 questions using made up words) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I3.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I3.i7.p1">
<p class="ltx_p" id="S8.I3.i7.p1.1">BIG-bench conlang translation (164 example problems in which the model is given translations of simple sentences between English and some fake constructed language) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.1"><span class="ltx_text ltx_font_bold" id="S8.p5.1.1">Symbolic problem solving</span> tasks test the modelâ€™s ability to solve a diverse range of symbolic tasks including arithmetic, logical reasoning, algorithms, and algebra. These datasets include:</p>
<ul class="ltx_itemize" id="S8.I4">
<li class="ltx_item" id="S8.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i1.p1">
<p class="ltx_p" id="S8.I4.i1.p1.1">BIG-bench elementary math QA (38,160 four-choice multiple choice arithmetic word problems) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i2.p1">
<p class="ltx_p" id="S8.I4.i2.p1.1">BIG-bench dyck languages (1000 complete-the-sequence questions) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i3.p1">
<p class="ltx_p" id="S8.I4.i3.p1.1">BIG-bench algorithms (1,320 questions) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i4.p1">
<p class="ltx_p" id="S8.I4.i4.p1.1">BIG-bench logical deduction (1500 four-choice multiple choice questions relating to relative ordering of objects) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i5.p1">
<p class="ltx_p" id="S8.I4.i5.p1.1">BIG-bench operators (210 questions involving mathematical operators) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i6.p1">
<p class="ltx_p" id="S8.I4.i6.p1.1">BIG-bench repeat copy logic (32 samples in which the model is required to follow some instructions for copying words/symbols)</p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i7.p1">
<p class="ltx_p" id="S8.I4.i7.p1.1">Simple arithmetic with spaces (1000 arithmetic problems consisting of up to 3 operations and using numbers of up to 3 digits, developed by MosaicML)</p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i8.p1">
<p class="ltx_p" id="S8.I4.i8.p1.1">Simple arithmetic without spaces (1000 arithmetic problems consisting of up to 3 operations and using numbers of up to 3 digits, developed by MosaicML)</p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i9.p1">
<p class="ltx_p" id="S8.I4.i9.p1.1">Math QA (2,983 four-choice multiple choice math word problems) <cite class="ltx_cite ltx_citemacro_cite">Amini etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib1" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I4.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I4.i10.p1">
<p class="ltx_p" id="S8.I4.i10.p1.1">LogiQA (651 four-logical word problems) <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib28" title="">2020</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S8.p6">
<p class="ltx_p" id="S8.p6.1">The <span class="ltx_text ltx_font_bold" id="S8.p6.1.1">Reading comprehension</span> benchmarks test a modelâ€™s ability to answer questions based on the information in a passage of text. The datasets include:</p>
</div>
<div class="ltx_para" id="S8.p7">
<ul class="ltx_itemize" id="S8.I5">
<li class="ltx_item" id="S8.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I5.i1.p1">
<p class="ltx_p" id="S8.I5.i1.p1.1">BIG-bench Understanding fables (189 short stories) <cite class="ltx_cite ltx_citemacro_cite">Srivastava etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib53" title="">2023</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I5.i2.p1">
<p class="ltx_p" id="S8.I5.i2.p1.1">Pubmed QA Labeled (1000 hand-labeled medical documents followed by a related question for which the model must respond yes/no/maybe) <cite class="ltx_cite ltx_citemacro_cite">Jin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib21" title="">2019</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I5.i3.p1">
<p class="ltx_p" id="S8.I5.i3.p1.1">SQuAD (10,570 short documents followed by a related question. The model is expected to output the exact correct answer) <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib47" title="">2016</a>)</cite></p>
</div>
</li>
<li class="ltx_item" id="S8.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S8.I5.i4.p1">
<p class="ltx_p" id="S8.I5.i4.p1.1">BoolQ (3,270 short passages on a diverse range of subjects followed by a yes/no questions) <cite class="ltx_cite ltx_citemacro_cite">Clark etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.20541v1#bib.bib8" title="">2019</a>)</cite></p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Evaluation Procedure</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">To compute model performance on the above datasets, the Eval Gauntlet uses one of the following three ICL metrics for each dataset (from MosaicMLâ€™s composer library).</p>
</div>
<div class="ltx_para" id="S8.SS1.p2">
<ol class="ltx_enumerate" id="S8.I6">
<li class="ltx_item" id="S8.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S8.I6.i1.p1">
<p class="ltx_p" id="S8.I6.i1.p1.1"><a class="ltx_ref ltx_href" href="https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.metrics.InContextLearningQAAccuracy.html" title="">InContextLearningQAAccuracy</a>: This metric uses the query, the corresponding correct answer and a list of alternative answers to measure a modelâ€™s prediction. If the modelâ€™s response conditioned on the query starts with either the correct answer or with one of the alternative answers, it is considered correct. This is used for question-answering tasks such as TriviaQA.</p>
</div>
</li>
<li class="ltx_item" id="S8.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S8.I6.i2.p1">
<p class="ltx_p" id="S8.I6.i2.p1.1"><a class="ltx_ref ltx_href" href="https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.metrics.InContextLearningLMAccuracy.html" title="">InContextLearningLMAccuracy</a>: This metric tests a modelâ€™s ability to output a precise set of tokens. A modelâ€™s output conditioned on a given query is judged to be correct only if the modelâ€™s highest probability tokens match the correct sequence of tokens. This is used for language modeling tasks such as LAMBADA.</p>
</div>
</li>
<li class="ltx_item" id="S8.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S8.I6.i3.p1">
<p class="ltx_p" id="S8.I6.i3.p1.1"><a class="ltx_ref ltx_href" href="https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.metrics.InContextLearningMultipleChoiceAccuracy.html" title="">InContextLearningMultipleChoiceAccuracy</a>: This metric is used for testing a modelâ€™s ability to answer multiple choice questions accurately. It compares the respective perplexity of the query prepended to each of the possible choices, according to the model. If the query-choice pair with the lowest per token perplexity is indeed the correct choice, then the modelâ€™s output is judged to be correct. This is used for multiple choice tasks such as HellaSwag, Winograd etc.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
</article>