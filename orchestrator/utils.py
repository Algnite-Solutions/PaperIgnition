import paper_pull
#from backend.index_service import index_papers
from AIgnite.data.docset import DocSetList, DocSet
import httpx
import sys

def check_connection_health(api_url, timeout=5.0):
    try:
        response = httpx.get(f"{api_url}/health", timeout=timeout)
        if response.status_code == 200:
            data = response.json()
            if data.get("status") == "healthy" and data.get("indexer_ready"):
                print("âœ… Connection health check passed")
                return True
            elif data.get("status") == "healthy" and not data.get("indexer_ready"):
                print("âŒ API server is not ready: ", data)
                return "not_ready"
            else:
                print("âŒ API server unhealthy: ", data)
        else:
            print(f"âŒ Health check failed: {response.text}")
    except Exception as e:
        print(f"âŒ Error: API server not accessible at {api_url}")
        print(f"Error details: {str(e)}")
    return False

def index_papers_via_api(papers, api_url, store_images=False, keep_temp_image=False):
    """
    Index papers using the /index_papers/ endpoint.
    
    Args:
        papers: List of DocSet objects
        api_url: API base URL
        store_images: Whether to store images to MinIO (default: False)
        keep_temp_image: If False, delete temporary image files after successful storage (default: False)
    """
    docset_list = DocSetList(docsets=papers)

    # Wrap in the expected format: {"docsets": DocSetList, "store_images": bool}
    data = {
        "docsets": docset_list.model_dump(),  # This creates {"docsets": [...]}
        "store_images": False
    }

    print(f"ğŸ“¤ Sending {len(papers)} papers to index...")
    if papers:
        print(f"ğŸ“‹ First paper: {papers[0].doc_id} - {papers[0].title[:50]}...")

    try:
        response = httpx.post(f"{api_url}/index_papers/", json=data, timeout=6000.0)
        response.raise_for_status()
        print("Indexing response:", response.json())
    except Exception as e:
        print("Failed to index papers:", e)

def search_papers_via_api(api_url, query, search_strategy='tf-idf', similarity_cutoff=0.1, filters=None):
    """Search papers using the /find_similar/ endpoint for a single query.
    Returns a list of DocSet objects corresponding to the results.
    """
    # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
    health = check_connection_health(api_url, timeout=5.0)
    if not health:
        print(f"âŒ æœç´¢æœåŠ¡ {api_url} ä¸å¯ç”¨ï¼Œè·³è¿‡æŸ¥è¯¢ '{query}'")
        return []
    
    # æ ¹æ®æ–°çš„APIç»“æ„æ„å»ºpayload
    payload = {
        "query": query,
        "top_k": 1,
        "similarity_cutoff": similarity_cutoff,
        "search_strategies": [(search_strategy, 1.5)],  # æ–°APIä½¿ç”¨å…ƒç»„æ ¼å¼ (strategy, threshold)
        "filters": filters,
        "result_include_types": ["metadata", "text_chunks"]  # ä½¿ç”¨æ­£ç¡®çš„ç»“æœç±»å‹
    }
    try:
        response = httpx.post(f"{api_url}/find_similar/", json=payload, timeout=30.0)
        response.raise_for_status()
        results = response.json()
        print(f"\nResults for query '{query}' (strategy: {search_strategy}, cutoff: {similarity_cutoff}):")
        docsets = []
        for r in results:
            # Create DocSet instance (handle missing fields gracefully)
            try:
                # æå–metadataä¸­çš„ä¿¡æ¯
                metadata = r.get('metadata', {})
                
                # å¤„ç†chunksæ•°æ®ï¼Œç¡®ä¿ç¬¦åˆDocSetå®šä¹‰
                def process_text_chunks(chunks_data):
                    """å¤„ç†text_chunksæ•°æ®ï¼Œè½¬æ¢ä¸ºç¬¦åˆDocSetå®šä¹‰çš„æ ¼å¼"""
                    if not chunks_data:
                        return []
                    
                    processed_chunks = []
                    for chunk in chunks_data:
                        if isinstance(chunk, dict):
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼
                            if 'id' in chunk and 'type' in chunk and 'text' in chunk:
                                processed_chunks.append(chunk)
                            elif 'chunk_id' in chunk and 'text_content' in chunk:
                                # è½¬æ¢APIæ ¼å¼åˆ°DocSetæ ¼å¼
                                converted_chunk = {
                                    'id': chunk['chunk_id'],
                                    'type': 'text',
                                    'text': chunk['text_content']
                                }
                                processed_chunks.append(converted_chunk)
                            else:
                                # è·³è¿‡æ— æ•ˆçš„chunk
                                print(f"Warning: Skipping invalid text chunk: {chunk}")
                        else:
                            print(f"Warning: Skipping non-dict text chunk: {chunk}")
                    return processed_chunks
                
                # ä¸ºç¼ºå¤±çš„å¿…éœ€å­—æ®µæä¾›é»˜è®¤å€¼ï¼Œç¡®ä¿ç¬¦åˆDocSetå®šä¹‰
                docset_data = {
                    'doc_id': r.get('doc_id'),
                    'title': metadata.get('title', 'Unknown Title'),
                    'authors': metadata.get('authors', []),
                    'categories': metadata.get('categories', []),
                    'published_date': metadata.get('published_date', ''),
                    'abstract': metadata.get('abstract', ''),
                    'pdf_path': metadata.get('pdf_path', ''),
                    'HTML_path': metadata.get('HTML_path'),
                    'text_chunks': process_text_chunks(r.get('text_chunks', [])),
                    'figure_chunks': [],
                    'table_chunks': [],
                    'metadata': metadata,
                    'comments': metadata.get('comments', '')
                }
                
                docset = DocSet(**docset_data)
                print(f"[DocSet] Created with title: {docset.title}")
                docsets.append(docset)
            except Exception as e:
                print(f"Failed to create DocSet for {r.get('doc_id')}: {e}")
                continue
        return docsets
    except httpx.TimeoutException:
        print(f"âŒ æœç´¢æŸ¥è¯¢ '{query}' è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡å™¨çŠ¶æ€")
        return []
    except httpx.ConnectError:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœç´¢æœåŠ¡ {api_url}ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ")
        return []
    except httpx.HTTPStatusError as e:
        print(f"âŒ æœç´¢æŸ¥è¯¢ '{query}' è¿”å›é”™è¯¯çŠ¶æ€ç : {e.response.status_code}")
        print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return []
    except Exception as e:
        print(f"âŒ æœç´¢æŸ¥è¯¢ '{query}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

def save_recommendations(username, papers, api_url):
    for paper in papers:
        print(paper)
        data = {
            "username": username,
            "paper_id": paper.get("paper_id"),
            "title": paper.get("title", ""),
            "authors": paper.get("authors", ""),
            "abstract": paper.get("abstract", ""),
            "url": paper.get("url", ""),
            "content": paper.get("content", ""),
            "blog": paper.get("blog", ""),
            "recommendation_reason": paper.get("recommendation_reason", ""),
            "relevance_score": paper.get("relevance_score", None),
            "blog_abs": paper.get("blog_abs", ""),
            "blog_title": paper.get("blog_title", ""),
            "submitted": paper.get("submitted", ""),
            "comment": paper.get("comment", ""),
        }
        try:
            resp = httpx.post(
                f"{api_url}/api/papers/recommend",
                params={"username": username},
                json=data,
                timeout=100.0
            )
            if resp.status_code == 201:
                print(f"âœ… æ¨èå†™å…¥æˆåŠŸ: {paper.get('paper_id')}")
            else:
                print(f"âŒ æ¨èå†™å…¥å¤±è´¥: {paper.get('paper_id')}ï¼ŒåŸå› : {resp.text}")
        except Exception as e:
            print(f"âŒ æ¨èå†™å…¥å¼‚å¸¸: {paper.get('paper_id')}ï¼Œé”™è¯¯: {e}")

def fetch_daily_papers(index_api_url: str, config, job_logger):
    """
    Fetch daily papers and return a list of DocSet objects.
    This function is a placeholder and should be replaced with the actual implementation.
    """
    # 1. Check connection health before indexing
    health = check_connection_health(index_api_url)
    if health == "not_ready" or not health:
        print("Attempting to initialize index service...")
        # Re-check health after initialization
        if not check_connection_health(index_api_url):
            print("Exiting due to failed health check after initialization.")
            sys.exit(1)

    papers = paper_pull.fetch_daily_papers()
    #papers=paper_pull.dummy_paper_fetch("./orchestrator/jsons")
    print(f"Fetched {len(papers)} papers.")

    # 2. Index papers
    index_papers_via_api(papers, index_api_url, store_images=True, keep_temp_image=True)
    
    # 3. Return the papers for further processing
    return papers
