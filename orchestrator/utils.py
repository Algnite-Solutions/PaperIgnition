import paper_pull
from generate_blog import run_batch_generation, run_batch_generation_abs, run_batch_generation_title
#from backend.index_service import index_papers
import requests
import os
from backend.app.db_utils import load_config as load_backend_config
from AIgnite.data.docset import DocSetList, DocSet
import httpx
import sys
import asyncio

def check_connection_health(api_url, timeout=5.0):
    try:
        response = httpx.get(f"{api_url}/health", timeout=timeout)
        if response.status_code == 200:
            data = response.json()
            if data.get("status") == "healthy" and data.get("indexer_ready"):
                print("âœ… Connection health check passed")
                return True
            elif data.get("status") == "healthy" and not data.get("indexer_ready"):
                print("âŒ API server is not ready: ", data)
                return "not_ready"
            else:
                print("âŒ API server unhealthy: ", data)
        else:
            print(f"âŒ Health check failed: {response.text}")
    except Exception as e:
        print(f"âŒ Error: API server not accessible at {api_url}")
        print(f"Error details: {str(e)}")
    return False

def index_papers_via_api(papers, api_url):
    docset_list = DocSetList(docsets=papers)
    data = docset_list.dict()
    try:
        response = httpx.post(f"{api_url}/index_papers/", json=data, timeout=3000.0)
        response.raise_for_status()
        print("Indexing response:", response.json())
    except Exception as e:
        print("Failed to index papers:", e)

def search_papers_via_api(api_url, query, search_strategy='tf-idf', similarity_cutoff=0.1, filters=None):
    """Search papers using the /find_similar/ endpoint for a single query.
    Returns a list of DocSet objects corresponding to the results.
    """
    payload = {
        "query": query,
        "top_k": 5,
        "similarity_cutoff": similarity_cutoff,
        "strategy_type": search_strategy,
        "filters": filters
    }
    try:
        response = httpx.post(f"{api_url}/find_similar/", json=payload, timeout=10.0)
        response.raise_for_status()
        results = response.json()
        print(f"\nResults for query '{query}' (strategy: {search_strategy}, cutoff: {similarity_cutoff}):")
        docsets = []
        for r in results:
            # Print for debug
            score = r.get('score', r.get('similarity_score'))
            title = r.get('title', r.get('metadata', {}).get('title'))
            print(f"  doc_id: {r.get('doc_id')}, score: {score}, title: {title}")
            
            # Add empty comments field to r
            r['comments'] = ""
            
            # Create DocSet instance (handle missing fields gracefully)
            try:
                docset = DocSet(**r)
            except TypeError:
                # If the API response has extra fields, filter them
                docset_fields = {k: v for k, v in r.items() if k in DocSet.__fields__}
                docset = DocSet(**docset_fields)
            print(f"[DocSet] Created with title: {docset.title}")  # Confirm DocSet creation
            docsets.append(docset)
        return docsets
    except Exception as e:
        print(f"Failed to search for query '{query}':", e)
        return []

def save_recommendations(username, papers, api_url):
    for paper in papers:
        print(paper)
        data = {
            "paper_id": paper.get("paper_id"),
            "title": paper.get("title", ""),
            "authors": paper.get("authors", ""),
            "abstract": paper.get("abstract", ""),
            "url": paper.get("url", ""),
            "content": paper.get("content", ""),  # å¿…é¡»è¡¥å…¨
            "blog": paper.get("blog", ""),
            "recommendation_reason": paper.get("recommendation_reason", ""),
            "relevance_score": paper.get("relevance_score", None),
            "blog_abs": paper.get("blog_abs", ""),
            "blog_title": paper.get("blog_title", ""),
        }
        try:
            resp = httpx.post(
                f"{api_url}/api/papers/recommend",
                params={"username": username},
                json=data,
                timeout=100.0
            )
            if resp.status_code == 201:
                print(f"âœ… æ¨èå†™å…¥æˆåŠŸ: {paper.get('paper_id')}")
            else:
                print(f"âŒ æ¨èå†™å…¥å¤±è´¥: {paper.get('paper_id')}ï¼ŒåŸå› : {resp.text}")
        except Exception as e:
            print(f"âŒ æ¨èå†™å…¥å¼‚å¸¸: {paper.get('paper_id')}ï¼Œé”™è¯¯: {e}")

def get_all_users(backend_api_url):
    """
    è·å–æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯ï¼Œè¿”å›ç”¨æˆ·å­—å…¸åˆ—è¡¨ï¼ˆå« username, interests_description ç­‰ï¼‰
    """
    resp = requests.get(f"{backend_api_url}/api/users/all", timeout=100.0)
    resp.raise_for_status()
    return resp.json()

def get_user_interest(username: str,backend_api_url):
        response = requests.get(f"{backend_api_url}/api/users/by_email/{username}")
        response.raise_for_status()
        user_data = response.json()
        return user_data.get("interests_description", [])

def fetch_daily_papers(index_api_url: str, config):
    """
    Fetch daily papers and return a list of DocSet objects.
    This function is a placeholder and should be replaced with the actual implementation.
    """
    # 1. Check connection health before indexing
    health = check_connection_health(index_api_url)
    if health == "not_ready" or not health:
        print("Attempting to initialize index service...")
        # Re-check health after initialization
        if not check_connection_health(index_api_url):
            print("Exiting due to failed health check after initialization.")
            sys.exit(1)

    papers = paper_pull.fetch_daily_papers()
    #papers=paper_pull.dummy_paper_fetch("./orchestrator/jsons")
    print(f"Fetched {len(papers)} papers.")

    # 2. Index papers
    index_papers_via_api(papers, index_api_url)
    
    # 3. Return the papers for further processing
    return papers

async def blog_generation_for_existing_user(index_api_url: str, backend_api_url: str):
    """
    Generate blog digests for existing users based on their interests.
    This function is a placeholder and should be replaced with the actual implementation.
    """
    all_users = get_all_users(backend_api_url)
    print(f"âœ… å…±è·å–åˆ° {len(all_users)} ä¸ªç”¨æˆ·")
    print([user.get("username") for user in all_users])
    for user in all_users:
        username = user.get("username")
        '''if username != "test@tongji.edu.cn":
            continue'''
        interests = get_user_interest(username,backend_api_url)
        print(f"\n=== ç”¨æˆ·: {username}ï¼Œå…´è¶£: {interests} ===")
        if not interests:
            print(f"ç”¨æˆ· {username} æ— å…´è¶£å…³é”®è¯ï¼Œè·³è¿‡æ¨èã€‚")
            continue
        
        # è·å–ç”¨æˆ·å·²æœ‰çš„è®ºæ–‡æ¨èï¼Œç”¨äºè¿‡æ»¤
        try:
            import requests
            user_papers_response = requests.get(f"{backend_api_url}/api/papers/recommendations/{username}")
            if user_papers_response.status_code == 200:
                user_existing_papers = user_papers_response.json()
                existing_paper_ids = [paper["id"] for paper in user_existing_papers if paper.get("id")]
                print(f"ç”¨æˆ· {username} å·²æœ‰ {len(existing_paper_ids)} ç¯‡è®ºæ–‡æ¨è")
                print(f"å·²æœ‰è®ºæ–‡ID: {existing_paper_ids[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
            else:
                existing_paper_ids = []
                print(f"è·å–ç”¨æˆ· {username} å·²æœ‰è®ºæ–‡å¤±è´¥ï¼ŒçŠ¶æ€ç : {user_papers_response.status_code}")
        except Exception as e:
            print(f"è·å–ç”¨æˆ· {username} å·²æœ‰è®ºæ–‡æ—¶å‡ºé”™: {e}")
            existing_paper_ids = []
        
        all_papers = []
        '''for query in interests:
            print(f"[TF-IDF] ç”¨æˆ· {username} å…´è¶£: {query}")
            papers = search_papers_via_api(index_api_url, query, 'tf-idf', 0.1)
            all_papers.extend(papers)'''
        
        for query in interests:
            print(f"[VECTOR] ç”¨æˆ· {username} å…´è¶£: {query}")
            
            # æ„å»ºè¿‡æ»¤å™¨ï¼Œæ’é™¤ç”¨æˆ·å·²æœ‰çš„è®ºæ–‡ID
            if existing_paper_ids:
                filter_params = {
                    "exclude": {
                        "doc_ids": existing_paper_ids
                    }
                }
                print(f"åº”ç”¨è¿‡æ»¤å™¨ï¼Œæ’é™¤ {len(existing_paper_ids)} ä¸ªå·²æœ‰è®ºæ–‡ID")
                papers = search_papers_via_api(index_api_url, query, 'vector', 0.1, filter_params)
            else:
                papers = search_papers_via_api(index_api_url, query, 'vector', 0.1)
            
            all_papers.extend(papers)

        # æ·»åŠ å»é‡é€»è¾‘ï¼šç¡®ä¿è®ºæ–‡IDä¸é‡å¤
        seen_paper_ids = set()
        unique_papers = []
        for paper in all_papers:
            if paper.doc_id not in seen_paper_ids:
                seen_paper_ids.add(paper.doc_id)
                unique_papers.append(paper)
        
        print(f"å»é‡å‰è®ºæ–‡æ•°é‡: {len(all_papers)}")
        print(f"å»é‡åè®ºæ–‡æ•°é‡: {len(unique_papers)}")
        
        # ä½¿ç”¨å»é‡åçš„è®ºæ–‡åˆ—è¡¨
        all_papers = unique_papers

        # 4. Generate blog digests for users
        print("Generating blog digests for users...")
        if all_papers:
            #run_batch_generation(all_papers)
            blog = await run_batch_generation(all_papers)
            print("Digest generation complete.")

            blog_abs = await run_batch_generation_abs(all_papers)
            blog_title = await run_batch_generation_title(all_papers)
            paper_infos = []
            for i, paper in enumerate(all_papers):
                try:
                    # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ŒåŸºäºå½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
                    blog_path = os.path.join(os.path.dirname(__file__), "blogs", f"{paper.doc_id}.md")
                    with open(blog_path, encoding="utf-8") as file:
                        blog = file.read()
                except FileNotFoundError:
                    blog = None  # æˆ–è€…å…¶ä»–å¤„ç†æ–¹å¼
                
                # è·å–å¯¹åº”çš„åšå®¢æ‘˜è¦å’Œæ ‡é¢˜
                blog_abs_content = blog_abs[i] if blog_abs and i < len(blog_abs) else None
                blog_title_content = blog_title[i] if blog_title and i < len(blog_title) else None
                
                paper_infos.append({
                    "paper_id": paper.doc_id,
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "abstract": paper.abstract,
                    "url": paper.HTML_path,
                    "content": paper.abstract,  # æˆ–å…¶ä»–å†…å®¹
                    "blog": blog,
                    "recommendation_reason": "This is a dummy recommendation reason for paper " + paper.title,
                    "relevance_score": 0.5,
                    "blog_abs": blog_abs_content,
                    "blog_title": blog_title_content,
                })

            # 5. Write recommendations
            save_recommendations(username, paper_infos, backend_api_url)
        else:
            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®ºæ–‡ï¼Œè·³è¿‡åšå®¢ç”Ÿæˆå’Œæ¨èä¿å­˜")
            continue

async def blog_generation_for_storage(index_api_url: str, backend_api_url: str, all_papers):
    """
    Generate blog digests for existing users based on their interests.
    This function is a placeholder and should be replaced with the actual implementation.
    """
    username = "BlogBot@gmail.com"
    
    seen_paper_ids = set()
    unique_papers = []
    for paper in all_papers:
        if paper.doc_id not in seen_paper_ids:
            seen_paper_ids.add(paper.doc_id)
            unique_papers.append(paper)
    
    print(f"å»é‡å‰è®ºæ–‡æ•°é‡: {len(all_papers)}")
    print(f"å»é‡åè®ºæ–‡æ•°é‡: {len(unique_papers)}")
    
    # ä½¿ç”¨å»é‡åçš„è®ºæ–‡åˆ—è¡¨
    all_papers = unique_papers

    # 4. Generate blog digests for users in batches
    print("Generating blog digests for users...")
    
    batch_size = 50
    total_papers = len(all_papers)
    processed_count = 0
    
    for batch_start in range(0, total_papers, batch_size):
        batch_end = min(batch_start + batch_size, total_papers)
        batch_papers = all_papers[batch_start:batch_end]
        
        print(f"ğŸ”„ Processing batch {batch_start//batch_size + 1}: papers {batch_start+1}-{batch_end} of {total_papers}")
        
        try:
            # ç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„åšå®¢
            await run_batch_generation(batch_papers)
            print(f"âœ… Blog generation completed for batch {batch_start//batch_size + 1}")
            
            # ç«‹å³å¤„ç†å¹¶ä¿å­˜å½“å‰æ‰¹æ¬¡çš„è®ºæ–‡
            paper_infos = []
            for paper in batch_papers:
                try:
                    # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ŒåŸºäºå½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
                    blog_path = os.path.join("/data3/guofang/peirongcan/PaperIgnition/orchestrator/blogs", f"{paper.doc_id}.md")
                    with open(blog_path, encoding="utf-8") as file:
                        blog = file.read()
                except FileNotFoundError:
                    blog = None
                
                paper_infos.append({
                    "paper_id": paper.doc_id,
                    "title": paper.title,
                    "authors": ", ".join(paper.authors),
                    "abstract": paper.abstract,
                    "url": paper.HTML_path,
                    "content": paper.abstract,
                    "blog": blog,
                    "recommendation_reason": f"This is a dummy recommendation reason for paper {paper.title}",
                    "relevance_score": 0.5
                })
            
            # ä¿å­˜å½“å‰æ‰¹æ¬¡
            print(f"ğŸ’¾ Saving batch {batch_start//batch_size + 1} ({len(paper_infos)} papers)...")
            save_recommendations(username, paper_infos, backend_api_url)
            
            processed_count += len(batch_papers)
            print(f"ğŸ“Š Progress: {processed_count}/{total_papers} papers processed")
            
        except Exception as e:
            print(f"âŒ Blog generation failed for batch {batch_start//batch_size + 1}: {e}")
            continue
    
    print(f"ğŸ‰ All batches completed! Total processed: {processed_count}/{total_papers}")
