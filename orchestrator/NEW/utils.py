import paper_pull
#from backend.index_service import index_papers
import requests
import os
from backend.app.db_utils import load_config as load_backend_config
from AIgnite.data.docset import DocSetList, DocSet
import httpx
import sys
import asyncio
import yaml
from pathlib import Path

def load_config():
    """Load configuration from config.yaml file"""
    config_path = Path(__file__).parent / "config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def check_connection_health(api_url, timeout=None):
    if timeout is None:
        config = load_config()
        timeout = config['timeouts']['health_check']
    try:
        config = load_config()
        health_endpoint = config['api']['index']['endpoints']['health']
        response = httpx.get(f"{api_url}{health_endpoint}", timeout=timeout)
        if response.status_code == 200:
            data = response.json()
            if data.get("status") == "healthy" and data.get("indexer_ready"):
                print("âœ… Connection health check passed")
                return True
            elif data.get("status") == "healthy" and not data.get("indexer_ready"):
                print("âŒ API server is not ready: ", data)
                return "not_ready"
            else:
                print("âŒ API server unhealthy: ", data)
        else:
            print(f"âŒ Health check failed: {response.text}")
    except Exception as e:
        print(f"âŒ Error: API server not accessible at {api_url}")
        print(f"Error details: {str(e)}")
    return False

def index_papers_via_api(papers, api_url, store_images=False, keep_temp_image=False):
    """
    Index papers using the /index_papers/ endpoint.
    
    Args:
        papers: List of DocSet objects
        api_url: API base URL
        store_images: Whether to store images to MinIO (default: False)
        keep_temp_image: If False, delete temporary image files after successful storage (default: False)
    """
    docset_list = DocSetList(docsets=papers)
    
    # æŒ‰ç…§IndexPapersRequestæ¨¡å‹æ„å»ºè¯·æ±‚ä½“
    request_data = {
        "docsets": docset_list.dict(),
        "store_images": store_images,
        "keep_temp_image": keep_temp_image
    }
    
    try:
        config = load_config()
        index_endpoint = config['api']['index']['endpoints']['index_papers']
        timeout = config['timeouts']['index_papers']
        response = httpx.post(f"{api_url}{index_endpoint}", json=request_data, timeout=timeout)
        response.raise_for_status()
        print("Indexing response:", response.json())
    except Exception as e:
        print("Failed to index papers:", e)

def search_papers_via_api(api_url, query, search_strategy=None, similarity_cutoff=None, filters=None):
    config = load_config()
    if search_strategy is None:
        search_strategy = config['generation']['search']['default_strategy']
    if similarity_cutoff is None:
        similarity_cutoff = config['generation']['search']['default_similarity_cutoff']
    """Search papers using the /find_similar/ endpoint for a single query.
    Returns a list of DocSet objects corresponding to the results.
    """
    # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
    health = check_connection_health(api_url)
    if not health:
        print(f"âŒ æœç´¢æœåŠ¡ {api_url} ä¸å¯ç”¨ï¼Œè·³è¿‡æŸ¥è¯¢ '{query}'")
        return []
    
    # æ ¹æ®æ–°çš„APIç»“æ„æ„å»ºpayload
    search_config = config['generation']['search']
    payload = {
        "query": query,
        "top_k": search_config['default_top_k'],
        "similarity_cutoff": similarity_cutoff,
        "search_strategies": [(search_strategy, search_config['default_threshold'])],  # æ–°APIä½¿ç”¨å…ƒç»„æ ¼å¼ (strategy, threshold)
        "filters": filters,
        "result_include_types": search_config['result_include_types']  # ä½¿ç”¨æ­£ç¡®çš„ç»“æœç±»å‹
    }
    try:
        find_similar_endpoint = config['api']['index']['endpoints']['find_similar']
        timeout = config['timeouts']['search_query']
        response = httpx.post(f"{api_url}{find_similar_endpoint}", json=payload, timeout=timeout)
        response.raise_for_status()
        results = response.json()
        print(f"\nResults for query '{query}' (strategy: {search_strategy}, cutoff: {similarity_cutoff}):")
        docsets = []
        for r in results:
            # Create DocSet instance (handle missing fields gracefully)
            try:
                # æå–metadataä¸­çš„ä¿¡æ¯
                metadata = r.get('metadata', {})
                
                # å¤„ç†chunksæ•°æ®ï¼Œç¡®ä¿ç¬¦åˆDocSetå®šä¹‰
                def process_text_chunks(chunks_data):
                    """å¤„ç†text_chunksæ•°æ®ï¼Œè½¬æ¢ä¸ºç¬¦åˆDocSetå®šä¹‰çš„æ ¼å¼"""
                    if not chunks_data:
                        return []
                    
                    processed_chunks = []
                    for chunk in chunks_data:
                        if isinstance(chunk, dict):
                            # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼
                            if 'id' in chunk and 'type' in chunk and 'text' in chunk:
                                processed_chunks.append(chunk)
                            elif 'chunk_id' in chunk and 'text_content' in chunk:
                                # è½¬æ¢APIæ ¼å¼åˆ°DocSetæ ¼å¼
                                converted_chunk = {
                                    'id': chunk['chunk_id'],
                                    'type': 'text',
                                    'text': chunk['text_content']
                                }
                                processed_chunks.append(converted_chunk)
                            else:
                                # è·³è¿‡æ— æ•ˆçš„chunk
                                print(f"Warning: Skipping invalid text chunk: {chunk}")
                        else:
                            print(f"Warning: Skipping non-dict text chunk: {chunk}")
                    return processed_chunks
                
                # ä¸ºç¼ºå¤±çš„å¿…éœ€å­—æ®µæä¾›é»˜è®¤å€¼ï¼Œç¡®ä¿ç¬¦åˆDocSetå®šä¹‰
                docset_data = {
                    'doc_id': r.get('doc_id'),
                    'title': metadata.get('title', 'Unknown Title'),
                    'authors': metadata.get('authors', []),
                    'categories': metadata.get('categories', []),
                    'published_date': metadata.get('published_date', ''),
                    'abstract': metadata.get('abstract', ''),
                    'pdf_path': metadata.get('pdf_path', ''),
                    'HTML_path': metadata.get('HTML_path'),
                    'text_chunks': process_text_chunks(r.get('text_chunks', [])),
                    'figure_chunks': [],
                    'table_chunks': [],
                    'metadata': metadata,
                    'comments': metadata.get('comments', '')
                }
                
                docset = DocSet(**docset_data)
                print(f"[DocSet] Created with title: {docset.title}")
                docsets.append(docset)
            except Exception as e:
                print(f"Failed to create DocSet for {r.get('doc_id')}: {e}")
                continue
        return docsets
    except httpx.TimeoutException:
        print(f"âŒ æœç´¢æŸ¥è¯¢ '{query}' è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡å™¨çŠ¶æ€")
        return []
    except httpx.ConnectError:
        print(f"âŒ æ— æ³•è¿æ¥åˆ°æœç´¢æœåŠ¡ {api_url}ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ")
        return []
    except httpx.HTTPStatusError as e:
        print(f"âŒ æœç´¢æŸ¥è¯¢ '{query}' è¿”å›é”™è¯¯çŠ¶æ€ç : {e.response.status_code}")
        print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return []
    except Exception as e:
        print(f"âŒ æœç´¢æŸ¥è¯¢ '{query}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return []

def save_recommendations(username, papers, api_url):
    for paper in papers:
        print(paper)
        data = {
            "paper_id": paper.get("paper_id"),
            "title": paper.get("title", ""),
            "authors": paper.get("authors", ""),
            "abstract": paper.get("abstract", ""),
            "url": paper.get("url", ""),
            "content": paper.get("content", ""),  # å¿…é¡»è¡¥å…¨
            "blog": paper.get("blog", ""),
            "recommendation_reason": paper.get("recommendation_reason", ""),
            "relevance_score": paper.get("relevance_score", None),
            "blog_abs": paper.get("blog_abs", ""),
            "blog_title": paper.get("blog_title", ""),
            "submitted": paper.get("submitted", ""),
            "comment": paper.get("comment", ""),
        }
        try:
            config = load_config()
            recommend_endpoint = config['api']['backend']['endpoints']['papers_recommend']
            timeout = config['timeouts']['save_recommendation']
            resp = httpx.post(
                f"{api_url}{recommend_endpoint}",
                params={"username": username},
                json=data,
                timeout=timeout
            )
            if resp.status_code == 201:
                print(f"âœ… æ¨èå†™å…¥æˆåŠŸ: {paper.get('paper_id')}")
            else:
                print(f"âŒ æ¨èå†™å…¥å¤±è´¥: {paper.get('paper_id')}ï¼ŒåŸå› : {resp.text}")
        except Exception as e:
            print(f"âŒ æ¨èå†™å…¥å¼‚å¸¸: {paper.get('paper_id')}ï¼Œé”™è¯¯: {e}")

def fetch_daily_papers(index_api_url: str, config):
    """
    Fetch daily papers and return a list of DocSet objects.
    This function is a placeholder and should be replaced with the actual implementation.
    """
    # 1. Check connection health before indexing
    health = check_connection_health(index_api_url)
    if health == "not_ready" or not health:
        print("Attempting to initialize index service...")
        # Re-check health after initialization
        if not check_connection_health(index_api_url):
            print("Exiting due to failed health check after initialization.")
            sys.exit(1)

    papers = _fetch_daily_papers()
    #papers=_dummy_paper_fetch("./orchestrator/jsons")
    print(f"Fetched {len(papers)} papers.")

    # 2. Index papers
    config = load_config()
    index_config = config['index']
    index_papers_via_api(papers, index_api_url, 
                        store_images=index_config['store_images'], 
                        keep_temp_image=index_config['keep_temp_image'])
    
    # 3. Return the papers for further processing
    return papers

import requests
from AIgnite.generation.generator import GeminiBlogGenerator, AsyncvLLMGenerator
from AIgnite.data.docset import DocSet
import os
import json
import yaml
import asyncio
#from backend.index_service import index_papers, find_similar
#from backend.user_service import get_all_users, get_user_interest

# @ch, replace it with backend.user_service
"""
to do:
ä»¥ä¸‹ä¸¤ä¸ªæ¥å£ç¼ºå°‘ä¸€ä¸ªå®‰å…¨éªŒè¯æœºåˆ¶
"""
def get_all_users():
    """
        è·å–æ‰€æœ‰ç”¨æˆ·ä¿¡æ¯ï¼ˆusername å’Œ interests_descriptionï¼‰,è¿”å›jsonï¼Œç¤ºä¾‹å¦‚ä¸‹
        [
            {
                'username': '3220102841@zju.edu.cn', 
                'interests_description': ['å¤§å‹è¯­è¨€æ¨¡å‹', 'å›¾ç¥ç»ç½‘ç»œ']
            },
            {
                'username': 'chtest@qq.com', 
                'interests_description': ['å¤§å‹è¯­è¨€æ¨¡å‹', 'å›¾ç¥ç»ç½‘ç»œ']
            }
        ]
    """
    config = load_config()
    backend_url = config['api']['backend']['base_url']
    users_endpoint = config['api']['backend']['endpoints']['users_all']
    response = requests.get(f"{backend_url}{users_endpoint}")
    response.raise_for_status()  # Raises an exception for bad status codes
    users_data = response.json()
    
    # Transform the data to the desired format
    transformed_users = []
    for user in users_data:
        transformed_users.append({
            "username": user.get("username"),
            "interests_description": user.get("interests_description", [])
        })
    return transformed_users

def get_user_interest(username: str):
    """
        è·å–æŒ‡å®šç”¨æˆ·çš„ç ”ç©¶å…´è¶£ï¼ˆinterests_descriptionï¼‰,è¿”å›jsonï¼Œç¤ºä¾‹å¦‚ä¸‹
        ['å¤§å‹è¯­è¨€æ¨¡å‹', 'å›¾ç¥ç»ç½‘ç»œ']
    """
    # å®é™…ä¸Šusernameå’Œuser_emailä¿æŒä¸€è‡´
    config = load_config()
    backend_url = config['api']['backend']['base_url']
    user_endpoint = config['api']['backend']['endpoints']['users_by_email']
    response = requests.get(f"{backend_url}{user_endpoint}/{username}") 
    response.raise_for_status() # Raises an exception for bad status codes (e.g., 404)
    user_data = response.json()
    return user_data.get("interests_description", [])


def run_Gemini_blog_generation(papers):
    config = load_config()
    paths = config['paths']
    generator = GeminiBlogGenerator(
        data_path=paths['orchestrator']['imgs'], 
        output_path=paths['output']['gemini_blogs'])
    blog = generator.generate_digest(papers)



async def run_batch_generation(papers):
    config = load_config()
    llm_config = config['api']['llm']
    paths = config['paths']
    generator = AsyncvLLMGenerator(
        model_name=llm_config['model_name'], 
        api_base=llm_config['base_url'],
        data_path=paths['orchestrator']['imgs'], 
        output_path=paths['output']['generated_blogs'])
    
    config_path = os.path.join(os.path.dirname(__file__), config['paths']['orchestrator']['config'], "prompt.yaml")
    with open(config_path, "r") as f:
        prompt_config = yaml.safe_load(f)

    system_prompt = prompt_config['prompts']['blog_generation']['system_prompt']
    user_prompt_template = prompt_config['prompts']['blog_generation']['user_prompt_template']

    prompts = []
    for paper in papers:
        # å‡†å¤‡å›¾ç‰‡è·¯å¾„
        image_path = generator.data_path
        
        prompt = user_prompt_template.format(
            title=paper.title, 
            authors=paper.authors, 
            abstract=paper.abstract, 
            text_chunks=paper.text_chunks,
            image_path=image_path,
            arxiv_id=paper.doc_id,
            table_chunks=paper.table_chunks,
        )
        text_chunk_limit = config['generation']['text_chunk_limit']
        if len(paper.text_chunks) > text_chunk_limit:
            prompt = prompt[:text_chunk_limit]
        prompts.append(prompt)
    try:
        max_tokens = config['generation']['max_tokens']
        blog = await generator.batch_generate(prompts=prompts, system_prompts=system_prompt, max_tokens=max_tokens, papers=papers)
        return blog
    except Exception as e:
        print(f"Error: {e}")
        return None

async def run_batch_generation_abs(papers):
    config = load_config()
    llm_config = config['api']['llm']
    paths = config['paths']
    generator = AsyncvLLMGenerator(
        model_name=llm_config['model_name'], 
        api_base=llm_config['base_url'],
        data_path=paths['orchestrator']['imgs'], 
        output_path=paths['output']['generated_blogs'])
    
    config_path = os.path.join(os.path.dirname(__file__), config['paths']['orchestrator']['config'], "prompt.yaml")
    with open(config_path, "r") as f:
        prompt_config = yaml.safe_load(f)

    system_prompt = prompt_config['prompts']['blog_generation_abs']['system_prompt']
    user_prompt_template = prompt_config['prompts']['blog_generation_abs']['user_prompt_template']

    prompts = []
    for paper in papers:  # éå† papers è€Œä¸æ˜¯ blogs
        try:
            # ä»ç£ç›˜è¯»å–åšå®¢æ–‡ä»¶
            blog_file_path = os.path.join(paths['output']['generated_blogs'], f"{paper.doc_id}.md")
            with open(blog_file_path, encoding="utf-8") as file:
                blog_content = file.read()
        except FileNotFoundError:
            print(f"âŒ Blog file not found for {paper.doc_id}")
            continue
        
        prompt = user_prompt_template.format(
            blog=blog_content
        )
        prompts.append(prompt)
    
    try:
        max_tokens = config['generation']['max_tokens']
        abs = await generator.batch_generate_not_save(prompts=prompts, system_prompts=system_prompt, max_tokens=max_tokens, papers=papers)
        return abs
    except Exception as e:
        print(f"Error: {e}")
        return None


async def run_batch_generation_title(papers):
    config = load_config()
    llm_config = config['api']['llm']
    paths = config['paths']
    generator = AsyncvLLMGenerator(
        model_name=llm_config['model_name'], 
        api_base=llm_config['base_url'],
        data_path=paths['orchestrator']['imgs'], 
        output_path=paths['output']['generated_blogs'])
    
    config_path = os.path.join(os.path.dirname(__file__), config['paths']['orchestrator']['config'], "prompt.yaml")
    with open(config_path, "r") as f:
        prompt_config = yaml.safe_load(f)

    system_prompt = prompt_config['prompts']['blog_generation_title']['system_prompt']
    user_prompt_template = prompt_config['prompts']['blog_generation_title']['user_prompt_template']

    prompts = []
    for paper in papers:  # éå† papers è€Œä¸æ˜¯ blogs
        prompt = user_prompt_template.format(
            title=paper.title
        )
        prompts.append(prompt)
    
    try:
        max_tokens = config['generation']['max_tokens']
        titles = await generator.batch_generate_not_save(prompts=prompts, system_prompts=system_prompt, max_tokens=max_tokens, papers=papers)
        return titles
    except Exception as e:
        print(f"Error: {e}")
        return None

from AIgnite.data.docset import *
from AIgnite.data.htmlparser import *
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import ProcessPoolExecutor, as_completed  
import json
from pathlib import Path
import os
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

def _fetch_daily_papers(time=None) -> list[DocSet]:
    # Replace this with your actual fetcher
    if time is None:
        time = get_time_str()
    print(f"fetching papers for {time}")
    docs = []
    # set up
    base_dir = os.path.dirname(__file__)
    html_text_folder = os.path.join(base_dir, "htmls")
    pdf_folder_path = os.path.join(base_dir, "pdfs")
    image_folder_path = os.path.join(base_dir, "imgs")
    json_output_path = os.path.join(base_dir, "jsons")
    arxiv_pool_path = os.path.join(base_dir, "html_url_storage/html_urls.txt")

    time_slots = divide_a_day_into(time, 3)
    # time_slots = divide_a_day_into('202405300000', 3)
    
    #make sure the folders exist
    os.makedirs(os.path.dirname(arxiv_pool_path), exist_ok=True)
    Path(arxiv_pool_path).touch(exist_ok=True)
    for path in [html_text_folder, pdf_folder_path, image_folder_path, json_output_path]:
        os.makedirs(path, exist_ok=True)

    #fetch daily papers in parallel
    newly_fetched_ids = set()
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for i in range(len(time_slots) - 1):
            start_str = time_slots[i]
            end_str = time_slots[i + 1]
            futures.append(executor.submit(run_extractor_for_timeslot, start_str, end_str))

        for f in futures:
            result = f.result()
            if result:  # å¦‚æœè¿”å›äº†æ–°æŠ“å–çš„IDåˆ—è¡¨
                newly_fetched_ids.update(result)
    
    print(f"ğŸ“Š æ–°æŠ“å–è®ºæ–‡IDæ•°é‡: {len(newly_fetched_ids)}")
    
    #summary docs from json - only return newly fetched papers
    new_docs = []
    for json_file in Path(json_output_path).glob("*.json"):
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ–°æŠ“å–çš„arxiv ID
        file_name = json_file.stem  # å»æ‰.jsonæ‰©å±•å
        if file_name in newly_fetched_ids:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                try:
                    docset = DocSet(**data)
                    new_docs.append(docset)
                    print(f"âœ… æ–°æŠ“å–è®ºæ–‡: {docset.doc_id} - {docset.title}")
                except Exception as e:
                    print(f"Failed to parse {json_file.name}: {e}")
    
    print(f"ğŸ“Š æ–°æŠ“å–è®ºæ–‡æ•°é‡: {len(new_docs)}")
    
    return new_docs

def _dummy_paper_fetch(file_path: str) -> list[DocSet]:
    docs = []
    path_obj = Path(file_path)
    
    if path_obj.is_dir():
        i = 0
        for json_file in path_obj.glob("*.json"):
            with open(json_file, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    docset = DocSet(**data)
                    print(f"Parsed {json_file.name}")
                    docs.append(docset)
                except Exception as e:
                    print(f"Failed to parse {json_file.name}: {e}")
    else:
        print(f"The path {file_path} is not a directory")
    return docs

def run_extractor_for_timeslot(start_str, end_str):
    base_dir = os.path.dirname(__file__)
    html_text_folder = os.path.join(base_dir, "htmls")
    pdf_folder_path = os.path.join(base_dir, "pdfs")
    image_folder_path = os.path.join(base_dir, "imgs")
    json_output_path = os.path.join(base_dir, "jsons")
    arxiv_pool_path = os.path.join(base_dir, "html_url_storage/html_urls.txt")
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–å¯†é’¥ï¼Œé¿å…ç¡¬ç¼–ç 
    ak = os.getenv("VOLCENGINE_AK", "")
    sk = os.getenv("VOLCENGINE_SK", "")

    extractor = ArxivHTMLExtractor(
        html_text_folder=html_text_folder,
        pdf_folder_path=pdf_folder_path,
        arxiv_pool=arxiv_pool_path,
        image_folder_path=image_folder_path,
        json_path=json_output_path,
        volcengine_ak=ak,
        volcengine_sk=sk,
        start_time=start_str,
        end_time=end_str
    )

    extractor.extract_all_htmls()
    extractor.pdf_parser_helper.docs = extractor.docs
    extractor.pdf_parser_helper.remain_docparser()
    extractor.docs = extractor.pdf_parser_helper.docs
    
    # è®°å½•æ–°æŠ“å–çš„è®ºæ–‡ID
    newly_fetched_ids = [doc.doc_id for doc in extractor.docs]
    
    extractor.serialize_docs()
    
    return newly_fetched_ids



def get_time_str(location = "Asia/Shanghai", count_delay = 1):
    # è®¾å®šæœ¬åœ°æ—¶åŒºï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    local_tz = ZoneInfo(location)  # ä¾‹å¦‚ä¸Šæµ·
    # è·å–æœ¬åœ°å½“å‰æ—¶é—´ï¼Œç²¾ç¡®åˆ°åˆ†é’Ÿ
    local_now = (datetime.now(local_tz)-timedelta(days=count_delay)).replace(second=0, microsecond=0)
    # è½¬æ¢ä¸ºUTCæ—¶é—´
    utc_now = local_now.astimezone(ZoneInfo("UTC"))
    # è½¬ä¸ºæŒ‡å®šæ ¼å¼å­—ç¬¦ä¸²
    utc_str = utc_now.strftime("%Y%m%d%H%M")
    return utc_str


# æ–°å¢å‡½æ•°
def divide_a_day_into(time: str, count: int):
    """
    è¾“å…¥: time (å¦‚202507150856), count (å¦‚3)
    è¾“å‡º: å°†[å‰ä¸€å¤©åŒä¸€æ—¶åˆ», è¾“å…¥æ—¶åˆ»]åˆ†æˆcountä»½ï¼Œè¿”å›æ¯ä¸ªåˆ†æ®µçš„æ—¶é—´ç‚¹å­—ç¬¦ä¸²æ•°ç»„ï¼ˆç²¾ç¡®åˆ°åˆ†é’Ÿï¼Œæ ¼å¼ä¸º%Y%m%d%H%Mï¼‰ï¼ŒåŒ…å«å¤´å°¾ã€‚
    """
    from datetime import datetime, timedelta
    fmt = "%Y%m%d%H%M"
    end_time = datetime.strptime(time, fmt)
    start_time = end_time - timedelta(days=1)
    total_minutes = int((end_time - start_time).total_seconds() // 60)
    step = total_minutes / count
    result = []
    for i in range(count + 1):
        t = start_time + timedelta(minutes=round(i * step))
        result.append(t.strftime(fmt))
    return result