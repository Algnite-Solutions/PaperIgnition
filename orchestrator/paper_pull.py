from AIgnite.data.docset import *
from AIgnite.data.htmlparser import ArxivHTMLExtractor
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import ProcessPoolExecutor, as_completed
import json
from pathlib import Path
import os
import logging
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import List, Optional, Literal
from enum import Enum


class PaperPullService:
    """
    Service for pulling and extracting papers from arXiv

    Supports both HTML and PDF extraction methods (PDF is TODO)
    """

    def __init__(
        self,
        base_dir: Optional[str] = None,
        max_workers: int = 3,
        time_slots_count: int = 3,
        location: str = "Asia/Shanghai",
        count_delay: int = 1,
        max_papers: Optional[int] = None
    ):
        """
        Initialize PaperPullService

        Args:
            base_dir: Base directory for storing papers (defaults to orchestrator dir)
            max_workers: Number of parallel workers for fetching
            time_slots_count: Number of time slots to divide the day into
            location: Timezone location for time calculations
            count_delay: Days to delay from current date
            max_papers: Maximum number of papers to fetch (None for unlimited)
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.max_workers = max_workers
        self.time_slots_count = time_slots_count
        self.location = location
        self.count_delay = count_delay
        self.max_papers = max_papers

        # Setup directories
        if base_dir is None:
            base_dir = os.path.dirname(__file__)
        self.base_dir = Path(base_dir)

        self.html_text_folder = self.base_dir / "htmls"
        self.pdf_folder_path = self.base_dir / "pdfs"
        self.image_folder_path = self.base_dir / "imgs"
        self.json_output_path = self.base_dir / "jsons"
        self.arxiv_pool_path = self.base_dir / "html_url_storage" / "html_urls.txt"

        # Get credentials from environment
        self.volcengine_ak = os.getenv("VOLCENGINE_AK", "")
        self.volcengine_sk = os.getenv("VOLCENGINE_SK", "")

        # Initialize directories
        self._setup_directories()

    def _setup_directories(self):
        """Create necessary directories if they don't exist"""
        self.arxiv_pool_path.parent.mkdir(parents=True, exist_ok=True)
        self.arxiv_pool_path.touch(exist_ok=True)

        for path in [self.html_text_folder, self.pdf_folder_path,
                     self.image_folder_path, self.json_output_path]:
            path.mkdir(parents=True, exist_ok=True)

    def _get_time_str(self) -> str:
        """
        Get UTC time string for the fetch window

        Returns:
            Time string in format YYYYMMDDHHMM
        """
        local_tz = ZoneInfo(self.location)
        local_now = (datetime.now(local_tz) - timedelta(days=self.count_delay)).replace(
            second=0, microsecond=0
        )
        utc_now = local_now.astimezone(ZoneInfo("UTC"))
        return utc_now.strftime("%Y%m%d%H%M")

    def _divide_time_into_slots(self, time: str) -> List[str]:
        """
        Divide a 24-hour period into time slots

        Args:
            time: End time in format YYYYMMDDHHMM

        Returns:
            List of time slot boundaries
        """
        fmt = "%Y%m%d%H%M"
        end_time = datetime.strptime(time, fmt)
        start_time = end_time - timedelta(days=1)
        total_minutes = int((end_time - start_time).total_seconds() // 60)
        step = total_minutes / self.time_slots_count

        result = []
        for i in range(self.time_slots_count + 1):
            t = start_time + timedelta(minutes=round(i * step))
            result.append(t.strftime(fmt))
        return result

    def _run_html_with_pdf_fallback(self, start_str: str, end_str: str, max_papers_per_slot: Optional[int] = None) -> List[str]:
        """
        Run HTML extractor first, then use PDF parser for papers that failed HTML extraction

        This is the default and recommended approach:
        1. Try HTML extraction for all papers
        2. For papers that failed HTML extraction, use PDF parser as fallback

        Args:
            start_str: Start time string
            end_str: End time string
            max_papers_per_slot: Maximum papers to fetch in this time slot

        Returns:
            List of newly fetched paper IDs
        """
        extractor = ArxivHTMLExtractor(
            html_text_folder=str(self.html_text_folder),
            pdf_folder_path=str(self.pdf_folder_path),
            arxiv_pool=str(self.arxiv_pool_path),
            image_folder_path=str(self.image_folder_path),
            json_path=str(self.json_output_path),
            volcengine_ak=self.volcengine_ak,
            volcengine_sk=self.volcengine_sk,
            start_time=start_str,
            end_time=end_str,
            max_results=max_papers_per_slot
        )

        # Step 1: Extract HTML papers
        extractor.extract_all_htmls()

        # Step 2: Use PDF parser for remaining docs (fallback for failed HTML extraction)
        extractor.pdf_parser_helper.docs = extractor.docs
        extractor.pdf_parser_helper.remain_docparser()
        extractor.docs = extractor.pdf_parser_helper.docs

        # Collect newly fetched paper IDs
        newly_fetched_ids = [doc.doc_id for doc in extractor.docs]

        # Serialize all docs to JSON
        extractor.serialize_docs()

        return newly_fetched_ids

    def _run_extractor_for_timeslot(self, start_str: str, end_str: str, max_papers_per_slot: int) -> List[str]:
        """
        Run the appropriate extractor for a time slot

        By default (HTML type), uses HTML extraction with PDF fallback for failed papers

        Args:
            start_str: Start time string
            end_str: End time string

        Returns:
            List of newly fetched paper IDs
        """
        # Default: HTML first, PDF fallback for failures
        return self._run_html_with_pdf_fallback(start_str, end_str, max_papers_per_slot)

    def fetch_daily_papers(self, time: Optional[str] = None) -> List[DocSet]:
        """
        Fetch daily papers from arXiv

        Args:
            time: End time for fetch window (defaults to current time)

        Returns:
            List of newly fetched DocSet objects
        """
        if time is None:
            time = self._get_time_str()

        self.logger.info(f"Fetching papers for {time}")
        if self.max_papers:
            self.logger.info(f"Max papers limit: {self.max_papers}")

        time_slots = self._divide_time_into_slots(time)
        num_slots = len(time_slots) - 1

        # Calculate max papers per slot if max_papers is set
        max_papers_per_slot = None
        if self.max_papers:
            max_papers_per_slot = self.max_papers // num_slots
            if self.max_papers % num_slots != 0:
                max_papers_per_slot += 1  # Round up to cover all papers
            self.logger.info(f"Max papers per time slot: {max_papers_per_slot} (across {num_slots} slots)")

        # Fetch papers in parallel using thread pool
        newly_fetched_ids = set()
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for i in range(num_slots):
                start_str = time_slots[i]
                end_str = time_slots[i + 1]
                futures.append(
                    executor.submit(self._run_extractor_for_timeslot, start_str, end_str, max_papers_per_slot)
                )

            for f in futures:
                result = f.result()
                if result:
                    newly_fetched_ids.update(result)

        self.logger.info(f"ðŸ“Š Newly fetched paper IDs: {len(newly_fetched_ids)}")

        # Load newly fetched papers from JSON
        new_docs = []
        for json_file in self.json_output_path.glob("*.json"):
            file_name = json_file.stem
            if file_name in newly_fetched_ids:
                try:
                    with open(json_file, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        docset = DocSet(**data)
                        new_docs.append(docset)
                        self.logger.info(f"âœ… Loaded: {docset.doc_id} - {docset.title}")
                except Exception as e:
                    self.logger.error(f"Failed to parse {json_file.name}: {e}")

        self.logger.info(f"ðŸ“Š Total newly fetched papers: {len(new_docs)}")
        return new_docs