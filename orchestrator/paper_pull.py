from AIgnite.data.docset import *
from AIgnite.data.htmlparser import *
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import ProcessPoolExecutor, as_completed  
import json
from pathlib import Path
import os
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo

def fetch_daily_papers(time=None) -> list[DocSet]:
    # Replace this with your actual fetcher
    if time is None:
        time = get_time_str()
    print(f"fetching papers for {time}")
    docs = []
    # set up
    base_dir = os.path.dirname(__file__)
    html_text_folder = os.path.join(base_dir, "htmls")
    pdf_folder_path = os.path.join(base_dir, "pdfs")
    image_folder_path = os.path.join(base_dir, "imgs")
    json_output_path = os.path.join(base_dir, "jsons")
    arxiv_pool_path = os.path.join(base_dir, "html_url_storage/html_urls.txt")

    time_slots = divide_a_day_into(time, 1)
    # time_slots = divide_a_day_into('202405300000', 3)
    
    #make sure the folders exist
    os.makedirs(os.path.dirname(arxiv_pool_path), exist_ok=True)
    Path(arxiv_pool_path).touch(exist_ok=True)
    for path in [html_text_folder, pdf_folder_path, image_folder_path, json_output_path]:
        os.makedirs(path, exist_ok=True)

    #fetch daily papers in parallel
    newly_fetched_ids = set()
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for i in range(len(time_slots) - 1):
            start_str = time_slots[i]
            end_str = time_slots[i + 1]
            futures.append(executor.submit(run_extractor_for_timeslot, start_str, end_str))

        for f in futures:
            result = f.result()
            if result:  # å¦‚æœè¿”å›äº†æ–°æŠ“å–çš„IDåˆ—è¡¨
                newly_fetched_ids.update(result)
    
    print(f"ğŸ“Š æ–°æŠ“å–è®ºæ–‡IDæ•°é‡: {len(newly_fetched_ids)}")
    #summary docs from json - only return newly fetched papers
    new_docs = []
    for json_file in Path(json_output_path).glob("*.json"):
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ–°æŠ“å–çš„arxiv ID
        file_name = json_file.stem  # å»æ‰.jsonæ‰©å±•å
        if file_name in newly_fetched_ids:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                try:
                    docset = DocSet(**data)
                    new_docs.append(docset)
                    print(f"âœ… æ–°æŠ“å–è®ºæ–‡: {docset.doc_id} - {docset.title}")
                except Exception as e:
                    print(f"Failed to parse {json_file.name}: {e}")
    
    print(f"ğŸ“Š æ–°æŠ“å–è®ºæ–‡æ•°é‡: {len(new_docs)}")
    
    return new_docs

def dummy_paper_fetch(file_path: str) -> list[DocSet]:
    docs = []
    path_obj = Path(file_path)
    
    if path_obj.is_dir():
        i = 0
        for json_file in path_obj.glob("*.json"):
            with open(json_file, "r", encoding="utf-8") as f:
                try:
                    data = json.load(f)
                    docset = DocSet(**data)
                    print(f"Parsed {json_file.name}")
                    docs.append(docset)
                except Exception as e:
                    print(f"Failed to parse {json_file.name}: {e}")
    else:
        print(f"The path {file_path} is not a directory")
    return docs

def run_extractor_for_timeslot(start_str, end_str):
    base_dir = os.path.dirname(__file__)
    html_text_folder = os.path.join(base_dir, "htmls")
    pdf_folder_path = os.path.join(base_dir, "pdfs")
    image_folder_path = os.path.join(base_dir, "imgs")
    json_output_path = os.path.join(base_dir, "jsons")
    arxiv_pool_path = os.path.join(base_dir, "html_url_storage/html_urls.txt")
    # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–å¯†é’¥ï¼Œé¿å…ç¡¬ç¼–ç 
    ak = os.getenv("VOLCENGINE_AK", "")
    sk = os.getenv("VOLCENGINE_SK", "")

    extractor = ArxivHTMLExtractor(
        html_text_folder=html_text_folder,
        pdf_folder_path=pdf_folder_path,
        arxiv_pool=arxiv_pool_path,
        image_folder_path=image_folder_path,
        json_path=json_output_path,
        volcengine_ak=ak,
        volcengine_sk=sk,
        start_time=start_str,
        end_time=end_str
    )

    extractor.extract_all_htmls()

    # TODO: rongcan: a separater pdf_extractor instead of this fall back logic below
    
    #extractor.pdf_parser_helper.docs = extractor.docs
    #extractor.pdf_parser_helper.remain_docparser()
    #extractor.docs = extractor.pdf_parser_helper.docs
    
    # è®°å½•æ–°æŠ“å–çš„è®ºæ–‡ID
    newly_fetched_ids = [doc.doc_id for doc in extractor.docs]
    
    extractor.serialize_docs()
    
    return newly_fetched_ids



def get_time_str(location = "Asia/Shanghai", count_delay = 2):
    # è®¾å®šæœ¬åœ°æ—¶åŒºï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    local_tz = ZoneInfo(location)  # ä¾‹å¦‚ä¸Šæµ·
    # è·å–æœ¬åœ°å½“å‰æ—¶é—´ï¼Œç²¾ç¡®åˆ°åˆ†é’Ÿ
    local_now = (datetime.now(local_tz)-timedelta(days=count_delay)).replace(second=0, microsecond=0)
    # è½¬æ¢ä¸ºUTCæ—¶é—´
    utc_now = local_now.astimezone(ZoneInfo("UTC"))
    # è½¬ä¸ºæŒ‡å®šæ ¼å¼å­—ç¬¦ä¸²
    utc_str = utc_now.strftime("%Y%m%d%H%M")
    return utc_str


# æ–°å¢å‡½æ•°
def divide_a_day_into(time: str, count: int):
    """
    è¾“å…¥: time (å¦‚202507150856), count (å¦‚3)
    è¾“å‡º: å°†[å‰ä¸€å¤©åŒä¸€æ—¶åˆ», è¾“å…¥æ—¶åˆ»]åˆ†æˆcountä»½ï¼Œè¿”å›æ¯ä¸ªåˆ†æ®µçš„æ—¶é—´ç‚¹å­—ç¬¦ä¸²æ•°ç»„ï¼ˆç²¾ç¡®åˆ°åˆ†é’Ÿï¼Œæ ¼å¼ä¸º%Y%m%d%H%Mï¼‰ï¼ŒåŒ…å«å¤´å°¾ã€‚
    """
    from datetime import datetime, timedelta
    fmt = "%Y%m%d%H%M"
    end_time = datetime.strptime(time, fmt)
    start_time = end_time - timedelta(days=1)
    total_minutes = int((end_time - start_time).total_seconds() // 60)
    step = total_minutes / count
    result = []
    for i in range(count + 1):
        t = start_time + timedelta(minutes=round(i * step))
        result.append(t.strftime(fmt))
    return result
