{
    "doc_id": "2405.20192v2",
    "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
    "authors": [
        "Shuyang Jiang",
        "Yusheng Liao",
        "Ya Zhang",
        "Yanfeng Wang",
        "Yu Wang"
    ],
    "categories": [
        "cs.CL"
    ],
    "published_date": "2024-05-30 15:57:19+00:00",
    "abstract": "Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: Training All parameters but Inferring with\nonly Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn\nusing two general instruction-tuning datasets and evaluate it on seven\ndownstream tasks involving math, reasoning, and knowledge understanding across\nLLMs of different parameter sizes and fine-tuning techniques. Our comprehensive\nexperiments demonstrate that \\trainallInfAttn achieves superior improvements\ncompared to both the fully fine-tuned model and the base model in most\nscenarios, with significant performance gains. The high tolerance of\n\\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning\nand enhances specialized tasks using general data. Code is available in\n\\url{https://github.com/pixas/TAIA_LLM}.",
    "text_chunks": [
        {
            "id": "S1",
            "type": "text",
            "title": "1Introduction",
            "caption": "1Introduction",
            "metadata": {},
            "text": "\n1 Introduction\nLarge language models\u00a0(LLMs) have revolutionized Natural Language Processing\u00a0(NLP), where LLMs have been pretrained on a massive textual corpus and encoded massive world knowledge\u00a0[8, 1]. These models achieve remarkable zero-shot and few-shot performance across a wide range of tasks\u00a0[6, 2, 45, 65, 66].\nThe innovation of instruction tuning, also known as supervised fine-tuning\u00a0(SFT), has further enhanced the instruction-following capabilities of LLMs\u00a0[46, 11], simplifying human-LLM interactions.\nDespite the availability of high-quality data for SFT being limited\u00a0[7, 86], expanding SFT datasets remains a straightforward method to adapt LLMs for specific tasks\u00a0[14]. Various SFT datasets, such as Alpaca\u00a0[49] and Natural Instructions\u00a0[41, 69], have been manually curated or artificially generated to create more generalized instruction-tuned LLMs.\n\nHowever, real-world applications of LLMs are diverse\u00a0[6] and complex\u00a0[34], often making public datasets insufficient.\nWhile synthetic data is useful, it is expensive and tends to exhibit a distribution shift biased towards the parent LLM\u00a0[67].\nConsequently, the data distribution that LLMs adapt to during fine-tuning often differs significantly from that required for specific tasks. This discrepancy leads to inferior performance on specialized tasks and knowledge forgetting due to disruptions in the parametric knowledge stored in LLMs\u00a0[14].\nFigure\u00a01 also shows that with more out-of-distribution\u00a0(OOD) tuning data, the vanilla fine-tune method brings\ncatastrophic forgetting problems, degrading models\u2019 performance on downstream tasks. The scarcity of natural data and the suboptimal quality of synthetic data present substantial challenges to effectively adapting LLMs for specialized tasks. In essence, the dependency on in-domain distribution fine-tuning corpora hampers the broader deployment of LLMs.\n\nTo address this, we propose avoiding such data dependency by leveraging the intrinsic properties of fine-tuning and developing an inference-time method that does not rely on high-quality in-distribution data.\nWe first conduct an in-depth investigation of the internal Transformer architecture.\nWe find that during fine-tuning, LLMs enhance their instruction-following ability, primarily controlled by the self-attention module\u00a0[75].\nConversely, parameterized knowledge is encoded by the key-value intrinsic of the feed-forward network (FFN) module\u00a0[18, 40] during pretraining\u00a0[56].\nFine-tuning primarily elicits this pretrained knowledge\u00a0[71, 46, 59], which remains relatively fixed\u00a0[86].\nThis insight prompts us to discard the FFN updates during fine-tuning, as only a small portion positively contributes to downstream performance, while most disrupt the knowledge when fine-tuned on task-mismatched data.\n\nA naive approach is to fine-tune only the attention parameters, but this fails to generalize to OOD data due to insufficient exploration of non-linearity. To ensure sufficient learning of non-linearity, we introduce additional FFN parameters during fine-tuning but retain only the beneficial self-attention updates. This strategy, named Training-All-Inferring-only-Attention\u00a0(TAIA), achieves both OOD generalization and sufficient optimization space. The comparisons between the proposed method and the vanilla fine-tuning method are shown in Figure\u00a02\n\nWe validate TAIA across seven datasets including math, reasoning, and knowledge understanding, using four LLM families and two fine-tuning techniques.\nExtensive experiments demonstrate the efficacy of TAIA across various model configurations and its outstanding robustness compared to other baselines.\nFurthermore, detailed analyses confirm the reproducibility of TAIA in terms of fine-tuning methods and fine-tuning dataset scales.\nTAIA also maintains the few-shot adaptation ability of base models and withstands multi-level red-teaming attacks.\nIt consistently improves performance in vertical domains like healthcare with increasing OOD data (see Figure\u00a01).\n\nOverall, we conclude our contributions as three-fold:\n\n1.\nNecessity Analysis: We analyze the necessity of leveraging OOD data for effective downstream fine-tuning, revisiting the roles of self-attention and FFN in the Transformer architecture and formalizing their contributions during fine-tuning.\n\n2.\nInference-Time Intervention: We propose a simple yet effective inference-time method that trains all parameters but retains only the self-attention updates. This approach optimizes performance across downstream and closed-book tasks, as validated by extensive experiments.\n\n3.\nExpanding Model Adaptability:\nOur approach introduces an innovative method for utilizing OOD data in fine-tuning LLMs, substantially decreasing the dependence on in-domain data. This advancement enhances the adaptability of LLMs, enabling them to perform exceptionally well across a wider range of specialized and real-world tasks.\n\n\n\n"
        },
        {
            "id": "S2",
            "type": "text",
            "title": "2Preliminaries",
            "caption": "2Preliminaries",
            "metadata": {},
            "text": "\n2 Preliminaries\nSelf-attention module\nLet {ti}i=1Nsuperscriptsubscriptsubscript\ud835\udc61\ud835\udc56\ud835\udc561\ud835\udc41\\{t_{i}\\}_{i=1}^{N}{ italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT represents the inputs to an Transformer-based LLM, and\n{\ud835\udc31i}i=1N\u2208\u211ddsuperscriptsubscriptsubscript\ud835\udc31\ud835\udc56\ud835\udc561\ud835\udc41superscript\u211d\ud835\udc51\\{\\mathbf{x}_{i}\\}_{i=1}^{N}\\in\\mathbb{R}^{d}{ bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT represent the token representations after the embedding layer of a Transformer-based LLM.\nFor each layer l\ud835\udc59litalic_l, LLM initially computes the query, key, and value vectors:\n\n\ud835\udc2aml=fq\u2062(\ud835\udc31ml,m)\ud835\udc24nl=fk\u2062(\ud835\udc31nl,n)\ud835\udc2fnl=fv\u2062(\ud835\udc31nl,n)formulae-sequencesuperscriptsubscript\ud835\udc2a\ud835\udc5a\ud835\udc59subscript\ud835\udc53\ud835\udc5esuperscriptsubscript\ud835\udc31\ud835\udc5a\ud835\udc59\ud835\udc5aformulae-sequencesuperscriptsubscript\ud835\udc24\ud835\udc5b\ud835\udc59subscript\ud835\udc53\ud835\udc58superscriptsubscript\ud835\udc31\ud835\udc5b\ud835\udc59\ud835\udc5bsuperscriptsubscript\ud835\udc2f\ud835\udc5b\ud835\udc59subscript\ud835\udc53\ud835\udc63superscriptsubscript\ud835\udc31\ud835\udc5b\ud835\udc59\ud835\udc5b\\displaystyle\\mathbf{q}_{m}^{l}=f_{q}(\\mathbf{x}_{m}^{l},m)\\quad\\mathbf{k}_{n}%\n^{l}=f_{k}(\\mathbf{x}_{n}^{l},n)\\quad\\mathbf{v}_{n}^{l}=f_{v}(\\mathbf{x}_{n}^{%\nl},n)bold_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_m ) bold_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_n ) bold_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_n )\n\nwhere m,n\ud835\udc5a\ud835\udc5bm,nitalic_m , italic_n are token indexes in the sequence and f{q;k;v}subscript\ud835\udc53\ud835\udc5e\ud835\udc58\ud835\udc63f_{\\{q;k;v\\}}italic_f start_POSTSUBSCRIPT { italic_q ; italic_k ; italic_v } end_POSTSUBSCRIPT are position embeddings parameterized by RoPE\u00a0[61].\nAfter that, the attention score is computed between these two position tokens:\n\n\ud835\udc28ml\u22a4=softmax\u2062(\ud835\udc2aml\u22a4\u2062Wql\u2062Wkl\u22a4\u2062\ud835\udc0aml\u22a4dk)\u2062\ud835\udc15ml\u2062Wvlsuperscriptsubscript\ud835\udc28\ud835\udc5alimit-from\ud835\udc59topsoftmaxsuperscriptsubscript\ud835\udc2a\ud835\udc5alimit-from\ud835\udc59topsuperscriptsubscript\ud835\udc4a\ud835\udc5e\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc58limit-from\ud835\udc59topsuperscriptsubscript\ud835\udc0a\ud835\udc5alimit-from\ud835\udc59topsubscript\ud835\udc51\ud835\udc58superscriptsubscript\ud835\udc15\ud835\udc5a\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc63\ud835\udc59\\displaystyle\\mathbf{o}_{m}^{l\\top}=\\mathrm{softmax}\\left(\\frac{\\mathbf{q}_{m}%\n^{l\\top}W_{q}^{l}W_{k}^{l\\top}\\mathbf{K}_{m}^{l\\top}}{\\sqrt{d_{k}}}\\right)%\n\\mathbf{V}_{m}^{l}W_{v}^{l}bold_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l \u22a4 end_POSTSUPERSCRIPT = roman_softmax ( divide start_ARG bold_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l \u22a4 end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l \u22a4 end_POSTSUPERSCRIPT bold_K start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l \u22a4 end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) bold_V start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT\n(1)\n\nwhere Wql,Wkl,Wvl\u2208\u211dd\u00d7dksuperscriptsubscript\ud835\udc4a\ud835\udc5e\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc58\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc63\ud835\udc59superscript\u211d\ud835\udc51subscript\ud835\udc51\ud835\udc58W_{q}^{l},W_{k}^{l},W_{v}^{l}\\in\\mathbb{R}^{d\\times d_{k}}italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT are learnable weight matrices, d\ud835\udc51ditalic_d is the model dimension and dksubscript\ud835\udc51\ud835\udc58d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is the inner dimension and \ud835\udc0aml=[\ud835\udc241l,\u22ef,\ud835\udc24ml]\u22a4\u2208\u211dm\u00d7d,\ud835\udc15ml=[\ud835\udc2f1l,\u22ef,\ud835\udc2fml]\u22a4\u2208\u211dm\u00d7dformulae-sequencesuperscriptsubscript\ud835\udc0a\ud835\udc5a\ud835\udc59superscriptsuperscriptsubscript\ud835\udc241\ud835\udc59\u22efsuperscriptsubscript\ud835\udc24\ud835\udc5a\ud835\udc59topsuperscript\u211d\ud835\udc5a\ud835\udc51superscriptsubscript\ud835\udc15\ud835\udc5a\ud835\udc59superscriptsuperscriptsubscript\ud835\udc2f1\ud835\udc59\u22efsuperscriptsubscript\ud835\udc2f\ud835\udc5a\ud835\udc59topsuperscript\u211d\ud835\udc5a\ud835\udc51\\mathbf{K}_{m}^{l}=[\\mathbf{k}_{1}^{l},\\cdots,\\mathbf{k}_{m}^{l}]^{\\top}\\in%\n\\mathbb{R}^{m\\times d},\\mathbf{V}_{m}^{l}=[\\mathbf{v}_{1}^{l},\\cdots,\\mathbf{v%\n}_{m}^{l}]^{\\top}\\in\\mathbb{R}^{m\\times d}bold_K start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = [ bold_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , \u22ef , bold_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_d end_POSTSUPERSCRIPT , bold_V start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = [ bold_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , \u22ef , bold_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_m \u00d7 italic_d end_POSTSUPERSCRIPT and we use the single-head notation for simplicity.\nFinally, another projection matrix Wol\u2208\u211dd\u00d7dksuperscriptsubscript\ud835\udc4a\ud835\udc5c\ud835\udc59superscript\u211d\ud835\udc51subscript\ud835\udc51\ud835\udc58W_{o}^{l}\\in\\mathbb{R}^{d\\times d_{k}}italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is used to project \ud835\udc28mlsuperscriptsubscript\ud835\udc28\ud835\udc5a\ud835\udc59\\mathbf{o}_{m}^{l}bold_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT back to token space \ud835\udc31ml=Wol\u2062\ud835\udc28mlsuperscriptsubscript\ud835\udc31\ud835\udc5a\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc5c\ud835\udc59superscriptsubscript\ud835\udc28\ud835\udc5a\ud835\udc59\\mathbf{x}_{m}^{l}=W_{o}^{l}\\mathbf{o}_{m}^{l}bold_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_o start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT.\nSelf-attention with rotary position embedding is more effective for computing contextual mappings\u00a0[84] in arbitrary sequences, particularly in long contexts\u00a0[61]. It incorporates an induction-head mechanism that enables the Transformer architecture to predict co-occurring tokens within a given sequence\u00a0[15, 44] from an in-context perspective. Meanwhile, Wu et\u00a0al. [75] systematically demonstrate that self-attention significantly enhances its instruction-following capability through fine-tuning. Knowledge tokens that do not appear in the context are stored as global tokens in the FFN memory\u00a0[5].\n\nFeed-forward network\u00a0(FFN)\nIn modern transformer architectures, the SiLU\u00a0[55] gating linear unit\u00a0[58] is adopted by various models\u00a0[65, 66, 3].\nIt is formulated as:\n\n\ud835\udc31ml=Wdl\u2062(SiLU\u2062(Wgl\u2062\ud835\udc31ml)\u2299Wul\u2062\ud835\udc31ml)superscriptsubscript\ud835\udc31\ud835\udc5a\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc51\ud835\udc59direct-productSiLUsuperscriptsubscript\ud835\udc4a\ud835\udc54\ud835\udc59superscriptsubscript\ud835\udc31\ud835\udc5a\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc62\ud835\udc59superscriptsubscript\ud835\udc31\ud835\udc5a\ud835\udc59\\displaystyle\\mathbf{x}_{m}^{l}=W_{d}^{l}(\\mathrm{SiLU}(W_{g}^{l}\\mathbf{x}_{m%\n}^{l})\\odot W_{u}^{l}\\mathbf{x}_{m}^{l})bold_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_W start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( roman_SiLU ( italic_W start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) \u2299 italic_W start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT bold_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT )\n(2)\n\nwhere Wgl,Wul\u2208\u211dd\u2032\u00d7dsuperscriptsubscript\ud835\udc4a\ud835\udc54\ud835\udc59superscriptsubscript\ud835\udc4a\ud835\udc62\ud835\udc59superscript\u211dsuperscript\ud835\udc51\u2032\ud835\udc51W_{g}^{l},W_{u}^{l}\\in\\mathbb{R}^{d^{\\prime}\\times d}italic_W start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , italic_W start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u00d7 italic_d end_POSTSUPERSCRIPT, Wdl\u2208\u211dd\u00d7d\u2032superscriptsubscript\ud835\udc4a\ud835\udc51\ud835\udc59superscript\u211d\ud835\udc51superscript\ud835\udc51\u2032W_{d}^{l}\\in\\mathbb{R}^{d\\times d^{\\prime}}italic_W start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT and d\u2032superscript\ud835\udc51\u2032d^{\\prime}italic_d start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is the hidden dimension. \u2299direct-product\\odot\u2299 is the element-wise multiplication and SiLU\u2062(x)=x\u2299sigmoid\u2062(x)SiLU\ud835\udc65direct-product\ud835\udc65sigmoid\ud835\udc65\\mathrm{SiLU}(x)=x\\odot\\mathrm{sigmoid}(x)roman_SiLU ( italic_x ) = italic_x \u2299 roman_sigmoid ( italic_x ).\nThe feed-forward network uses inverse-bottleneck parameterization methods, which inherently enlarges the representation space and eases the encoding of diverse knowledge from different tasks\u00a0[18, 40].\n\nFinetuning towards tasks\nDuring fine-tuning in task-related data, natural practices format data as {instruction,(input),output} pairs: (I,xt,yt)\ud835\udc3csuperscript\ud835\udc65\ud835\udc61superscript\ud835\udc66\ud835\udc61(I,x^{t},y^{t})( italic_I , italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) where t\u2208{1,2,\u22ef,N}\ud835\udc6112\u22ef\ud835\udc41t\\in\\{1,2,\\cdots,N\\}italic_t \u2208 { 1 , 2 , \u22ef , italic_N } and N\ud835\udc41Nitalic_N is the dataset size.\nIn a causal LLM architecture, the learning objective is to minimize the task distribution with the LLM\u2019s internal distribution, via a negative log-likelihood manner:\n\n\u2112\ud835\udf3d=\u22121N\u2062\u2211t=1N\u2211i=1Tlog\u2061p\ud835\udf3d\u2062(yit|y<it,xt,I)subscript\u2112\ud835\udf3d1\ud835\udc41superscriptsubscript\ud835\udc611\ud835\udc41superscriptsubscript\ud835\udc561\ud835\udc47subscript\ud835\udc5d\ud835\udf3dconditionalsuperscriptsubscript\ud835\udc66\ud835\udc56\ud835\udc61superscriptsubscript\ud835\udc66absent\ud835\udc56\ud835\udc61superscript\ud835\udc65\ud835\udc61\ud835\udc3c\\mathcal{L}_{\\boldsymbol{\\theta}}=-\\frac{1}{N}\\sum_{t=1}^{N}\\sum_{i=1}^{T}\\log\np%\n_{\\boldsymbol{\\theta}}(y_{i}^{t}|y_{<i}^{t},x^{t},I)caligraphic_L start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT = - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG \u2211 start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_\u03b8 end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT | italic_y start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_I )\n(3)\n\nwhere T\ud835\udc47Titalic_T is the output sequence length.\nThis objective prompts \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 to converge when the generated response y^tsuperscript^\ud835\udc66\ud835\udc61\\hat{y}^{t}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT matches ytsuperscript\ud835\udc66\ud835\udc61y^{t}italic_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, i.e., the internal distribution of \ud835\udf3d\ud835\udf3d\\boldsymbol{\\theta}bold_italic_\u03b8 aligns with the fine-tuning dataset.\n\n"
        },
        {
            "id": "S3",
            "type": "text",
            "title": "3TAIA:TrainingAll Parameters butInferring with OnlyAttention",
            "caption": "3TAIA:TrainingAll Parameters butInferring with OnlyAttention",
            "metadata": {},
            "text": "\n3 TAIA: Training All Parameters but Inferring with Only Attention\n\n3.1 Motivation\nFine-tuning LLMs for downstream tasks typically requires a substantial amount of high-quality conversational data. While a large volume of high-quality synthetic data generated by GPT-4\u00a0[45] is publicly available and useful for general domains like Math Word Problems and programming, real-world applications of LLMs are diverse\u00a0[6] and complex\u00a0[34]. This diversity renders public datasets insufficient for many scenarios.\nSynthetic data, although useful, is costly and often exhibits distribution shifts towards the parent LLM\u00a0[67]. Consequently, the data distribution achieved through fine-tuning often diverges from that required for specific tasks. The scarcity of natural task-specific data and the suboptimal quality of synthetic data present significant challenges for the effective transfer of LLMs to specific tasks.\n\nTo address these issues and reduce LLMs\u2019 dependency on specialized data, we aim to enable LLMs to perform proficiently on specific tasks using OOD data.\nGiven that the self-attention and feed-forward network (FFN) modules within LLMs function differently, we re-evaluate their respective roles during supervised fine-tuning.\nOur study indicates that OOD fine-tuning introduces noisy parametric knowledge into the FFN memory. Therefore, filtering out noisy parameters while retaining beneficial ones is crucial for generalization\u00a0(\u00a73.2). Through systematic analysis and empirical validation, we demonstrate that the optimal parameter selection strategy is achieved by TAIA, which involves training all parameters but retaining only attention updates\u00a0(\u00a73.3).\n\n\n3.2 Parameter Selection for Out-of-Distribution\u00a0(OOD) SFT\nPrior research has demonstrated that LLMs possess a wide range of task knowledge\u00a0[50, 6, 56] after semi-supervised learning on web data. To enhance their proficiency in specific tasks, domain-related instruction-tuning\u00a0[85] is utilized to improve the knowledge access process\u00a0[71, 46, 59].\nMoreover, studies\u00a0[75] have shown that self-attention improves LLMs ability to follow instructions through fine-tuning, which aids in effective knowledge elicitation. However, when trained on OOD data, the optimization objective\u00a0(Eq.\u00a03) involves significant distribution shifts in certain parameter groups. This can disrupt the pre-trained knowledge encoded through the Transformer\u2019s feed-forward network (FFN)\u00a0[5].\nTherefore, a balanced approach is to disregard the parameters that are noisily disrupted, while preserving the parameters that contribute to held-out tasks. This approach is already endorsed by existing research\u00a0[80].\nSince fine-tuning prioritizes effective instruction following over absorbing potentially misleading knowledge, it can be inferred that the knowledge acquired by the FFNs during fine-tuning could be considered somewhat redundant.\n\n\n3.3 Towards Optimal Parameter Selection Strategy\nBased on the above analysis, a subsequent action is to directly fine-tune only the parameters of the self-attention modules and freeze the FFN ones, which we call TOA\u00a0(Train Only Attention Parameters).\n\nA similar practice to TOA is parameter-efficient fine-tuning\u00a0(PEFT), as they both only train partial parameters. PEFT has achieved a trade-off between performance and training efficiency due to the superfluity of parameters in LLMs\u00a0[21, 80]. We anticipate that TOA could yield results comparable to those obtained by training all parameters, as it essentially represents a form of the PEFT method and has been verified in vision tasks\u00a0[79].\nHowever, as shown in Equation\u00a01, there are few non-linear operations during the attention computation, which inhibits TOA from learning complex representations of the training data. Without sufficient representation exploration, TOA suffers from unlearning of general features via OOD data, despite circumventing catastrophic forgetting problems.\nWe conduct experiments\u00a0(details in Appendix\u00a0E.6) to validate the inferiority of TOA in learning general instruction-following ability and show the results in Figure\u00a03. With FFN modules participating in gradient descents, the performance on the downstream task increases with the proper selection of FFN modules, even if the introduction of FFN modules disrupts pretrained memory. However, TOA still lags far behind the base model, indicating its inability to extract non-intervention features from OOD data.\nTo maintain the advantage of the non-linearity of FFN modules on the update of self-attention modules, we adopt another approach: we add all parameters into the optimizer group. In contrast to the vanilla method, we only maintain the updated self-attention part and reuse the pretrained weights for FFN modules after fine-tuning, which we name Training All parameters but Inferring with only Attention\u00a0(TAIA).\nTAIA guarantees that self-attention can leverage the gradient descent process to optimize its parameters with redundant FFN fine-tuning parameters. The removal of updated FFN parameters during inference, on the other hand, ensures the integrity of parameterized knowledge stored in original FFN modules and the well-learning of beneficial knowledge from OOD data, as supported in Figure\u00a03.\n\n\n3.4 Implementation of TAIA\n\nDuring training, the parameters of FFN and self-attention are updated based on max-likelihood modeling:\n\n\u03b8f\u2062f\u2062n\u2032,\u03b8a\u2062t\u2062t\u2062n\u2032=arg\u2061max{\u03b8}\u2062\u2211i=1Np\u2062(\ud835\udc9ai|\ud835\udc7fi,\u03b8f\u2062f\u2062n,\u03b8a\u2062t\u2062t\u2062n)superscriptsubscript\ud835\udf03\ud835\udc53\ud835\udc53\ud835\udc5b\u2032superscriptsubscript\ud835\udf03\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5b\u2032subscript\ud835\udf03superscriptsubscript\ud835\udc561\ud835\udc41\ud835\udc5dconditionalsubscript\ud835\udc9a\ud835\udc56subscript\ud835\udc7f\ud835\udc56subscript\ud835\udf03\ud835\udc53\ud835\udc53\ud835\udc5bsubscript\ud835\udf03\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5b{\\theta}_{ffn}^{\\prime},{\\theta}_{attn}^{\\prime}=\\arg\\max_{\\{\\theta\\}}\\sum_{i=%\n1}^{N}p(\\boldsymbol{y}_{i}|\\boldsymbol{X}_{i},{\\theta}_{ffn},{\\theta}_{attn})italic_\u03b8 start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_\u03b8 start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = roman_arg roman_max start_POSTSUBSCRIPT { italic_\u03b8 } end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_\u03b8 start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT , italic_\u03b8 start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT )\n(4)\n\nwhere N\ud835\udc41Nitalic_N is the number of training samples and \ud835\udc7fi,\ud835\udc9aisubscript\ud835\udc7f\ud835\udc56subscript\ud835\udc9a\ud835\udc56\\boldsymbol{X}_{i},\\boldsymbol{y}_{i}bold_italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are query and response sequences sampled from any conversational data, like Alpaca-GPT4 or CoT-Collection, respectively. \u03b8(\u22c5)\u2032subscriptsuperscript\ud835\udf03\u2032\u22c5{\\theta}^{\\prime}_{(\\cdot)}italic_\u03b8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT is the updated weight in full fine-tuning or the merged weight of LoRA tuning. After training, TAIA only utilizes the updated attention parameters and reuses the pre-trained FFN parameters to perform inference:\n\n\ud835\udc9a=arg\u2061max\ud835\udc9a\u2062\u2211j=1Klog\u2061p\u2062(yj|\ud835\udc9aj\u22121,\ud835\udc7f,\u03b8f\u2062f\u2062n,\u03b8a\u2062t\u2062t\u2062n\u2032)\ud835\udc9asubscript\ud835\udc9asuperscriptsubscript\ud835\udc571\ud835\udc3e\ud835\udc5dconditionalsubscript\ud835\udc66\ud835\udc57subscript\ud835\udc9a\ud835\udc571\ud835\udc7fsubscript\ud835\udf03\ud835\udc53\ud835\udc53\ud835\udc5bsuperscriptsubscript\ud835\udf03\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5b\u2032\\boldsymbol{y}=\\arg\\max_{\\boldsymbol{y}}\\sum_{j=1}^{K}\\log p(y_{j}|\\boldsymbol%\n{y}_{j-1},\\boldsymbol{X},{\\theta}_{ffn},{\\theta}_{attn}^{\\prime})bold_italic_y = roman_arg roman_max start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | bold_italic_y start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT , bold_italic_X , italic_\u03b8 start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT , italic_\u03b8 start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT )\n(5)\n\nwhere K\ud835\udc3eKitalic_K is the generated sequence length and \ud835\udc7f\ud835\udc7f\\boldsymbol{X}bold_italic_X is the query input to LLMs that shares different distributions with the training data. In this scenario, \u03b8f\u2062f\u2062nsubscript\ud835\udf03\ud835\udc53\ud835\udc53\ud835\udc5b{\\theta}_{ffn}italic_\u03b8 start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT is the original parameter of FFN in pre-trained models and \u03b8a\u2062t\u2062t\u2062n\u2032superscriptsubscript\ud835\udf03\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5b\u2032{\\theta}_{attn}^{\\prime}italic_\u03b8 start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is the updated parameter groups of self-attention in full fine-tuning (or merged weight of self-attention in LoRA tuning).\n\n"
        },
        {
            "id": "S4",
            "type": "text",
            "title": "4Experiments",
            "caption": "4Experiments",
            "metadata": {},
            "text": "\n4 Experiments\n\n4.1 Backbone LLMs\nWe select two LLM families, Qwen1.5\u00a0[3] and LLaMA\u00a0[66] and delicately chose control groups to address the following three concerns: (1) Different Model Sizes within the Same LLM Family: We choose Qwen1.5-1.8B and Qwen1.5-7B to test within the same LLM family, how TAIA works for different sizes of LLMs with the same pretraining data;\n(2) Same Model Size across Different LLM Families: We choose Qwen1.5-7B and LLaMA2-7B to test among different LLM families but the same size, whether TAIA still holds;\nand (3) Impact of Enlarged Pretraining Data: We choose LLaMA2-7B and LLaMA3-8B to test whether TAIA is applicable when the LLM pretraining data is significantly enlarged.\nWe choose the chat version for all models.\n\n\n4.2 Experiment Details\nWe choose two instruction tuning corpus to further demonstrate the high generalization of TAIA under PEFT methods.\nWe choose Alpaca-GPT4-bilingual mixed from Alpaca-GPT4 and Alpaca-GPT4-zh\u00a0[49].\nApart from this, we also adopt CoT-Collection\u00a0[24] which is a mixture of various tasks presented in the Chain-of-Thought\u00a0[72] format.\nWe train 1 epoch for each dataset with the maximum context set to 3072 and the batch size set to 128.\nWe set the learning rate to 2\u2062e\u221242\ud835\udc5242e-42 italic_e - 4 for all runs and adopt LoRA\u00a0[21] and Mixture-of-LoRA\u00a0(MoLoRA)[76, 30] as representative PEFT methods.\nThe LoRA rank is set to 16 and LoRA alpha is set to 32.\nIn MoLoRA, we set the expert count to 4 and activate 1 during inference for all settings.\nAll experiments were conducted on 4 NVIDIA A100 GPUs with ZeRO3\u00a0[53] optimization. For the test set, we selected seven widely used datasets: two for evaluating models\u2019 knowledge understanding and five for testing LLMs\u2019 reasoning ability. A detailed description of these test sets can be found in Appendix\u00a0E.\n\n\n4.3 Quantitative Analysis\nTable\u00a01 presents a comprehensive comparison between various fine-tuning methods (vanilla, LoRA, MoLoRA, and our proposed TAIA) across different training datasets and model backbones.\nThe results clearly demonstrate that TAIA enhances the utilization of training data, consistently outperforming other methods across mentioned benchmarks.\nFor weaker LLMs like Qwen1.5-1.8B and LLaMA2-7B, TAIA significantly amplifies the improvements achieved by standard fine-tuning. For stronger backbones such as Qwen1.5-7B and LLaMA3-8B, standard fine-tuning often degrades performance, but TAIA maintains and even enhances the original capabilities.\nNotably, TAIA-fine-tuned LLaMA3-8B excels in SVAMP and MMB benchmarks, achieving top scores of 85.10 and 59.07, respectively, indicating its robustness in deep math comprehension and medical reasoning tasks.\nFurthermore, in the MMLU benchmark, TAIA-fine-tuned models achieve superior average scores, confirming that TAIA not only protects pretrainpretrained knowledge from disturbance but also enables better knowledge utilization for reasoning. These findings underscore the superior efficacy of TAIA in enhancing LLMs\u2019 performance across diverse reasoning and knowledge domains.\n\n\n\n4.4 Compare with Other OOD Generalization Methods\nWe mainly choose methods aimed for continual learning\u00a0(CL) which also attempts to improve models with incoming OOD training data.\nWe select L2, EWC\u00a0[25], Self-Distill\u00a0[78] and LoRACL, which is a variant of AdapterCL\u00a0[39] as the competitors and the detailed settings of the experiment are discussed in Appendix\u00a0E.7\nTable\u00a02 shows that although CL-based methods can leverage OOD data for downstream tasks, they are ineffective in certain evaluation sets\u00a0(e.g., L2 on MMedBench or LoRACL on CommonsenseQA).\nIt indicates that these methods have specific preferences for downstream tasks and cannot be perfectly applied to any arbitrary application.\nIn contrast, TAIA is not only implementation friendly but also generalizable enough for improving most downstream performances.\n\n\n4.5 Ablation Study\nWe test three variants of TAIA, all designed to reduce distribution shifting after fine-tuning on OOD data: TOA, TOF, and TAIF. The latter two, TOF and TAIF, are similar to TOA and TAIA respectively, but with relevant parameters changed from self-attention to FFN. Experiments were conducted on the Qwen1.5-1.8B model using the same setting described in \u00a74.2. Results are shown in Table\u00a03. We observe that both TAIA and TAIF demonstrate better generalization properties compared to the vanilla method, with TAIA achieving the best. This again confirms the crucial role of self-attention in maintaining the generalization ability of LLMs. In contrast, TOF and TOA both suffer from inadequate parameter exploration and even perform worse than the baseline when tuned on OpenMath\u00a0[64], further supporting the practice of retaining redundant parameters during training.\n\n\n4.6 Representation Analysis\nIn \u00a73.3, we infer that TAIA can obtain more general hidden representations compared to the baseline and TOA.\nWe here examine the generalization of TAIA from a perspective of activation similarities.\nWe define the activation similarity of the i\ud835\udc56iitalic_i-th data sample between two models, \ud835\udf3dpsubscript\ud835\udf3d\ud835\udc5d\\boldsymbol{\\theta}_{p}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and \ud835\udf3dqsubscript\ud835\udf3d\ud835\udc5e\\boldsymbol{\\theta}_{q}bold_italic_\u03b8 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, which are trained separately on two corpora Dpsubscript\ud835\udc37\ud835\udc5dD_{p}italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and Dqsubscript\ud835\udc37\ud835\udc5eD_{q}italic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT with different distributions, as\n\nSim\u2062(\ud835\udc21p,\ud835\udc21q)i=1L\u2062\u2211l=1L\ud835\udc21p\u2062il\u22a4\u2062\ud835\udc21q\u2062il\u2016\ud835\udc21p\u2062il\u2016\u22c5\u2016\ud835\udc21q\u2062il\u2016Simsubscriptsubscript\ud835\udc21\ud835\udc5dsubscript\ud835\udc21\ud835\udc5e\ud835\udc561\ud835\udc3fsuperscriptsubscript\ud835\udc591\ud835\udc3fsuperscriptsubscript\ud835\udc21\ud835\udc5d\ud835\udc56limit-from\ud835\udc59topsuperscriptsubscript\ud835\udc21\ud835\udc5e\ud835\udc56\ud835\udc59\u22c5normsuperscriptsubscript\ud835\udc21\ud835\udc5d\ud835\udc56\ud835\udc59normsuperscriptsubscript\ud835\udc21\ud835\udc5e\ud835\udc56\ud835\udc59\\displaystyle\\mathrm{Sim}(\\mathbf{h}_{p},\\mathbf{h}_{q})_{i}=\\frac{1}{L}\\sum_{%\nl=1}^{L}\\frac{\\mathbf{h}_{pi}^{l\\top}\\mathbf{h}_{qi}^{l}}{\\|\\mathbf{h}_{pi}^{l%\n}\\|\\cdot\\|\\mathbf{h}_{qi}^{l}\\|}roman_Sim ( bold_h start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , bold_h start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_L end_ARG \u2211 start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT divide start_ARG bold_h start_POSTSUBSCRIPT italic_p italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l \u22a4 end_POSTSUPERSCRIPT bold_h start_POSTSUBSCRIPT italic_q italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT end_ARG start_ARG \u2225 bold_h start_POSTSUBSCRIPT italic_p italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u2225 \u22c5 \u2225 bold_h start_POSTSUBSCRIPT italic_q italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT \u2225 end_ARG\n(6)\n\nwhere \ud835\udc21p,\ud835\udc21qsubscript\ud835\udc21\ud835\udc5dsubscript\ud835\udc21\ud835\udc5e\\mathbf{h}_{p},\\mathbf{h}_{q}bold_h start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , bold_h start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT are the activation hidden states after certain modules, and L\ud835\udc3fLitalic_L is the number of hidden layers.\nWe select C-Eval\u00a0[22] as the test and Medical-Collection, a 180K subset of CoT-Collection and OpenMath\u00a0[64] as our training corpus.\nWe follow the same experimental setting as described in \u00a74.2 and present the performance-similarity relations in Figure\u00a05. The results show that a large proportion of activation similarities for TAIA are close to 1, significantly higher than those of other methods. This high activation consistency of TAIA correlates with its superior performance, regardless of the training corpus used. It confirms that by emphasizing instruction-following ability through TAIA, LLMs demonstrate robust generalization performance and effective transferability of training data.\n\n"
        },
        {
            "id": "S5",
            "type": "text",
            "title": "5Analysis",
            "caption": "5Analysis",
            "metadata": {},
            "text": "\n5 Analysis\nIn this section, we discuss the following research questions (RQ) of the TAIA strategy:\n\nRQ1:\nDoes TAIA suit full fine-tuning where the catastrophic forgetting is even more severe?\n\nRQ2:\nWe have confirmed that TAIA learns only the beneficial parts of the fine-tuning data. Does this mean that it can survive in red teaming and enhance the model\u2019s helpfulness?\n\nRQ3:\nHow proficient can TAIA show if we scale the training corpus?\n\nRQ4:\nWang et\u00a0al. [68] finds that supervised fine-tuning hurts LLMs few-shot performance on unseen tasks. Can TAIA restore similar few-shot ability as the base LLM?\n\nRQ5:\nFine-tuning converges to the downstream distribution, leading to the diminishing rank compared to the base LLM. How does the rank change when TAIA is adopted?\n\n\n\n\nResponse to RQ1: TAIA is also applicable to the full fine-tuning technique.\nOur analysis and empirical study focused on PEFT scenarios, mitigating catastrophic forgetting. To test TAIA in a full fine-tuning context, we maintained the same experiment settings as with LoRA tuning but lowered the learning rate to 5\u2062e\u221255\ud835\udc5255e-55 italic_e - 5 for stability and used the CoT-Collection as the fine-tuning corpus. Testing on Qwen1.8b and 7b sizes, the results (Table\u00a04) indicate TAIA maintains superior performance in reasoning tasks (SVAMP, MATH, CommonsenseQA). However, due to extensive parameter modifications during full fine-tuning, TAIA experiences significant catastrophic forgetting in knowledge-intensive tasks (MMLU, MMedBench). Despite this, it still outperforms the vanilla inference method, validating its applicability and generalization in full fine-tuning scenarios.\n\nResponse to RQ2: TAIA significantly reduces harmfulness and improves helpfulness.\nThe analysis and experiments above have demonstrated that TAIA enables LLMs to generalize on OOD data, reducing dependency on data quality. To explore if TAIA can handle training data with harmful information while enhancing LLM usefulness without substantially increasing harmfulness, we followed Qi et\u00a0al. [51] to red-team LLaMA2-7B-chat using three attack levels and evaluated on Advbench\u00a0[10]. We used 100 of the most harmful samples from the Anthropic red team dataset\u00a0[16], 10 identity-shifting samples from Qi et\u00a0al. [51], and benign data from Alpaca-GPT4\u00a0[49]. For models tuned on benign data, we also tested helpfulness on AlpacaEval\u00a0[29]. Results in Table\u00a05 show that TAIA significantly reduces the attack success rate after tuning on three levels of red-teaming data while gaining higher helpfulness from the benign dataset. This demonstrates that careful parameter selection can distill out unsafe parameters and enhance LLM robustness.\n\nResponse to RQ3: TAIA succeeds in varying data sizes.\nTo validate the efficacy of TAIA across different sizes of fine-tuning datasets, we sampled the CoT-Collection dataset to create six fine-tuning corpora of varying sizes: [1K, 10K, 50K, 100K, 200K, 1.8M]. We used the same experimental settings as described in \u00a74. The results, shown in Figure\u00a04(a), indicate that TAIA achieves higher performance more quickly and with less data, demonstrating a more efficient utilization of OOD data. Additionally, unlike vanilla fine-tuning, which experiences significant performance drops when trained on a 1K dataset, TAIA is minimally affected by the distribution gap between its internal distribution and that of the 1,000 samples. This demonstrates the high robustness and generalization capability of TAIA. The full results are detailed in Table\u00a014.\n\nResponse to RQ4: TAIA fully restores the few-shot capability of the base LLM and even improves the performance.\nBrown et\u00a0al. [6] demonstrate the generalization of LLMs as they can adapt to new tasks with few-shot in-context learning.\nAs LLMs are incapable of few-shot learning after SFT on a specific dataset\u00a0[68], we want to verify whether TAIA can maintain the superb few-shot learning ability.We evaluate the few-shot adaptation of TAIA using the same checkpoint trained with CoT-collection and test it in a 100 subset sampled from MATH\u00a0[20].\nResults in Figure\u00a04(b) show that the vanilla fine-tuning method has lost its few-shot learning capability.\nIn contrast, TAIA has regained the ability to learn contextually as the base LLM, achieving performance leap from demonstrations in an approximately linear manner.\n\nResponse to RQ5: TAIA increases the representation rank of self-attention.\nDong et\u00a0al. [13] denote that the residual rank of the representation highly correlates to the final performance of Transformer models.\nThe residual rank of any hidden state \ud835\udc17\u2208\u211dn\u00d7d\ud835\udc17superscript\u211d\ud835\udc5b\ud835\udc51\\mathbf{X}\\in\\mathbb{R}^{n\\times d}bold_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_d end_POSTSUPERSCRIPT can be obtained by \u2016res\u2062(\ud835\udc17)\u20161,\u221e=\u2016\ud835\udc17\u2212\ud835\udf41\u20161,\u221esubscriptnormres\ud835\udc171subscriptnorm\ud835\udc17\ud835\udf411\\|\\mathrm{res}(\\mathbf{X})\\|_{1,\\infty}=\\|\\mathbf{X}-\\boldsymbol{\\mu}\\|_{1,\\infty}\u2225 roman_res ( bold_X ) \u2225 start_POSTSUBSCRIPT 1 , \u221e end_POSTSUBSCRIPT = \u2225 bold_X - bold_italic_\u03bc \u2225 start_POSTSUBSCRIPT 1 , \u221e end_POSTSUBSCRIPT, where \ud835\udf41\u2208\u211dd\ud835\udf41superscript\u211d\ud835\udc51\\boldsymbol{\\mu}\\in\\mathbb{R}^{d}bold_italic_\u03bc \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is the averaged representation and \u2016\ud835\udc17\u20161,\u221e=\u2016\ud835\udc17\u20161\u2062\u2016\ud835\udc17\u2016\u221esubscriptnorm\ud835\udc171subscriptnorm\ud835\udc171subscriptnorm\ud835\udc17\\|\\mathbf{X}\\|_{1,\\infty}=\\sqrt{\\|\\mathbf{X}\\|_{1}\\|\\mathbf{X}\\|_{\\infty}}\u2225 bold_X \u2225 start_POSTSUBSCRIPT 1 , \u221e end_POSTSUBSCRIPT = square-root start_ARG \u2225 bold_X \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 bold_X \u2225 start_POSTSUBSCRIPT \u221e end_POSTSUBSCRIPT end_ARG.\nTo quantify this metric human-friendly, we recompute the ratio between the residual rank and the original rank of \ud835\udc17\ud835\udc17\\mathbf{X}bold_X after each attention module as \u2016res\u2062(\ud835\udc17l)\u20161,\u221e/\u2016\ud835\udc17l\u20161,\u221esubscriptnormressubscript\ud835\udc17\ud835\udc591subscriptnormsubscript\ud835\udc17\ud835\udc591\\|\\mathrm{res}(\\mathbf{X}_{l})\\|_{1,\\infty}/\\|\\mathbf{X}_{l}\\|_{1,\\infty}\u2225 roman_res ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) \u2225 start_POSTSUBSCRIPT 1 , \u221e end_POSTSUBSCRIPT / \u2225 bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 1 , \u221e end_POSTSUBSCRIPT, where l=0,1,\u2026,L\ud835\udc5901\u2026\ud835\udc3fl=0,1,\\ldots,Litalic_l = 0 , 1 , \u2026 , italic_L.\nThe comparisons between the vanilla method, TAIA and TOA trained and evaluated on MATH, are shown in Figure\u00a04(c).\nWe notice a negligible difference between the baseline and TOA, indicating that simply training the self-attention modules does not affect dealing with OOD data.\nNotably, TAIA significantly increases the residual rank of activations of self-attention modules across all layers, which promises high expressiveness and generality.\n\n"
        },
        {
            "id": "S6",
            "type": "text",
            "title": "6Conclusion",
            "caption": "6Conclusion",
            "metadata": {},
            "text": "\n6 Conclusion\nWe revisit the intrinsic properties of LLM fine-tuning and determine that supervised fine-tuning poses minimal requirements for updated FFN parameters. Building on this insight, we introduce TAIA, an inference-time intervention strategy designed to address data scarcity challenges in real-world applications of LLMs.\nTAIA adheres to the traditional fine-tuning technique but only retains updated self-attention parameters for inference. This approach demonstrates superior generalization ability across various contexts. The high generality of TAIA enables effective training using a mixture of limited in-domain data and extensive OOD data. This method enhances LLM performance on downstream tasks while filtering out undesired information from the fine-tuning set, such as hallucination interference or the decline of few-shot capability.\n"
        },
        {
            "id": "Sx1",
            "type": "text",
            "title": "Acknowledgments and Disclosure of Funding",
            "caption": "Acknowledgments and Disclosure of Funding",
            "metadata": {},
            "text": "\nAcknowledgments and Disclosure of Funding\nWe thank Jiangtao Feng for his valuable suggestions on the paper structure. We also thank the anonymous reviewers for their insightful comments and suggestions.\nThis work is supported by National Key R&D Program of China (No. 2022ZD0162101), National Natural Science Foundation of China (No. 62106140) and STCSM (No. 21511101100, No. 22DZ2229005)\n"
        },
        {
            "id": "A1",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix A Future Work\nTAIA has succeeded in harnessing OOD data for fine-tuning LLMs, reducing substantial reliance on in-domain data.\nTo improve this generalization, there are two main directions to expand this adaptability.\nFirstly, we hope to find a minimal set of trainable parameters to guarantee sufficient parameter exploration while reducing distribution aliasing.\nJust as Figure\u00a03 shows, LLMs tuned with [0,18)018[0,18)[ 0 , 18 ) layers of FFN gain higher performance than vanilla fine-tuning with TAIA method.\nCoupled with this observation, it is possible to reduce knowledge overlaps between pretrained parameters and downstream ones through a fine-grained selection of tunable parameters.\nSecondly, an adaptive parameter maintenance strategy instead of the coarse separation of FFN modules can both improve the generalization of LLMs as well as the adaptation of LLMs on knowledge-intensive tasks.\nWe hope our work can provide inspiration on how to improve the parameter utilization of LLMs to adapt to universal distribution datasets.\n"
        },
        {
            "id": "A2",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix B Limitations\nOur experiments are conducted based on the assumption that LLMs have gathered certain task knowledge but cannot well utilize it.\nIn tasks like summarization\u00a0[42] or reading comprehension\u00a0[54], TAIA still needs to learn the task knowledge except for instruction following.\nWe follow the same experiment setting as \u00a74.2 and use the XSum\u00a0[42] training set and SQuAD v2.0\u00a0[54] training set to fine-tune both 1.8B and 7B sizes of Qwen1.5 models.\nTable\u00a06 shows that TAIA is inferior to the vanilla fine-tuning method when it is unfamiliar with the downstream knowledge.\nHowever, the gap is relatively small\u00a0(0.7 on SQuAD and 3.21 R-L on XSum on 1.8b scale model), and TAIA reverses such gap in 7B LLM in SQuAD v2.0 (-0.7\u2192\u2192\\rightarrow\u2192+1.38). For the performance on XSum, it is believed that the model needs to learn specific domain knowledge, such as writing style and word usage preferences, to achieve greater overlap with the reference and thus obtain a higher Rouge score. This shows that TAIA is still applicable to unfamiliar tasks and is significantly suitable for well-pretrained LLMs with sufficient domain knowledge.\n"
        },
        {
            "id": "A3",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix C Broader Impact\nTAIA is designed to optimize the fine-tuned LLMs on downstream tasks after tuning on OOD data, by removing the tuned FFN parameters after the normal fine-tuning process.\nThe potential positive implications imply lower difficulty in applying LLM on data-limited domains, including finance or healthcare, thus increasing the accessibility of LLMs on these scenarios.\nTAIA also makes positive impacts on strengthening the safety and helpfulness of fine-tuned LLMs, and thus brings positive social benefits.\nMoreover, TAIA does not introduce additional costs and is deployment-friendly.\nAs such, we do not foresee any immediate negative ethical or societal consequences stemming from our work that are different from those that apply to enabling LLMs with OOD generalization capability.\n\n"
        },
        {
            "id": "A4",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix D Related Work\nSupervised Fine-tuning\nSupervised Fine-tuning\u00a0(SFT) is a general methodology to adapt base Large language models\u00a0(LLM) to downstream tasks and specific domains.\nBy constructing instruction-input-output pairs on target tasks and training base LLMs on such training data with maximum log-likelihood, open-sourced LLMs pretrained on web data can adapt to various domains via zero-shot or few-shot prompting, including medical\u00a0[74, 9, 30], programming\u00a0[38, 28, 35], finance\u00a0[77, 73, 81], and math word problems\u00a0(MWP)\u00a0[83, 82, 36].\nDue to the large scale of such domain-specific data, fine-tuning the whole large language model is costly; therefore, parameter-efficient fine-tuning\u00a0(PEFT) is proposed to achieve comparable downstream performances with negligible fine-tuning consumption.\nAmong PEFT methods, Low-rank Adaption\u00a0(LoRA)[21] and its variants DoRA\u00a0[33] are the most successful, introducing two trainable low-rank adapters and significantly saving training resources without imposing inference latency\n\nChallenges and Limitations of Fine-tuning\nFine-tuning is a straightforward method for adapting LLMs to various downstream tasks, but it incurs several significant drawbacks, including hallucination, harmfulness, catastrophic forgetting, and safety concerns.\nGekhman et\u00a0al. [17] indicated that fine-tuning instructs the model to produce factually inaccurate responses, as the training process encourages the generation of information not anchored in its pre-existing knowledge base.\nFurthermore, supervised fine-tuning for specific tasks often results in catastrophic forgetting of the initial alignment\u00a0[37] and creates trade-offs between helpfulness and harmlessness\u00a0[4].\nAdditionally, Kumar et\u00a0al. [26] highlighted that fine-tuning significantly reduces the resistance of LLMs to jailbreaking, thereby increasing their vulnerability.\nQi et\u00a0al. [51] also demonstrated that even when benign fine-tuning datasets are used, well-aligned LLMs inevitably become more unsafe and harmful, not to mention the issues arising from red-teaming tuning data.\nIn contrast, TAIA can significantly mitigate these drawbacks while still enhancing helpfulness through fine-tuning. By focusing on the intrinsic properties of fine-tuning and developing an inference-time method that leverages only beneficial self-attention updates, TAIA provides a robust solution to the challenges posed by traditional fine-tuning approaches.\n\n"
        },
        {
            "id": "A5",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix E Experiment Details\n\nE.1 PEFT Settings\nFor LoRA method, we add a LoRA module for each linear layer except the language model head and embedding layer, resulting in the following target modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj].\nFor MoLoRA method, we add a vanilla LoRA module for each linear layer in attention modules and an MoLoRA module for each linear layer in FFN modules.\nThis results in the following attention targets: [q_proj, k_proj, v_proj, o_proj] and FFN targets: [gate_proj, up_proj, down_proj], respectively.\n\n\nE.2 Test Sets Description\nWe here describe the used seven evaluation sets:\n\n1.\nMATH\u00a0[20] is a collection of challenging competition mathematics problems containing 5,000 problems in the test set. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.\n\n2.\nBIG-Bench Hard\u00a0[62]\u00a0(BBH) is a collection of 23 challenging tasks from BIG-Bench. The 6,511 problems are the tasks for which prior language model evaluations did not outperform the average human-rater.\n\n3.\nCommonsenseQA\u00a0[63] is to test models\u2019 ability to answer questions using only the parameterized knowledge instead of the context knowledge. It contains 1,000 problems sourced from ConceptNet\u00a0[60].\n\n4.\nLogiQA\u00a0[31] collects questions about natural language inference\u00a0(NLI) and requires models to infer the conclusion based on provided premises. It contains 653 problems for both English and Chinese subsets.\n\n5.\nSVAMP\u00a0[48] are much simpler datasets compared to MATH, which both test models\u2019 math problem-solving ability. It contains 1,221 problems which are all solvable with one or two simple equations.\n\n6.\nMMedBench\u00a0[52] contains test sets written in six languages for testing models\u2019 capability in healthcare. Its English subset contains 1,273 problems and its Chinese subset contains 3,426 problems. We use MMB./MMed EN and MMed ZH indicates the English and Chinese subset, respectively.\n\n7.\nMMLU\u00a0[19] is to measure LLM\u2019s multitask accuracy, which contains 14,421 problems. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem-solving ability.\n\n\n\nE.3 Fine-tuning Sets Description\n\n1.\nBilingual Alpaca-GPT4 is a dataset composed of Alpaca-GPT4 and Alpaca-GPT4-Zh\u00a0[49], which contains 104k sample data in total.\n\n2.\nCOT Collection\u00a0[24] is a dataset designed to induce Chain-of-Thought\u00a0[72] capabilities into language models. While proprietary LLMs excel at generating Chain-of-Thoughts based on prompting, smaller LMs do not have this capability. Thus, by fine-tuning to generate Chain-of-Thoughts, it could acquire such abilities.\n\n3.\nOpenMaths\u00a0[64] is a math instruction tuning dataset with 1.8M problem-solution pairs generated using a permissively licensed Mixtral-8x7B model\u00a0[23]. The problems are from GSM8K\u00a0[12] and MATH\u00a0[20] training subsets and the solutions are synthetically generated by allowing Mixtral model to use a mix of text reasoning and code blocks executed by Python interpreter.\n\n4.\nMedical Collection is a collection of bilingual medical multiple choice question answering data with Rational, composed of the Chinese and English subset of MMedBench\u00a0[52], CMExam\u00a0[32], and a subset sampled from MedMCQA\u00a0[47]. It comprises a total of approximately 160k questions, with half in English and half in Chinese.\n\n5.\nXsum\u00a0[43] is a dataset for the evaluation of abstractive single-document summarization systems. The dataset consists of 226,711 news articles accompanied with a one-sentence summary. The articles are collected from BBC articles (2010 to 2017) and cover a wide variety of domains (e.g., News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts).\n\n6.\nSQuAD v2.0\u00a0[54] is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD v2.0, the correct answers to questions can be any sequence of tokens in the given text. SQuAD v2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowd workers in forms that are similar to the answerable ones.\n\n\n\nE.4 Evaluation Details\nOur evaluations contain different types of metrics, including Exact Match\u00a0(EM) and Multiple Choice Accuracy\u00a0(Acc).\nFor EM metric, we extract the contents followed by The answer is  to reduce evaluation biases.\nFor different datasets, we adopt different evaluation prompts after the original problem description:\n\n1.\nMATH\u00a0[20], BIG-Bench Hard\u00a0[62]\u00a0(BBH), SVAMP\u00a0[48]: Please format the final answer at the end of the response as: The answer is {answer}.\n\n2.\nCommonsenseQA\u00a0[63]: Let\u2019s think step by step. Please format the final answer at the end of the response as: The answer is {answer}.\n\n3.\nLogiQA\u00a0[31], MMedBench\u00a0[52], MMLU\u00a0[19]: Please answer with option letter directly, do not output other information.\n\nWe use greedy decoding to maintain that all results are reproducible.\n\n\nE.5 Data Mixing Experiments\nWe constructed synthetic mixed datasets to investigate the impact of different out-of-distribution\u00a0(OOD) data ratios on the generalization of TAIA. We selected medical knowledge as the target domain and chose MMedBench as the in-domain data and test set. Additionally, we designed three different mixed data scenarios:\na) Mixing within the same domain. We selected CMExam\u00a0[32], which also pertained to the medical domain, as the OOD data and maintained the mixed dataset size at 20k.\nb) Mixing across different domains. We chose COT, from the general domain, as the OOD data and kept the mixed dataset size at 20k.\nc) Mixing under a constant in-domain data size. We again selected COT as the OOD data, but this time we maintained the in-domain dataset size at 20k.\n\nFor the implementation of fine-tuning, we adopt mixture-of-LoRA\u00a0(MoLoRA) with four experts in total and activate one of them for each token during inference. The rank \u03b1\ud835\udefc\\alphaitalic_\u03b1 of each LoRA in the miture-of-expert\u00a0(MOE) is set to 16 and the scale factor r\ud835\udc5fritalic_r is set to 32.\nWe test the 1.8b and 7b sizes of Qwen1.5, and the results are shown in Fig.\u00a01. It is found that TAIA surpasses the base LLM in all cases, while TOA suffers from performance degradation as the OOD data proportion increases. Additionally, TAIA can utilize OOD data to a greater extent without being adversely affected by the shifted distribution, thereby reducing the model\u2019s dependency on specialized domain datasets. This is particularly significant for tasks with limited data resources.\n\n\nE.6 Layer-wise FFN Fine-tuning Experiments\nPrevious work has found that fine-tuning the FFN module can disrupt the knowledge encoded in the model and that fine-tuning the attention module can enhance the model\u2019s ability to follow instructions.\nHowever, we have discovered that without the assistance of the FFN, merely fine-tuning the attention module alone does not achieve the expected results.\nTo demonstrate this, we conducted experiments where all attention parameters were fine-tuned, while simultaneously fine-tuning FFN parameters at various positions and quantities to observe their impact on model performance. We choose the fine-tuning corpus mixture with 50 percent of OOD data used in the case (b) of the data mixing experiments in Appendix\u00a0E.5.\nWe designed seven experimental setups, including {[0,6),[0,12),[0,18),[0,24),[6,24),[12,24),[18,24)}0601201802462412241824\\{[0,6),[0,12),[0,18),[0,24),[6,24),[12,24),[18,24)\\}{ [ 0 , 6 ) , [ 0 , 12 ) , [ 0 , 18 ) , [ 0 , 24 ) , [ 6 , 24 ) , [ 12 , 24 ) , [ 18 , 24 ) }, where [0,6)06[0,6)[ 0 , 6 ) indicates that only the FFN modules at layer 1 to 6 (with index 0 to 5) are fine-tuned. We choose Qwen1.5-1.8B as the seed LLM and adopt the LoRA with \u03b1=16\ud835\udefc16\\alpha=16italic_\u03b1 = 16 and r=32\ud835\udc5f32r=32italic_r = 32.\nAs shown in Figure\u00a03, the performance of TAIA achieves the best with the fine-tuned FFN modules located at layers 1 to 18.\nBesides, with equal parameter quantities, fine-tuning the FFN in the earlier layers aids in the optimization of the attention layers, whereas the later layers do not yield such an improvement.\nThis shows that the self-attention module functions mostly in early to middle layers, which is also consistent with\u00a0Li et\u00a0al. [27].\n\n\nE.7 Comparing with OOD Generalization Methods\nHere, we present the implementation details of other OOD generalization methods to ensure the reproducibility of all comparisons.\nFor LoRACL, we compare the PPL value of the input prompt between the base model and the LoRA-tuned model and pick the model with lower PPL as the evaluation model.\n\nFor the consolidation method, the fine-tuning loss can be formulated as the sum of the negative log-likelihood\u00a0(Eq.\u00a03) and the regularization items:\n\n\u2112\u03b8r\u2062e\u2062g=\u2112\u03b8+\u03bb\u2062\u2211\u03b8i\u2208{\u03b8}\u03a9i\u2062(\u03b8i\u2212\u03b8i\u2062n\u2062i\u2062t)2subscriptsuperscript\u2112\ud835\udc5f\ud835\udc52\ud835\udc54\ud835\udf03subscript\u2112\ud835\udf03\ud835\udf06subscriptsubscript\ud835\udf03\ud835\udc56\ud835\udf03subscript\u03a9\ud835\udc56superscriptsubscript\ud835\udf03\ud835\udc56subscript\ud835\udf03\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc612\\mathcal{L}^{reg}_{\\theta}=\\mathcal{L}_{\\theta}+\\lambda\\sum_{\\theta_{i}\\in\\{%\n\\theta\\}}\\Omega_{i}(\\theta_{i}-\\theta_{init})^{2}caligraphic_L start_POSTSUPERSCRIPT italic_r italic_e italic_g end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT + italic_\u03bb \u2211 start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 { italic_\u03b8 } end_POSTSUBSCRIPT roman_\u03a9 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_\u03b8 start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n(7)\n\nwhere {\u03b8}\ud835\udf03\\{{\\theta}\\}{ italic_\u03b8 } is the collection of all tunable parameters, \u03b8i\u2062n\u2062i\u2062tsubscript\ud835\udf03\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\\theta_{init}italic_\u03b8 start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT is the parameter of the base model and \u03bb\ud835\udf06\\lambdaitalic_\u03bb is a balance hyperparameter.\nConsidering that we use the LoRA method to fine-tune the model, the updated parameters \u03b8isubscript\ud835\udf03\ud835\udc56\\theta_{i}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT can be calculated as the sum of the LoRA parameter \u0394\u2062\u03b8i\u0394subscript\ud835\udf03\ud835\udc56\\Delta\\theta_{i}roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the base model: \u03b8i=\u0394\u2062\u03b8i+\u03b8i\u2062n\u2062i\u2062tsubscript\ud835\udf03\ud835\udc56\u0394subscript\ud835\udf03\ud835\udc56subscript\ud835\udf03\ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\\theta_{i}=\\Delta\\theta_{i}+\\theta_{init}italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_\u03b8 start_POSTSUBSCRIPT italic_i italic_n italic_i italic_t end_POSTSUBSCRIPT.\nWe adopt two types of consolidation methods\u00a0(L2 and EWC) as the baseline. For L2, we add an L2 constraint to tunable parameters:\n\n\u2112\u03b8L\u20622=\u2112\u03b8+\u03bb\u2062\u2211\u03b8i\u2208{\u03b8}\u0394\u2062\u03b8i2subscriptsuperscript\u2112\ud835\udc3f2\ud835\udf03subscript\u2112\ud835\udf03\ud835\udf06subscriptsubscript\ud835\udf03\ud835\udc56\ud835\udf03\u0394superscriptsubscript\ud835\udf03\ud835\udc562\\mathcal{L}^{L2}_{\\theta}=\\mathcal{L}_{\\theta}+\\lambda\\sum_{\\theta_{i}\\in\\{%\n\\theta\\}}\\Delta\\theta_{i}^{2}caligraphic_L start_POSTSUPERSCRIPT italic_L 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT + italic_\u03bb \u2211 start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 { italic_\u03b8 } end_POSTSUBSCRIPT roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n(8)\n\nFor EWC, we use the Fisher information matrix to measure the distance between the parameters:\n\n\u2112\u03b8E\u2062W\u2062C=\u2112\u03b8+\u03bb\u2062\u2211\u03b8i\u2208{\u03b8}Fi\u2062\u0394\u2062\u03b8i2subscriptsuperscript\u2112\ud835\udc38\ud835\udc4a\ud835\udc36\ud835\udf03subscript\u2112\ud835\udf03\ud835\udf06subscriptsubscript\ud835\udf03\ud835\udc56\ud835\udf03subscript\ud835\udc39\ud835\udc56\u0394superscriptsubscript\ud835\udf03\ud835\udc562\\mathcal{L}^{EWC}_{\\theta}=\\mathcal{L}_{\\theta}+\\lambda\\sum_{\\theta_{i}\\in\\{%\n\\theta\\}}F_{i}\\Delta\\theta_{i}^{2}caligraphic_L start_POSTSUPERSCRIPT italic_E italic_W italic_C end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT + italic_\u03bb \u2211 start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 { italic_\u03b8 } end_POSTSUBSCRIPT italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT\n(9)\n\nwhere Fisubscript\ud835\udc39\ud835\udc56F_{i}italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is estimated by calculating the batch average of the squared gradients of the parameters.\n\nFor Self-Distill, we first use the official prompt of Yang et\u00a0al. [78] to generate the distilled Alpaca-GPT4 dataset using Qwen1.5-1.8B and fine-tune it using the distilled dataset with the same experiment setting with vanilla fine-tuning.\n\n"
        },
        {
            "id": "A6",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix F Supplementary Experiments\n\nF.1 More Results of TAIA on Helpfulness\nWe here discuss the effect of data quality on TAIA.\nWe select two fine-tuning datasets, ShareGPT-52K\u00a0[57] generated from gpt-3.5-turbo and Alpaca-GPT4\u00a0[49] generated from GPT4\u00a0[45].\nShareGPT-52K collects data from both GPT API and websites, so it contains noisy samples or hallucinations, including misspelled words or repeated sentences.\nOn the other hand, Alpaca-GPT4 collects samples generated through Self-Instruct\u00a0[70], in which the automatic generation process guarantees fewer noisy contents.\nWe aim to verify whether TAIA still benefits from low-quality data.\nWe choose Alpaca-Eval\u00a0[29] as the evaluation set which uses GPT4 as the evaluator.\nWe use the same hyperparameter settings as \u00a74.2 to train each model family on these two datasets and display the results in Table\u00a07.\nWe observe that the noisy contents bring a significant decrease in helpfulness across all LLM families with the vanilla fine-tuning technique.\nIn contrast, TAIA reduces such performance drops, especially when tuned in the ShareGPT-52K dataset, and even harvests higher helpfulness than base LLMs when they are relatively weak\u00a0(e.g., Qwen 1.8B and LLaMA2 7B).\n\n\nF.2 Different LoRA Ranks Experiments\nIn this section, we explore the impact of varying the LoRA ranks in the attention and FFN modules on the TAIA. We fine-tuned the Qwen1.5-1.8B model on the medical collection dataset and tested its performance on the general knowledge benchmark under different conditions. We use a\u2062r\ud835\udc4e\ud835\udc5faritalic_a italic_r and f\u2062r\ud835\udc53\ud835\udc5ffritalic_f italic_r to represent the rank of attention and FFN modules, respectively. As shown in Table\u00a013, when the a\u2062r\ud835\udc4e\ud835\udc5faritalic_a italic_r is less than or equal to f\u2062r\ud835\udc53\ud835\udc5ffritalic_f italic_r, the TAIA achieves the best performance among the three cases and significantly surpasses the Vanilla. However, when the a\u2062r\ud835\udc4e\ud835\udc5faritalic_a italic_r is larger than f\u2062r\ud835\udc53\ud835\udc5ffritalic_f italic_r, the performance of the TAIA lags behind the TAIF.\nThis indicates that with more parameter alteration, self-attention will function more as FFN to encode knowledge, which brings significant knowledge overlap with pretrained models and finally results in worse knowledge understanding performances.\n\n\nF.3 Full Results on Dataset Scales\nWe present the full experiment results discussed in RQ3 of \u00a75 in Table\u00a014.\n\n\nF.4 More Discussion on ID/OOD Data\nWe here empirically validate that the mixture of ID and OOD data as the training set of TAIA is a sweet configuration compared to other settings.\nFollowing the experiment setting in Figure\u00a01(c), we fine-tune Qwen1.5-1.8B models with LoRA where r=16,\u03b1=32formulae-sequence\ud835\udc5f16\ud835\udefc32r=16,\\alpha=32italic_r = 16 , italic_\u03b1 = 32. We design three types of methods to differentiate the ID and OOD data during the fine-tuning process:\n\n1.\nRather than mixing the ID and OOD data evenly in Figure\u00a01(c), we adopt linear annealing to gradually decrease the proportion of ID data from 1 to 0 as training progresses.\n\n2.\nWe fine-tune the Qwen1.5-1.8B with 20k ID data in the first stage and 20-80k OOD data in the second stage. Note that we adopt 2-stage fine-tuning based on TAIA-tuned models for better performance.\n\n3.\nWe follow experiment 2\u2019s setting but we fine-tune the Qwen1.5-1.8B with 20-80k OOD data in the first stage and 20k ID data in the second stage.\n\nTable\u00a08 shows that the linear annealing method can mitigate the noise introduced by OOD data, while TAIA can further enhance the model\u2019s ability to utilize OOD data, thereby achieving higher performance on ID data.\nTable\u00a09 indicates that the 2-stage fine-tuning of OOD data causes a serious performance drop. The first stage not only fits the model to the ID data distribution but also causes the model to lose its generalization ability, making it more sensitive to the noise introduced by OOD data. As a result, the 2-stage fine-tuning method severely degraded the model\u2019s performance.\nTable\u00a010 demonstrates that compared to the results of Table\u00a09, the models fine-tuned on the OOD data in the 1-stage retain their generalization ability due to the data diversity of the CoT-Collection. However, the results show that such differentiation of the ID and OOD data still fails to improve performance further in the data mixing scenario.\nAs a result, we use the simple yet effective data mixing setting and adopt one-stage training methodology across the whole paper.\n\n\nF.5 Success of TAIA: A Similarity Perspective\nIn this experiment, we measure similarity to the pre-trained model. This would measure whether TAIA is essentially regularizing the model towards the pre-trained model (similar to what L2 or EWC would do, but apparently better).\nWe compute the hidden state similarity after each layer and average them just as Figure\u00a05\u2019s setting.\nThe results are shown in Table\u00a011.\nThe results reflect that TAIA regularizes the fine-tuned model in an implicit manner without disturbing the learning and parameter exploration, which happens in other regularization methods like L2 and EWC.\n\n\nF.6 Model Scaling of TAIA\n\nTo investigate what happens with even larger scale models, we conduct experiments based on the 14B and 32B sizes of Qwen1.5 using LoRA tuning and Alpaca-GPT4 data and show the results in Table\u00a012 shows that TAIA still outperforms both the base model and LoRA tuning model with Alpaca-GPT4 data, especially in reasoning tasks like MATH and BBH, which further verifies the effectiveness of TAIA.\n\n\n\nF.7 Variability of TAIA\n\nIn this section, we validate that TAIA is a robust method that is hardly affected by random seeds.\nTo this end, we choose Qwen1.5 7B as the base model and Alpaca-GPT4 as the training data, and repeat the training process for three runs.\nThe results are shown in Table\u00a015.\nWe observe that the improvements do not vary among different runs, which emphasizes the robustness of TAIA.\n\n"
        },
        {
            "id": "A7",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix G Formalize the utility of TAIA\n\nSuppose an LLM p\u03b8subscript\ud835\udc5d\ud835\udf03p_{\\theta}italic_p start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT containing pretrained weight \u03b80subscript\ud835\udf030\\theta_{0}italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, the vanilla LoRA-tuned model yields \u0394\u2062\u03b80\u0394subscript\ud835\udf030\\Delta\\theta_{0}roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT weight which is to be merged back to pretrained weight and has a relatively small norm. Suppose a simplified neural network layer with nonlinear operators, a layer normalization layer LayerNorm\u2062(X)=X\u2212\u03bc\u03c3LayerNorm\ud835\udc4b\ud835\udc4b\ud835\udf07\ud835\udf0e\\mathrm{LayerNorm}(X)=\\frac{X-\\mu}{\\sigma}roman_LayerNorm ( italic_X ) = divide start_ARG italic_X - italic_\u03bc end_ARG start_ARG italic_\u03c3 end_ARG and a residual connection:\n\nMW\u2062(X)=Act\u2062(W\u2062x)subscript\ud835\udc40\ud835\udc4a\ud835\udc4bAct\ud835\udc4a\ud835\udc65M_{W}(X)=\\text{Act}(Wx)italic_M start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT ( italic_X ) = Act ( italic_W italic_x )\n(10)\n\nAs the magnitude of \u0394\u2062\u03b80\u0394subscript\ud835\udf030\\Delta\\theta_{0}roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is quite small compared with \u03b80subscript\ud835\udf030\\theta_{0}italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, we perform a first-order Taylor expansion on M\u03b80+\u0394\u2062\u03b80\u2062(X)subscript\ud835\udc40subscript\ud835\udf030\u0394subscript\ud835\udf030\ud835\udc4bM_{\\theta_{0}+\\Delta\\theta_{0}}(X)italic_M start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ):\n\nM\u03b80+\u0394\u2062\u03b80\u2062(X)\u2248M\u03b80\u2062(X)+J\u03b80\u2062(X)\u2062\u0394\u2062\u03b80subscript\ud835\udc40subscript\ud835\udf030\u0394subscript\ud835\udf030\ud835\udc4bsubscript\ud835\udc40subscript\ud835\udf030\ud835\udc4bsubscript\ud835\udc3dsubscript\ud835\udf030\ud835\udc4b\u0394subscript\ud835\udf030M_{\\theta_{0}+\\Delta\\theta_{0}}(X)\\approx M_{\\theta_{0}}(X)+J_{\\theta_{0}}(X)%\n\\Delta\\theta_{0}italic_M start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) \u2248 italic_M start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) + italic_J start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\n(11)\n\nwhere J\u03b80\u2062(X)subscript\ud835\udc3dsubscript\ud835\udf030\ud835\udc4bJ_{\\theta_{0}}(X)italic_J start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) is the Jacobian matrix of M\u03b80\u2062(X)subscript\ud835\udc40subscript\ud835\udf030\ud835\udc4bM_{\\theta_{0}}(X)italic_M start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) for \u03b80subscript\ud835\udf030\\theta_{0}italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. We define z=X+M\u03b80\u2062(X)\ud835\udc67\ud835\udc4bsubscript\ud835\udc40subscript\ud835\udf030\ud835\udc4bz=X+M_{\\theta_{0}}(X)italic_z = italic_X + italic_M start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) and z\u2032=X+M\u03b80\u2062(X)+J\u03b80\u2062(X)\u2062\u0394\u2062\u03b80superscript\ud835\udc67\u2032\ud835\udc4bsubscript\ud835\udc40subscript\ud835\udf030\ud835\udc4bsubscript\ud835\udc3dsubscript\ud835\udf030\ud835\udc4b\u0394subscript\ud835\udf030z^{\\prime}=X+M_{\\theta_{0}}(X)+J_{\\theta_{0}}(X)\\Delta\\theta_{0}italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_X + italic_M start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) + italic_J start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to compare LayerNorm\u2062(z)LayerNormz\\mathrm{LayerNorm(z)}roman_LayerNorm ( roman_z ) and LayerNorm\u2062(z\u2032)LayerNormsuperscript\ud835\udc67\u2032\\text{LayerNorm}(z^{\\prime})LayerNorm ( italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ). Compare the addition of \u0394\u2062\u03b80\u0394subscript\ud835\udf030\\Delta\\theta_{0}roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT inside the transformer module:\n\n\nX=LayerNorm\u2062(z)X\u2032=LayerNorm\u2062(z\u2032)formulae-sequence\ud835\udc4bLayerNorm\ud835\udc67superscript\ud835\udc4b\u2032LayerNormsuperscript\ud835\udc67\u2032X=\\text{LayerNorm}(z)\\quad X^{\\prime}=\\text{LayerNorm}(z^{\\prime})italic_X = LayerNorm ( italic_z ) italic_X start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = LayerNorm ( italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT )\n(12)\n\n\nConsidering z\u2032=z+J\u03b80\u2062(X)\u2062\u0394\u2062\u03b80superscript\ud835\udc67\u2032\ud835\udc67subscript\ud835\udc3dsubscript\ud835\udf030\ud835\udc4b\u0394subscript\ud835\udf030z^{\\prime}=z+J_{\\theta_{0}}(X)\\Delta\\theta_{0}italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = italic_z + italic_J start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, we have\n\n\u03bcz\u2032\u2248\u03bcz+\u03bcJ\u03b80\u2062(X)\u2062\u0394\u2062\u03b80\u03c3z\u2032\u2248\u03c3zformulae-sequencesubscript\ud835\udf07superscript\ud835\udc67\u2032subscript\ud835\udf07\ud835\udc67subscript\ud835\udf07subscript\ud835\udc3dsubscript\ud835\udf030\ud835\udc4b\u0394subscript\ud835\udf030subscript\ud835\udf0esuperscript\ud835\udc67\u2032subscript\ud835\udf0e\ud835\udc67\\mu_{z^{\\prime}}\\approx\\mu_{z}+\\mu_{J_{\\theta_{0}}(X)\\Delta\\theta_{0}}\\quad%\n\\sigma_{z^{\\prime}}\\approx\\sigma_{z}italic_\u03bc start_POSTSUBSCRIPT italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u2248 italic_\u03bc start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT + italic_\u03bc start_POSTSUBSCRIPT italic_J start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_\u03c3 start_POSTSUBSCRIPT italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT \u2248 italic_\u03c3 start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT\n(13)\n\nSince |\u0394\u2062\u03b80|\u0394subscript\ud835\udf030|\\Delta\\theta_{0}|| roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | is quite small compared to \u03b80subscript\ud835\udf030\\theta_{0}italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, J\u03b80\u2062(X)\u2062\u0394\u2062\u03b80subscript\ud835\udc3dsubscript\ud835\udf030\ud835\udc4b\u0394subscript\ud835\udf030J_{\\theta_{0}}(X)\\Delta\\theta_{0}italic_J start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_X ) roman_\u0394 italic_\u03b8 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is also of small norm, which can be considered to be ignored for the mean and standard deviation. Based on it, we have\n\n\nLayerNorm\u2062(z\u2032)\u2248LayerNorm\u2062(z)LayerNormsuperscript\ud835\udc67\u2032LayerNorm\ud835\udc67\\text{LayerNorm}(z^{\\prime})\\approx\\text{LayerNorm}(z)LayerNorm ( italic_z start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) \u2248 LayerNorm ( italic_z )\n\n\nIn other words, if the updated parameter is small enough compared to the pretrained model, the output distribution after each submodule remains consistent with the pretrained model. Meanwhile, we need to prove that the perturbation brought by attention is far lower than that brought by FFN so that the removal of FFN results in higher improvement than the removal of attention modules. Specifically, omitting the multi-head mechanism, self-attention is formalized as such:\n\nAttn\u2062(X)=SoftMax\u2062(X\u2062Wq\u2062(X\u2062Wk)\u22a4dk)\u2062X\u2062WvAttn\ud835\udc4bSoftMax\ud835\udc4bsubscript\ud835\udc4a\ud835\udc5esuperscript\ud835\udc4bsubscript\ud835\udc4a\ud835\udc58topsubscript\ud835\udc51\ud835\udc58\ud835\udc4bsubscript\ud835\udc4a\ud835\udc63\\text{Attn}(X)=\\text{SoftMax}\\left(\\frac{XW_{q}(XW_{k})^{\\top}}{\\sqrt{d_{k}}}%\n\\right)XW_{v}Attn ( italic_X ) = SoftMax ( divide start_ARG italic_X italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_X italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_X italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT\n(14)\n\nwhere X\u2208\u211dn\u00d7d\ud835\udc4bsuperscript\u211d\ud835\udc5b\ud835\udc51X\\in\\mathbb{R}^{n\\times d}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_d end_POSTSUPERSCRIPT, and Wq,Wk\u2208\u211dd\u00d7dk,Wv\u2208\u211dd\u00d7dvformulae-sequencesubscript\ud835\udc4a\ud835\udc5esubscript\ud835\udc4a\ud835\udc58superscript\u211d\ud835\udc51subscript\ud835\udc51\ud835\udc58subscript\ud835\udc4a\ud835\udc63superscript\u211d\ud835\udc51subscript\ud835\udc51\ud835\udc63W_{q},W_{k}\\in\\mathbb{R}^{d\\times d_{k}},W_{v}\\in\\mathbb{R}^{d\\times d_{v}}italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, respectively.\n\nFor small perturbations \u0394\u2062Wq,\u0394\u2062Wk,\u0394\u2062Wv\u0394subscript\ud835\udc4a\ud835\udc5e\u0394subscript\ud835\udc4a\ud835\udc58\u0394subscript\ud835\udc4a\ud835\udc63\\Delta W_{q},\\Delta W_{k},\\Delta W_{v}roman_\u0394 italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , roman_\u0394 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , roman_\u0394 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, the perturbed Attention output is:\n\nAttn\u0394\u2062(X)=SoftMax\u2062(X\u2062(Wq+\u0394\u2062Wq)\u2062(X\u2062(Wk+\u0394\u2062Wk))\u22a4dk)\u2062X\u2062(Wv+\u0394\u2062Wv)subscriptAttn\u0394\ud835\udc4bSoftMax\ud835\udc4bsubscript\ud835\udc4a\ud835\udc5e\u0394subscript\ud835\udc4a\ud835\udc5esuperscript\ud835\udc4bsubscript\ud835\udc4a\ud835\udc58\u0394subscript\ud835\udc4a\ud835\udc58topsubscript\ud835\udc51\ud835\udc58\ud835\udc4bsubscript\ud835\udc4a\ud835\udc63\u0394subscript\ud835\udc4a\ud835\udc63\\text{Attn}_{\\Delta}(X)=\\text{SoftMax}\\left(\\frac{X(W_{q}+\\Delta W_{q})(X(W_{k%\n}+\\Delta W_{k}))^{\\top}}{\\sqrt{d_{k}}}\\right)X(W_{v}+\\Delta W_{v})Attn start_POSTSUBSCRIPT roman_\u0394 end_POSTSUBSCRIPT ( italic_X ) = SoftMax ( divide start_ARG italic_X ( italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + roman_\u0394 italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) ( italic_X ( italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + roman_\u0394 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_X ( italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT + roman_\u0394 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )\n(15)\n\nUsing the first-order Taylor expansion, we have:\n\n\u0394\u2062Attn\u2062(X)\u2248\u2202Attn\u2062(X)\u2202Wq\u2062\u0394\u2062Wq+\u2202Attn\u2062(X)\u2202Wk\u2062\u0394\u2062Wk+\u2202Attn\u2062(X)\u2202Wv\u2062\u0394\u2062Wv\u0394Attn\ud835\udc4bAttn\ud835\udc4bsubscript\ud835\udc4a\ud835\udc5e\u0394subscript\ud835\udc4a\ud835\udc5eAttn\ud835\udc4bsubscript\ud835\udc4a\ud835\udc58\u0394subscript\ud835\udc4a\ud835\udc58Attn\ud835\udc4bsubscript\ud835\udc4a\ud835\udc63\u0394subscript\ud835\udc4a\ud835\udc63\\Delta\\text{Attn}(X)\\approx\\frac{\\partial\\text{Attn}(X)}{\\partial W_{q}}\\Delta\nW%\n_{q}+\\frac{\\partial\\text{Attn}(X)}{\\partial W_{k}}\\Delta W_{k}+\\frac{\\partial%\n\\text{Attn}(X)}{\\partial W_{v}}\\Delta W_{v}roman_\u0394 Attn ( italic_X ) \u2248 divide start_ARG \u2202 Attn ( italic_X ) end_ARG start_ARG \u2202 italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG roman_\u0394 italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + divide start_ARG \u2202 Attn ( italic_X ) end_ARG start_ARG \u2202 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG roman_\u0394 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + divide start_ARG \u2202 Attn ( italic_X ) end_ARG start_ARG \u2202 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_ARG roman_\u0394 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT\n(16)\n\n\nUsing the first-order Taylor expansion and the partial derivatives of softmax, we obtain the perturbation bound of attention:\n\n\u2225\u0394Attn(X)\u2225\u22641dk(\u2225(diag(P)\u2212PP\u22a4)X(XWk)\u22a4XWv\u2225\u2225\u0394Wq\u2225\\displaystyle\\|\\Delta\\text{Attn}(X)\\|\\leq\\frac{1}{\\sqrt{d_{k}}}(\\|(\\text{diag}%\n(P)-PP^{\\top})X(XW_{k})^{\\top}XW_{v}\\|\\|\\Delta W_{q}\\|\u2225 roman_\u0394 Attn ( italic_X ) \u2225 \u2264 divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ( \u2225 ( diag ( italic_P ) - italic_P italic_P start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ) italic_X ( italic_X italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_X italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2225 \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT \u2225\n\n\n+\u2225(diag(P)\u2212PP\u22a4)XWq\u22a4XXWv\u2225\u2225\u0394Wk\u2225)+\u2225P\u22a4X\u2225\u2225\u0394Wv\u2225\\displaystyle+\\|(\\text{diag}(P)-PP^{\\top})XW_{q}^{\\top}XXW_{v}\\|\\|\\Delta W_{k}%\n\\|)+\\|P^{\\top}X\\|\\|\\Delta W_{v}\\|+ \u2225 ( diag ( italic_P ) - italic_P italic_P start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ) italic_X italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_X italic_X italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2225 \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 ) + \u2225 italic_P start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT italic_X \u2225 \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2225\n\nwhere P=SoftMax\u2062(X\u2062Wq\u2062(X\u2062Wk)\u22a4dk)\ud835\udc43SoftMax\ud835\udc4bsubscript\ud835\udc4a\ud835\udc5esuperscript\ud835\udc4bsubscript\ud835\udc4a\ud835\udc58topsubscript\ud835\udc51\ud835\udc58P=\\text{SoftMax}(\\frac{XW_{q}(XW_{k})^{\\top}}{\\sqrt{d_{k}}})italic_P = SoftMax ( divide start_ARG italic_X italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_X italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) We simplify the bound as such \u2016\u0394\u2062Attn\u2062(X)\u2016\u2264Ca\u2062t\u2062t\u2062n\u2062(\u2016\u0394\u2062Wq\u2016+\u2016\u0394\u2062Wk\u2016+\u2016\u0394\u2062Wv\u2016)norm\u0394Attn\ud835\udc4bsubscript\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5bnorm\u0394subscript\ud835\udc4a\ud835\udc5enorm\u0394subscript\ud835\udc4a\ud835\udc58norm\u0394subscript\ud835\udc4a\ud835\udc63\\|\\Delta\\text{Attn}(X)\\|\\leq C_{attn}(\\|\\Delta W_{q}\\|+\\|\\Delta W_{k}\\|+\\|%\n\\Delta W_{v}\\|)\u2225 roman_\u0394 Attn ( italic_X ) \u2225 \u2264 italic_C start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT ( \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT \u2225 + \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 + \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2225 ) where Ca\u2062t\u2062t\u2062n=\u2016X\u20163\u2062\u2016Wk\u2016\u2062\u2016Wv\u2016dk\u2062\u2016diag\u2062(P)\u2212P\u2062P\u22a4\u2016subscript\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5bsuperscriptnorm\ud835\udc4b3normsubscript\ud835\udc4a\ud835\udc58normsubscript\ud835\udc4a\ud835\udc63subscript\ud835\udc51\ud835\udc58normdiag\ud835\udc43\ud835\udc43superscript\ud835\udc43topC_{attn}=\\frac{\\|X\\|^{3}\\|W_{k}\\|\\|W_{v}\\|}{\\sqrt{d_{k}}}\\|\\text{diag}(P)-PP^{%\n\\top}\\|italic_C start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT = divide start_ARG \u2225 italic_X \u2225 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT \u2225 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 \u2225 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2225 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG \u2225 diag ( italic_P ) - italic_P italic_P start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT \u2225.\n\nIn terms of FFN modules, we use ReLU as the activation function instead of SwiGLU in FFN, which is defined as:\n\nFFN\u2062(X)=ReLU\u2062(W1\u2062(ReLU\u2062(W2\u2062x+b2))+b1)FFN\ud835\udc4bReLUsubscript\ud835\udc4a1ReLUsubscript\ud835\udc4a2\ud835\udc65subscript\ud835\udc4f2subscript\ud835\udc4f1\\text{FFN}(X)=\\text{ReLU}(W_{1}(\\text{ReLU}(W_{2}x+b_{2}))+b_{1})FFN ( italic_X ) = ReLU ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( ReLU ( italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )\n(17)\n\nwhere W1\u2208\u211dd\u00d74\u2062dsubscript\ud835\udc4a1superscript\u211d\ud835\udc514\ud835\udc51W_{1}\\in\\mathbb{R}^{d\\times 4d}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 4 italic_d end_POSTSUPERSCRIPT, W2\u2208\u211d4\u2062d\u00d7dsubscript\ud835\udc4a2superscript\u211d4\ud835\udc51\ud835\udc51W_{2}\\in\\mathbb{R}^{4d\\times d}italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT 4 italic_d \u00d7 italic_d end_POSTSUPERSCRIPT.\nFor small perturbations \u0394\u2062W1,\u0394\u2062W2,\u0394\u2062b1,\u0394\u2062b2\u0394subscript\ud835\udc4a1\u0394subscript\ud835\udc4a2\u0394subscript\ud835\udc4f1\u0394subscript\ud835\udc4f2\\Delta W_{1},\\Delta W_{2},\\Delta b_{1},\\Delta b_{2}roman_\u0394 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , roman_\u0394 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , roman_\u0394 italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , roman_\u0394 italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, the perturbed FFN output is:\n\nFFN\u0394\u2062(X)=ReLU\u2062((W1+\u0394\u2062W1)\u2062(ReLU\u2062((W2+\u0394\u2062W2)\u2062X+b2+\u0394\u2062b2))+b1+\u0394\u2062b1)subscriptFFN\u0394\ud835\udc4bReLUsubscript\ud835\udc4a1\u0394subscript\ud835\udc4a1ReLUsubscript\ud835\udc4a2\u0394subscript\ud835\udc4a2\ud835\udc4bsubscript\ud835\udc4f2\u0394subscript\ud835\udc4f2subscript\ud835\udc4f1\u0394subscript\ud835\udc4f1\\text{FFN}_{\\Delta}(X)=\\text{ReLU}((W_{1}+\\Delta W_{1})(\\text{ReLU}((W_{2}+%\n\\Delta W_{2})X+b_{2}+\\Delta b_{2}))+b_{1}+\\Delta b_{1})FFN start_POSTSUBSCRIPT roman_\u0394 end_POSTSUBSCRIPT ( italic_X ) = ReLU ( ( italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + roman_\u0394 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ( ReLU ( ( italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + roman_\u0394 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) italic_X + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + roman_\u0394 italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + roman_\u0394 italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )\n(18)\n\nSimilar to the above analysis, we obtain the perturbation bound of FFN modules:\n\n\u2016\u0394\u2062FFN\u2062(X)\u2016\u2264\u2016f\u2032\u2062(Y)\u2062X\u2062W1\u2062f\u2032\u2062(X\u2062W2+b2)\u2016\u2062\u2016\u0394\u2062W2\u2016+\u2016f\u2032\u2062(Y)\u2062W1\u2062f\u2032\u2062(X2+b2)\u2016\u2062\u2016\u0394\u2062b2\u2016norm\u0394FFN\ud835\udc4bnormsuperscript\ud835\udc53\u2032\ud835\udc4c\ud835\udc4bsubscript\ud835\udc4a1superscript\ud835\udc53\u2032\ud835\udc4bsubscript\ud835\udc4a2subscript\ud835\udc4f2norm\u0394subscript\ud835\udc4a2normsuperscript\ud835\udc53\u2032\ud835\udc4csubscript\ud835\udc4a1superscript\ud835\udc53\u2032subscript\ud835\udc4b2subscript\ud835\udc4f2norm\u0394subscript\ud835\udc4f2\\displaystyle\\|\\Delta\\text{FFN}(X)\\|\\leq\\|f^{\\prime}(Y)XW_{1}f^{\\prime}(XW_{2}%\n+b_{2})\\|\\|\\Delta W_{2}\\|+\\|f^{\\prime}(Y)W_{1}f^{\\prime}(X_{2}+b_{2})\\|\\|%\n\\Delta b_{2}\\|\u2225 roman_\u0394 FFN ( italic_X ) \u2225 \u2264 \u2225 italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_Y ) italic_X italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_X italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2225 \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225 + \u2225 italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_Y ) italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2225 \u2225 roman_\u0394 italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225\n\n\n+\u2016f\u2032\u2062(Y)\u2062Y\u2016\u2062\u2016\u0394\u2062W1\u2016+\u2016f\u2032\u2062(Y)\u2016\u2062\u2016\u0394\u2062b1\u2016normsuperscript\ud835\udc53\u2032\ud835\udc4c\ud835\udc4cnorm\u0394subscript\ud835\udc4a1normsuperscript\ud835\udc53\u2032\ud835\udc4cnorm\u0394subscript\ud835\udc4f1\\displaystyle+\\|f^{\\prime}(Y)Y\\|\\|\\Delta W_{1}\\|+\\|f^{\\prime}(Y)\\|\\|\\Delta b_{%\n1}\\|+ \u2225 italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_Y ) italic_Y \u2225 \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 + \u2225 italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_Y ) \u2225 \u2225 roman_\u0394 italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225\n\n\nwhere f\u2032superscript\ud835\udc53\u2032f^{\\prime}italic_f start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is the derivative of ReLU and Y=ReLU\u2062(X\u2062W2+b2)\ud835\udc4cReLU\ud835\udc4bsubscript\ud835\udc4a2subscript\ud835\udc4f2Y=\\text{ReLU}(XW_{2}+b_{2})italic_Y = ReLU ( italic_X italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) and we simplify the bound as such: \u2016\u0394\u2062FFN\u2062(X)\u2016\u2264Cf\u2062f\u2062n\u2062(\u2016\u0394\u2062W2\u2016+\u2016\u0394\u2062W1\u2016+\u2016\u0394\u2062b1\u2016+\u2016\u0394\u2062b2\u2016)norm\u0394FFN\ud835\udc4bsubscript\ud835\udc36\ud835\udc53\ud835\udc53\ud835\udc5bnorm\u0394subscript\ud835\udc4a2norm\u0394subscript\ud835\udc4a1norm\u0394subscript\ud835\udc4f1norm\u0394subscript\ud835\udc4f2\\|\\Delta\\text{FFN}(X)\\|\\leq C_{ffn}(\\|\\Delta W_{2}\\|+\\|\\Delta W_{1}\\|+\\|\\Delta\nb%\n_{1}\\|+\\|\\Delta b_{2}\\|)\u2225 roman_\u0394 FFN ( italic_X ) \u2225 \u2264 italic_C start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT ( \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225 + \u2225 roman_\u0394 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 + \u2225 roman_\u0394 italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 + \u2225 roman_\u0394 italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225 ) where Cf\u2062f\u2062n=n\u2062d\u2062\u2016X\u2016\u22c5\u2016W1\u2062|b1\u2016\u22c5\u2016W2|\u2062b2\u2016subscript\ud835\udc36\ud835\udc53\ud835\udc53\ud835\udc5b\u22c5\ud835\udc5b\ud835\udc51norm\ud835\udc4bnorm\u22c5subscript\ud835\udc4a1delimited-|\u2016subscript\ud835\udc4f1delimited-\u2016|subscript\ud835\udc4a2subscript\ud835\udc4f2C_{ffn}=nd\\|X\\|\\cdot\\|W_{1}|b_{1}\\|\\cdot\\|W_{2}|b_{2}\\|italic_C start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT = italic_n italic_d \u2225 italic_X \u2225 \u22c5 \u2225 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 \u22c5 \u2225 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225.\n\nThe SoftMax function\u2019s normalization limits the perturbation\u2019s amplification and hence limits the \u2016diag\u2062(P)\u2212P\u2062P\u22a4\u2016\u22641normdiag\ud835\udc43\ud835\udc43superscript\ud835\udc43top1\\|\\text{diag}(P)-PP^{\\top}\\|\\leq 1\u2225 diag ( italic_P ) - italic_P italic_P start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT \u2225 \u2264 1 and the overall perturbation. Besides, \u2016X\u201622\u223c\u03c72,\u2016X\u2016\u2248dformulae-sequencesimilar-tosuperscriptsubscriptnorm\ud835\udc4b22superscript\ud835\udf122norm\ud835\udc4b\ud835\udc51\\|X\\|_{2}^{2}\\sim\\chi^{2},\\|X\\|\\ \\approx\\sqrt{d}\u2225 italic_X \u2225 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT \u223c italic_\u03c7 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , \u2225 italic_X \u2225 \u2248 square-root start_ARG italic_d end_ARG. Due to ReLU\u2019s non-linear activation, perturbations are significantly amplified around the non-linear activations, especially in the high-dimensional space. Based on the above comparison\n\nCa\u2062t\u2062t\u2062n\u2248d\u2062\u2016Wk\u2016\u2062\u2016Wv\u2016\u2062\u2016diag\u2062(P)\u2212P\u2062P\u22a4\u2016\u226aCf\u2062f\u2062n=n\u2062d\u2062d\u22c5\u2016W1\u2062|b1\u2016\u22c5\u2016W2|\u2062b2\u2016subscript\ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc5b\ud835\udc51normsubscript\ud835\udc4a\ud835\udc58normsubscript\ud835\udc4a\ud835\udc63normdiag\ud835\udc43\ud835\udc43superscript\ud835\udc43topmuch-less-thansubscript\ud835\udc36\ud835\udc53\ud835\udc53\ud835\udc5b\u22c5\ud835\udc5b\ud835\udc51\ud835\udc51norm\u22c5subscript\ud835\udc4a1delimited-|\u2016subscript\ud835\udc4f1delimited-\u2016|subscript\ud835\udc4a2subscript\ud835\udc4f2C_{attn}\\approx d\\|W_{k}\\|\\|W_{v}\\|\\|\\text{diag}(P)-PP^{\\top}\\|\\ll C_{ffn}=nd%\n\\sqrt{d}\\cdot\\|W_{1}|b_{1}\\|\\cdot\\|W_{2}|b_{2}\\|italic_C start_POSTSUBSCRIPT italic_a italic_t italic_t italic_n end_POSTSUBSCRIPT \u2248 italic_d \u2225 italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 \u2225 italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2225 \u2225 diag ( italic_P ) - italic_P italic_P start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT \u2225 \u226a italic_C start_POSTSUBSCRIPT italic_f italic_f italic_n end_POSTSUBSCRIPT = italic_n italic_d square-root start_ARG italic_d end_ARG \u22c5 \u2225 italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 \u22c5 \u2225 italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2225\n(19)\n\nwhich implies\n\n\u2016\u0394\u2062Attn\u2062(X)\u2016<\u2016\u0394\u2062FFN\u2062(X)\u2016norm\u0394Attn\ud835\udc4bnorm\u0394FFN\ud835\udc4b\\|\\Delta\\text{Attn}(X)\\|<\\|\\Delta\\text{FFN}(X)\\|\u2225 roman_\u0394 Attn ( italic_X ) \u2225 < \u2225 roman_\u0394 FFN ( italic_X ) \u2225\n(20)\n\n\nThus, for equal-magnitude parameter perturbations, the output perturbation bound of the Attention module is less than that of the FFN module. This indicates that the FFN updates will introduce great perturbation to the distribution of transformers and narrow the similarity between tuned models and pre-trained models.\n"
        },
        {
            "id": "A8",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix H Case Study\nHere we present three cases of TAIA and vanilla method evaluated on Advbench in Figure\u00a06,\u00a07 and\u00a08, which are tuned on explicitly harmful contents, identity shifting contents and benign contents, respectively.\nTAIA produces harmless contents against severe jailbreaking, albeit undertaking red teaming tuning.\n\n\n"
        }
    ],
    "figure_chunks": [
        {
            "id": "S0.F1",
            "type": "figure",
            "title": "2405.20192v2_Figure1",
            "caption": "(a)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure1.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S0.F1.sf1",
            "type": "figure",
            "title": "2405.20192v2_Figure1(1)",
            "caption": "(a)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure1(1).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S0.F1.sf2",
            "type": "figure",
            "title": "2405.20192v2_Figure1(2)",
            "caption": "(b)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure1(2).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S0.F1.sf3",
            "type": "figure",
            "title": "2405.20192v2_Figure1(3)",
            "caption": "(c)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure1(3).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S1.F2",
            "type": "figure",
            "title": "2405.20192v2_Figure2",
            "caption": "Figure 2:Comparison between different fine-tuning and inference methods. Parameters colored withgreenandyellowrepresent models finetuned with in-domain and out-of-distribution data, respectively. \u201cID\u201d and \u201cOOD\u201d represents in-distribution and out-of-distribution, respectively. When we train in-domain data (colored asgreen) and out-of-domain data (colored asyellow) and evaluate in in-domain test sets and out-of-domain test sets, respectively (The second row; fine-tuning). The vanilla fine-tuning method can only perform well when trained on ID data and evaluated in ID test sets. Compared to vanilla tuning, TAIA can perform generally well on both types of test sets when given OOD data. As a similar approach that only trains attention, TOA (Train-only-attention) performs badly on both types of evaluation sets as it loses sufficient exploration of optimal parameter groups.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure2.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S3.F3",
            "type": "figure",
            "title": "2405.20192v2_Figure3",
            "caption": "Figure 3:Performance of TOA andTAIAwith the layer-wise FFN LoRA. All models are equipped with attention LoRA at each layer and fine-tuned on a corpus mixture with 50% OOD data.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure3.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F4",
            "type": "figure",
            "title": "2405.20192v2_Figure4",
            "caption": "(a)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure4.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F4.sf1",
            "type": "figure",
            "title": "2405.20192v2_Figure4(1)",
            "caption": "(a)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure4(1).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F4.sf2",
            "type": "figure",
            "title": "2405.20192v2_Figure4(2)",
            "caption": "(b)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure4(2).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F4.sf3",
            "type": "figure",
            "title": "2405.20192v2_Figure4(3)",
            "caption": "(c)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure4(3).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "A0.F5",
            "type": "figure",
            "title": "2405.20192v2_Figure5",
            "caption": "(a)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure5.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "A0.F5.sf1",
            "type": "figure",
            "title": "2405.20192v2_Figure5(1)",
            "caption": "(a)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure5(1).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "A0.F5.sf2",
            "type": "figure",
            "title": "2405.20192v2_Figure5(2)",
            "caption": "(b)",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure5(2).png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "A8.F6",
            "type": "figure",
            "title": "2405.20192v2_Figure6",
            "caption": "Figure 6:Comparison betweenTAIAand vanilla fine-tuning method on Advbench with explicitly harmful tuning contents. We choose LLaMA2-7B-chat as the base LLM. Wehighlightthe harmful contents generated by the vanilla method anddenotethe rejective contents produced byTAIA.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure6.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "A8.F7",
            "type": "figure",
            "title": "2405.20192v2_Figure7",
            "caption": "Figure 7:Comparison betweenTAIAand vanilla fine-tuning method on Advbench with identity shifting tuning contents. We choose LLaMA2-7B-chat as the base LLM. Wehighlightthe harmful contents generated by the vanilla method anddenotethe rejective contents produced byTAIA.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure7.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "A8.F8",
            "type": "figure",
            "title": "2405.20192v2_Figure8",
            "caption": "Figure 8:Comparison betweenTAIAand vanilla fine-tuning method on Advbench with benign tuning contents\u00a0(Alpaca-GPT4). We choose LLaMA2-7B-chat as the base LLM. Wehighlightthe harmful contents generated by the vanilla method anddenotethe rejective contents produced byTAIA.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.20192v2_Figure8.png",
            "alt_text": "Refer to caption"
        }
    ],
    "table_chunks": [
        {
            "id": "S3.T1",
            "type": "table",
            "title": "2405.20192v2_Table1",
            "caption": "Table 1:Validation experiments using two training corpus and four seed backbones across seven test sets. \u201cCQA.\u201d refers to CommonsenseQA and \u201cMMB.\u201d refers to the English subset of MMedbench benchmark. \u201cFT Method\u201d denotes the fine-tuning method, which is either LoRA or MoLoRA.Boldindicates the optimal result in each subgroup andunderlineindicates the suboptimal result. TheTAIAsetting achieves optimal fine-tuning results in most cases.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.7.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt\" id=\"S3.T1.7.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.1.1.1\"><span class=\"ltx_text\" id=\"S3.T1.7.1.1.1.1.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.1.1.2\"> <span class=\"ltx_text\" id=\"S3.T1.7.1.1.1.1.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.7.1.1.1.1.2.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.7.1.1.1.1.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.1.1.1.1.2.1.1.1.1\">Training</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.7.1.1.1.1.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.1.1.1.1.2.1.1.2.1\">Dataset</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S3.T1.7.1.1.1.1.2.2\"></span></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt\" id=\"S3.T1.7.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.2.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt\" id=\"S3.T1.7.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.3.1\">FT Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_tt\" id=\"S3.T1.7.1.1.4\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.4.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_tt\" colspan=\"5\" id=\"S3.T1.7.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.5.1\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_tt\" colspan=\"2\" id=\"S3.T1.7.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.6.1\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt\" id=\"S3.T1.7.1.1.7\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.1.7.1\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.1.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.2.1\">BBH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.3.1\">CQA.</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.4.1\">LogiQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.5.1\">SVAMP</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.6.1\">MMB.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.2.7.1\">MMLU</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.3.1.1\">Base Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.2\">Qwen1.5-1.8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.3\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.3.4\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.5\">4.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.6\">16.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.7\">58.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.8\">32.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.3.9\">24.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.10\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.3.11\">43.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.3.12\">30.58</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.1\">Qwen1.5-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.2\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.4.3\">\u2013</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.4\">20.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.5\">30.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.6\">78.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.7\">42.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.4.8\">54.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.9\">45.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.4.10\">57.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.4.11\">47.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.1\">LLaMA2-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.2\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.5.3\">\u2013</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.4\">8.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.5\">26.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.6\">48.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.7\">33.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.5.8\">44.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.9\">32.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.5.10\">42.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.5.11\">33.71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.1\">LLaMA3-8B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.2\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.6.3\">\u2013</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.4\">27.92</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.5\">29.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.6\">73.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.7\">41.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.6.8\">83.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.9\">60.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.6.10\">59.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.6.11\">53.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.1\" rowspan=\"16\"><span class=\"ltx_text\" id=\"S3.T1.7.1.7.1.1\"><span class=\"ltx_text\" id=\"S3.T1.7.1.7.1.1.1\"></span> <span class=\"ltx_text\" id=\"S3.T1.7.1.7.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.7.1.7.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.7.1.7.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.1.7.1.1.2.1.1.1\">Alpaca-</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.7.1.7.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.1.7.1.1.2.1.2.1\">GPT4</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S3.T1.7.1.7.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.2\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.7.2.1\">Qwen1.5-1.8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.7.3.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.7.4\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.5\">8.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.6\">28.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.7\">60.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.8\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.7.9\">22.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.10\">34.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.7.11\">41.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.7.12\">32.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.8.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.8.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.2.1\" style=\"background-color:#E6E6E6;\">10.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.8.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.3.1\" style=\"background-color:#E6E6E6;\">30.03</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.8.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.4.1\" style=\"background-color:#E6E6E6;\">64.29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.8.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.5.1\" style=\"background-color:#E6E6E6;\">33.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.8.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.6.1\" style=\"background-color:#E6E6E6;\">29.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.8.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.7.1\" style=\"background-color:#E6E6E6;\">34.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.8.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.8.8.1\" style=\"background-color:#E6E6E6;\">43.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.8.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.8.9.1\" style=\"background-color:#E6E6E6;\">35.21</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.9.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.9.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.3\">8.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.4\">23.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.5\">60.69</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.6\">32.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.9.7\">25.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.8\">34.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.9.9\">42.58</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.9.10\">32.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.10.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.10.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.2.1\" style=\"background-color:#E6E6E6;\">10.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.10.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.3.1\" style=\"background-color:#E6E6E6;\">27.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.10.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.4.1\" style=\"background-color:#E6E6E6;\">63.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.10.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.5.1\" style=\"background-color:#E6E6E6;\">32.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.10.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.6.1\" style=\"background-color:#E6E6E6;\">31.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.10.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.7.1\" style=\"background-color:#E6E6E6;\">34.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.10.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.10.8.1\" style=\"background-color:#E6E6E6;\">43.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.10.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.10.9.1\" style=\"background-color:#E6E6E6;\">34.74</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.11.1.1\">Qwen1.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.11.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.11.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.4\">17.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.5\">36.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.6\">77.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.7\">37.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.11.8\">57.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.9\">44.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.11.10\">54.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.11.11\">46.50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.12.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.12.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.2.1\" style=\"background-color:#E6E6E6;\">24.98</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.12.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.3.1\" style=\"background-color:#E6E6E6;\">43.46</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.12.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.4.1\" style=\"background-color:#E6E6E6;\">77.31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.12.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.5.1\" style=\"background-color:#E6E6E6;\">41.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.12.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.6.1\" style=\"background-color:#E6E6E6;\">67.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.12.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.7.1\" style=\"background-color:#E6E6E6;\">46.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.12.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.12.8.1\" style=\"background-color:#E6E6E6;\">57.29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.12.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.12.9.1\" style=\"background-color:#E6E6E6;\">51.27</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.13.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.13.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.3\">16.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.4\">35.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.5\">76.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.6\">38.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.13.7\">56.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.8\">45.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.13.9\">55.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.13.10\">46.31</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.14.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.14.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.2.1\" style=\"background-color:#E6E6E6;\">25.04</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.14.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.3.1\" style=\"background-color:#E6E6E6;\">42.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.14.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.4.1\" style=\"background-color:#E6E6E6;\">77.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.14.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.5.1\" style=\"background-color:#E6E6E6;\">41.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.14.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.6.1\" style=\"background-color:#E6E6E6;\">65.60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.14.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.7.1\" style=\"background-color:#E6E6E6;\">46.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.14.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.14.8.1\" style=\"background-color:#E6E6E6;\">57.15</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.14.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.14.9.1\" style=\"background-color:#E6E6E6;\">50.92</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.15.1.1\">LLama2-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.15.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.15.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.4\">7.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.5\">33.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.6\">63.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.7\">35.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.15.8\">43.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.9\">38.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.15.10\">43.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.15.11\">37.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.16.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.16.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.2.1\" style=\"background-color:#E6E6E6;\">8.02</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.16.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.3.1\" style=\"background-color:#E6E6E6;\">31.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.16.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.4.1\" style=\"background-color:#E6E6E6;\">63.06</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.16.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.5.1\" style=\"background-color:#E6E6E6;\">34.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.16.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.6.1\" style=\"background-color:#E6E6E6;\">57.08</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.16.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.7.1\" style=\"background-color:#E6E6E6;\">38.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.16.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.16.8.1\" style=\"background-color:#E6E6E6;\">41.91</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.16.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.16.9.1\" style=\"background-color:#E6E6E6;\">39.13</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.17\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.17.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.17.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.3\">7.42</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.4\">32.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.5\">64.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.6\">34.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.17.7\">45.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.8\">37.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.17.9\">41.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.17.10\">37.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.18.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.18.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.2.1\" style=\"background-color:#E6E6E6;\">8.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.18.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.3.1\" style=\"background-color:#E6E6E6;\">30.93</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.18.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.4.1\" style=\"background-color:#E6E6E6;\">63.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.18.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.5.1\" style=\"background-color:#E6E6E6;\">34.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.18.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.6.1\" style=\"background-color:#E6E6E6;\">51.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.18.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.7.1\" style=\"background-color:#E6E6E6;\">36.53</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.18.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.18.8.1\" style=\"background-color:#E6E6E6;\">41.21</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.18.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.18.9.1\" style=\"background-color:#E6E6E6;\">38.17</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.19.1.1\">LLama3-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.19.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.19.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.4\">25.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.5\">37.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.6\">75.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.7\">37.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.19.8\">69.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.9\">57.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.19.10\">60.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.19.11\">51.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.20.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.20.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.2.1\" style=\"background-color:#E6E6E6;\">28.34</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.20.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.3.1\" style=\"background-color:#E6E6E6;\">31.30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.20.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.4.1\" style=\"background-color:#E6E6E6;\">75.92</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.20.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.5.1\" style=\"background-color:#E6E6E6;\">40.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.20.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.6.1\" style=\"background-color:#E6E6E6;\">85.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.20.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.7.1\" style=\"background-color:#E6E6E6;\">58.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.20.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.20.8.1\" style=\"background-color:#E6E6E6;\">61.87</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.20.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.20.9.1\" style=\"background-color:#E6E6E6;\">54.54</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.21\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.21.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.21.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.3\">25.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.4\">35.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.5\">77.15</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.6\">39.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.21.7\">71.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.8\">57.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.21.9\">61.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.21.10\">52.71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.22\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.22.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.22.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.2.1\" style=\"background-color:#E6E6E6;\">28.16</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.22.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.3.1\" style=\"background-color:#E6E6E6;\">29.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.22.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.4.1\" style=\"background-color:#E6E6E6;\">77.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.22.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.5.1\" style=\"background-color:#E6E6E6;\">40.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.22.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.6.1\" style=\"background-color:#E6E6E6;\">84.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.22.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.7.1\" style=\"background-color:#E6E6E6;\">59.07</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.22.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.22.8.1\" style=\"background-color:#E6E6E6;\">61.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.22.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.22.9.1\" style=\"background-color:#E6E6E6;\">54.32</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.23\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_tt\" id=\"S3.T1.7.1.23.1\" rowspan=\"16\"><span class=\"ltx_text\" id=\"S3.T1.7.1.23.1.1\"><span class=\"ltx_text\" id=\"S3.T1.7.1.23.1.1.1\"></span> <span class=\"ltx_text\" id=\"S3.T1.7.1.23.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.7.1.23.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.7.1.23.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.1.23.1.1.2.1.1.1\">CoT-</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.7.1.23.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.1.23.1.1.2.1.2.1\">Collection</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S3.T1.7.1.23.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.2\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.23.2.1\">Qwen1.5-1.8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.23.3.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.23.4\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.5\">7.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.6\">13.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.7\">58.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.8\">21.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.23.9\">39.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.10\">27.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.7.1.23.11\">25.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.7.1.23.12\">27.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.24\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.24.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.24.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.2.1\" style=\"background-color:#E6E6E6;\">9.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.24.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.3.1\" style=\"background-color:#E6E6E6;\">21.93</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.24.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.4.1\" style=\"background-color:#E6E6E6;\">67.32</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.24.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.5.1\" style=\"background-color:#E6E6E6;\">32.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.24.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.6.1\" style=\"background-color:#E6E6E6;\">40.30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.24.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.7.1\" style=\"background-color:#E6E6E6;\">34.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.24.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.24.8.1\" style=\"background-color:#E6E6E6;\">42.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.24.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.24.9.1\" style=\"background-color:#E6E6E6;\">35.54</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.25\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.25.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.25.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.3\">7.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.4\">12.99</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.5\">58.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.6\">22.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.25.7\">38.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.8\">27.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.25.9\">23.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.25.10\">27.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.26\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.26.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.26.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.2.1\" style=\"background-color:#E6E6E6;\">9.08</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.26.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.3.1\" style=\"background-color:#E6E6E6;\">22.49</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.26.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.4.1\" style=\"background-color:#E6E6E6;\">67.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.26.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.5.1\" style=\"background-color:#E6E6E6;\">34.56</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.26.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.6.1\" style=\"background-color:#E6E6E6;\">44.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.26.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.7.1\" style=\"background-color:#E6E6E6;\">36.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.26.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.26.8.1\" style=\"background-color:#E6E6E6;\">44.13</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.26.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.26.9.1\" style=\"background-color:#E6E6E6;\">37.07</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.27\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.27.1.1\">Qwen1.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.27.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.27.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.4\">13.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.5\">24.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.6\">72.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.7\">21.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.27.8\">53.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.9\">27.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.27.10\">25.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.27.11\">33.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.28\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.28.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.28.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.2.1\" style=\"background-color:#E6E6E6;\">20.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.28.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.3.1\" style=\"background-color:#E6E6E6;\">32.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.28.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.4.1\" style=\"background-color:#E6E6E6;\">80.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.28.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.5.1\" style=\"background-color:#E6E6E6;\">41.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.28.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.6.1\" style=\"background-color:#E6E6E6;\">61.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.28.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.7.1\" style=\"background-color:#E6E6E6;\">46.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.28.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.28.8.1\" style=\"background-color:#E6E6E6;\">56.52</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.28.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.28.9.1\" style=\"background-color:#E6E6E6;\">48.57</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.29\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.29.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.29.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.3\">13.38</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.4\">22.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.5\">75.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.6\">22.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.29.7\">52.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.8\">27.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.29.9\">25.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.29.10\">34.26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.30\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.30.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.30.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.2.1\" style=\"background-color:#E6E6E6;\">19.74</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.30.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.3.1\" style=\"background-color:#E6E6E6;\">30.96</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.30.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.4.1\" style=\"background-color:#E6E6E6;\">78.84</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.30.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.5.1\" style=\"background-color:#E6E6E6;\">41.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.30.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.6.1\" style=\"background-color:#E6E6E6;\">58.81</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.30.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.7.1\" style=\"background-color:#E6E6E6;\">46.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.30.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.30.8.1\" style=\"background-color:#E6E6E6;\">57.01</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.30.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.30.9.1\" style=\"background-color:#E6E6E6;\">47.62</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.31\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.31.1.1\">LLama2-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.31.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.31.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.4\">7.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.5\">19.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.6\">56.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.7\">30.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.31.8\">52.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.9\">38.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.31.10\">46.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.31.11\">36.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.32\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.32.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.32.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.2.1\" style=\"background-color:#E6E6E6;\">8.44</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.32.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.3.1\" style=\"background-color:#E6E6E6;\">26.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.32.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.4.1\" style=\"background-color:#E6E6E6;\">60.77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.32.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.5.1\" style=\"background-color:#E6E6E6;\">31.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.32.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.6.1\" style=\"background-color:#E6E6E6;\">58.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.32.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.7.1\" style=\"background-color:#E6E6E6;\">38.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.32.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.32.8.1\" style=\"background-color:#E6E6E6;\">42.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.32.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.32.9.1\" style=\"background-color:#E6E6E6;\">38.03</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.33\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.33.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.33.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.3\">4.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.4\">20.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.5\">61.55</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.6\">36.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.33.7\">56.00</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.8\">37.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.33.9\">45.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.33.10\">37.36</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.34\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.34.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.34.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.2.1\" style=\"background-color:#E6E6E6;\">8.04</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.34.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.3.1\" style=\"background-color:#E6E6E6;\">30.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.34.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.4.1\" style=\"background-color:#E6E6E6;\">63.49</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.34.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.5.1\" style=\"background-color:#E6E6E6;\">33.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.34.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.6.1\" style=\"background-color:#E6E6E6;\">55.40</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.34.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.7.1\" style=\"background-color:#E6E6E6;\">37.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.34.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.34.8.1\" style=\"background-color:#E6E6E6;\">45.34</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.34.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.34.9.1\" style=\"background-color:#E6E6E6;\">39.05</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.35\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_t\" id=\"S3.T1.7.1.35.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"S3.T1.7.1.35.1.1\">LLama3-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.35.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.35.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.4\">16.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.5\">23.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.6\">67.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.7\">27.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.35.8\">78.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.9\">55.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.1.35.10\">60.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.7.1.35.11\">47.00</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.36\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.36.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.36.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.2.1\" style=\"background-color:#E6E6E6;\">26.28</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.36.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.3.1\" style=\"background-color:#E6E6E6;\">18.86</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.36.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.4.1\" style=\"background-color:#E6E6E6;\">71.25</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.36.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.5.1\" style=\"background-color:#E6E6E6;\">41.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.36.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.6.1\" style=\"background-color:#E6E6E6;\">82.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.36.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.7.1\" style=\"background-color:#E6E6E6;\">58.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.36.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.36.8.1\" style=\"background-color:#E6E6E6;\">63.16</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.36.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S3.T1.7.1.36.9.1\" style=\"background-color:#E6E6E6;\">51.74</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.37\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.37.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.7.1.37.1.1\">MoLoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.37.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.37.3\">17.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.37.4\">22.24</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.37.5\">71.74</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.37.6\">28.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.37.7\">79.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.37.8\">58.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.7.1.37.9\">60.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.7.1.37.10\">48.25</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.7.1.38\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_r\" id=\"S3.T1.7.1.38.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.38.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.2.1\" style=\"background-color:#E6E6E6;\">25.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.38.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.3.1\" style=\"background-color:#E6E6E6;\">28.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.38.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.4.1\" style=\"background-color:#E6E6E6;\">73.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.38.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.5.1\" style=\"background-color:#E6E6E6;\">40.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_r\" id=\"S3.T1.7.1.38.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.6.1\" style=\"background-color:#E6E6E6;\">83.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.38.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.7.1\" style=\"background-color:#E6E6E6;\">60.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_r\" id=\"S3.T1.7.1.38.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S3.T1.7.1.38.8.1\" style=\"background-color:#E6E6E6;\">61.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb\" id=\"S3.T1.7.1.38.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.7.1.38.9.1\" style=\"background-color:#E6E6E6;\">53.24</span></td>\n</tr>\n</table>"
        },
        {
            "id": "S4.T2",
            "type": "table",
            "title": "2405.20192v2_Table2",
            "caption": "Table 2:Comparison with other OOD generalization methods.TAIAis more robust and general than other competitive methods and requires no additional implementation efforts.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.5.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.1.1.1\">Datasets</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T2.5.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.1.2.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" id=\"S4.T2.5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.1.3.1\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S4.T2.5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.1.4.1\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.5.1.1.5\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.1.5.1\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.1.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.2.1\">BBH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.3.1\">CQA.</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.4.1\">LogiQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.5.1\">SVAMP</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.6.1\">MMB.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.2.7.1\">MMLU</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.1\">Base Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.3.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.3\">4.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.4\">16.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.5\">58.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.6\">32.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.3.7\">24.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.8\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.3.9\">43.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.3.10\">30.58</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T2.5.1.4.1.1\"><span class=\"ltx_text\" id=\"S4.T2.5.1.4.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T2.5.1.4.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.5.1.4.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.5.1.4.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.5.1.4.1.1.2.1.1.1\">Alpaca-</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.5.1.4.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.5.1.4.1.1.2.1.2.1\">GPT4</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.5.1.4.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.4.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.3\">8.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.4\">28.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.5\">60.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.6\">35.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.4.7\">22.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.8\">34.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.4.9\">41.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.4.10\">32.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.5.1\">L2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.5.2\">3.68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.5.3\">24.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.5.4\">57.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.5.5.1\">35.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.5.6\">21.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.5.7\">35.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.5.8\">41.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.5.9\">31.26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.6.1\">EWC</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.6.2\">3.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.6.3\">25.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.6.4\">60.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.6.5\">34.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.6.6\">22.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.6.7\">34.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.6.8\">41.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.6.9\">30.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.7.1\">Self-Distill</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.7.2\">7.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.7.3\">26.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.7.4\">53.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.7.5\">28.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.7.6\">18.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.7.7\">35.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.7.8\">39.87</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.7.9\">28.09</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.8.1\">LoRACL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.8.2\">8.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.8.3\">28.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.8.4\">60.03</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.8.5\">34.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.8.6\">27.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.8.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.8.7.1\">35.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.8.8\">42.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.8.9\">33.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.9\">\n<td class=\"ltx_td\" id=\"S4.T2.5.1.9.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.9.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T2.5.1.9.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.9.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.9.3.1\" style=\"background-color:#E6E6E6;\">10.82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.9.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.9.4.1\" style=\"background-color:#E6E6E6;\">30.03</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.9.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.9.5.1\" style=\"background-color:#E6E6E6;\">64.29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.9.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T2.5.1.9.6.1\" style=\"background-color:#E6E6E6;\">33.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.9.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.9.7.1\" style=\"background-color:#E6E6E6;\">29.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.9.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T2.5.1.9.8.1\" style=\"background-color:#E6E6E6;\">34.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.9.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.9.9.1\" style=\"background-color:#E6E6E6;\">43.73</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.9.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.9.10.1\" style=\"background-color:#E6E6E6;\">35.21</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.5.1.10.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T2.5.1.10.1.1\"><span class=\"ltx_text\" id=\"S4.T2.5.1.10.1.1.1\"></span> <span class=\"ltx_text\" id=\"S4.T2.5.1.10.1.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.5.1.10.1.1.2.1\">\n<span class=\"ltx_tr\" id=\"S4.T2.5.1.10.1.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.5.1.10.1.1.2.1.1.1\">CoT-</span></span>\n<span class=\"ltx_tr\" id=\"S4.T2.5.1.10.1.1.2.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.5.1.10.1.1.2.1.2.1\">Collection</span></span>\n</span></span> <span class=\"ltx_text\" id=\"S4.T2.5.1.10.1.1.3\"></span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.10.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.10.3\">7.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.10.4\">13.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.10.5\">58.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.10.6\">21.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.10.7\">39.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.10.8\">27.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.5.1.10.9\">25.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.5.1.10.10\">27.68</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.11.1\">L2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.11.2\">0.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.11.3\">5.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.11.4\">23.26</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.11.5\">22.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.11.6\">41.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.11.7\">27.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.11.8\">23.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.11.9\">20.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.12.1\">EWC</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.12.2\">0.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.12.3\">7.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.12.4\">22.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.12.5\">22.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.12.6\">40.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.12.7\">27.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.12.8\">23.67</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.12.9\">20.65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.13.1\">LoRACL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.13.2\">7.68</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.13.3\">14.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.13.4\">58.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.13.5\">21.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.13.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.13.6.1\">41.60</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.13.7\">27.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.5.1.13.8\">23.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.5.1.13.9\">27.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.5.1.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.1.14.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T2.5.1.14.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.14.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.2.1\" style=\"background-color:#E6E6E6;\">9.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.14.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.3.1\" style=\"background-color:#E6E6E6;\">21.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.14.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.4.1\" style=\"background-color:#E6E6E6;\">67.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.14.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.5.1\" style=\"background-color:#E6E6E6;\">32.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.1.14.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T2.5.1.14.6.1\" style=\"background-color:#E6E6E6;\">40.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.14.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.7.1\" style=\"background-color:#E6E6E6;\">34.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T2.5.1.14.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.8.1\" style=\"background-color:#E6E6E6;\">42.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.5.1.14.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.5.1.14.9.1\" style=\"background-color:#E6E6E6;\">35.54</span></td>\n</tr>\n</table>"
        },
        {
            "id": "S4.T3",
            "type": "table",
            "title": "2405.20192v2_Table3",
            "caption": "Table 3:Ablation experiments on different inference modes under two training corpus. We validate the performance of inference modes by considering both general tasks and domain tasks.Boldindicates the optimal result in each subgroup andunderlineindicates the suboptimal result. Note that even fine-tuned on out-of-domain data,TAIAstill achieves optimal results on specific domain tasks and even surpasses the performance of the base model.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.7.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.7.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.1.1\">Training Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.7.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.2.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S4.T3.7.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.3.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S4.T3.7.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.4.1\">Genereal Task</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\" id=\"S4.T3.7.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.5.1\">Domain Task</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.7.1.1.6\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.1.6.1\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.2.1.1\">CMMLU</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.2.2.1\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.2.3.1\">CEval</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.2.4.1\">MMed ZH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.2.5.1\">MMed EN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.2.6.1\">MATH</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.1\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.2\">Qwen1.5-1.8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.3.3\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.4\">52.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.5\">43.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.3.6\">55.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.7\">62.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.8\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.3.9\">4.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.3.10\">41.99</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T3.7.1.4.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S4.T3.7.1.4.1.1.1\">\n<span class=\"ltx_tr\" id=\"S4.T3.7.1.4.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.4.1.1.1.1.1\">Medical</span></span>\n<span class=\"ltx_tr\" id=\"S4.T3.7.1.4.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.7.1.4.1.1.1.2.1\">Collection</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T3.7.1.4.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.4.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.4\">39.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.5\">27.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.4.6\">37.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.7\">57.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.8\">29.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.4.9\">8.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.4.10\">33.44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.5.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.7.1.5.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.5.2.1\" style=\"background-color:#E6E6E6;\">54.58</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.5.3.1\" style=\"background-color:#E6E6E6;\">44.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.5.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.5.4.1\" style=\"background-color:#E6E6E6;\">55.57</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.5.5.1\" style=\"background-color:#E6E6E6;\">64.97</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.5.6.1\" style=\"background-color:#E6E6E6;\">36.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.5.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.5.7.1\" style=\"background-color:#E6E6E6;\">10.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.5.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.5.8.1\" style=\"background-color:#E6E6E6;\">44.37</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.6.1\">TAIF</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.2\">47.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.3\">42.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.6.4\">45.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.5\">58.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.6\">32.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.6.7\">9.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.6.8\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S4.T3.7.1.6.8.1\">39.10</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.7.1\">TOA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.2\">43.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.3\">29.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.7.4\">39.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.5\">59.54</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.6\">30.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.7.7\">7.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.7.8\">35.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.8.1\">TOF</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.8.2\">41.18</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.8.3\">26.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.8.4\">39.82</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.8.5\">58.17</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.8.6\">29.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.8.7\">8.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.8.8\">33.86</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.7.1.9.1\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.1.1\">OpenMath</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T3.7.1.9.2\" rowspan=\"5\"><span class=\"ltx_text\" id=\"S4.T3.7.1.9.2.1\">LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.9.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.9.4\">54.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.9.5\">39.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.9.6\">50.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.9.7\">58.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.9.8\">33.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.7.1.9.9\">7.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.7.1.9.10\">40.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.10.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S4.T3.7.1.10.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.10.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.2.1\" style=\"background-color:#E6E6E6;\">54.35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.10.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.3.1\" style=\"background-color:#E6E6E6;\">43.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.10.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.4.1\" style=\"background-color:#E6E6E6;\">56.32</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.10.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.5.1\" style=\"background-color:#E6E6E6;\">63.81</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.10.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.6.1\" style=\"background-color:#E6E6E6;\">35.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.10.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S4.T3.7.1.10.7.1\" style=\"background-color:#E6E6E6;\">11.68</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.10.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.7.1.10.8.1\" style=\"background-color:#E6E6E6;\">44.33</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.11.1\">TAIF</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.11.2\">53.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.11.3\">43.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.11.4\">54.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.11.5\">62.84</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.11.6\">36.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.11.7\">7.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.11.8\"><span class=\"ltx_text ltx_ulem_uline\" id=\"S4.T3.7.1.11.8.1\">43.09</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.12.1\">TOA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.12.2\">53.37</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.12.3\">33.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.12.4\">50.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.12.5\">57.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.12.6\">30.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.7.1.12.7\">7.50</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.7.1.12.8\">38.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.7.1.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.7.1.13.1\">TOF</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.13.2\">52.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.13.3\">37.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.7.1.13.4\">48.37</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.13.5\">55.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.13.6\">31.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S4.T3.7.1.13.7\">7.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.7.1.13.8\">38.88</td>\n</tr>\n</table>"
        },
        {
            "id": "S5.T4",
            "type": "table",
            "title": "2405.20192v2_Table4",
            "caption": "Table 4:The application ofTAIAon full fine-tuning technique trained on CoT-Collection. It still surpasses the vanilla fine-tuning method but lags behind the base LLM.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T4.5.1\">\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.2.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.3.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.4.1\">BBH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.5.1\">CQA.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.6.1\">LogiQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.7.1\">SVAMP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.8.1\">MMB.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.9.1\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.5.1.1.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.5.1.1.10.1\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T4.5.1.2.1.1\">Qwen1.5-1.8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.2\">Base Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.3\">4.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.4\">16.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.5\">58.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.6\">32.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.7\">27.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.8\">33.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.9\">43.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.2.10\">31.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.1\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.2\">6.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.3\">14.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.4\">59.21</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.5\">20.28</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.6\">34.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.7\">27.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.8\">23.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.3.9\">26.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T4.5.1.4.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.2.1\" style=\"background-color:#E6E6E6;\">8.22</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.3.1\" style=\"background-color:#E6E6E6;\">15.56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.4.1\" style=\"background-color:#E6E6E6;\">60.61</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.5.1\" style=\"background-color:#E6E6E6;\">25.65</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.6.1\" style=\"background-color:#E6E6E6;\">39.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.7.1\" style=\"background-color:#E6E6E6;\">28.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.8.1\" style=\"background-color:#E6E6E6;\">25.65</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.4.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.4.9.1\" style=\"background-color:#E6E6E6;\">28.98</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.5.1.5.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T4.5.1.5.1.1\">Qwen1.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.2\">Base Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.3\">20.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.4\">30.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.5\">78.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.6\">42.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.7\">54.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.8\">45.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.9\">57.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.5.1.5.10\">47.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.1\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.2\">9.34</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.3\">23.85</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.4\">71.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.5\">21.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.6\">57.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.7\">27.57</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.8\">24.44</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.5.1.6.9\">33.69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.5.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T4.5.1.7.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.2.1\" style=\"background-color:#E6E6E6;\">14.60</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.3.1\" style=\"background-color:#E6E6E6;\">27.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.4.1\" style=\"background-color:#E6E6E6;\">72.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.5.1\" style=\"background-color:#E6E6E6;\">33.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.6.1\" style=\"background-color:#E6E6E6;\">64.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.7.1\" style=\"background-color:#E6E6E6;\">40.39</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.8.1\" style=\"background-color:#E6E6E6;\">35.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.5.1.7.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"S5.T4.5.1.7.9.1\" style=\"background-color:#E6E6E6;\">41.31</span></td>\n</tr>\n</table>"
        },
        {
            "id": "S5.T5",
            "type": "table",
            "title": "2405.20192v2_Table5",
            "caption": "Table 5:Comparison ofTAIAwith vanilla fine-tuning on red-teaming resistance. When jailbreaking LLMs on harmful datasets,TAIAharvests lower attack success rates than vanilla fine-tuning on both harmful and benign datasets, showing its strong generalization in distilling out harmful features.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T5.4.4\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.2.1\">Base Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.3.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S5.T5.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.4.1\">Advbench</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\" id=\"S5.T5.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.1.1.1.1.1\">AlpacaEval<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.1.1.1.1.1.1.m1.1\"><semantics id=\"S5.T5.1.1.1.1.1.1.m1.1a\"><mo id=\"S5.T5.1.1.1.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T5.1.1.1.1.1.1.m1.1.1.cmml\">\u2191</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.1.1.1.1.1.1.m1.1b\"><ci id=\"S5.T5.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T5.1.1.1.1.1.1.m1.1.1\">\u2191</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.1.1.1.1.1.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T5.1.1.1.1.1.1.m1.1d\">\u2191</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.2.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.2.2.2.1.1\">Explicitly harmful<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.2.2.2.1.1.m1.1\"><semantics id=\"S5.T5.2.2.2.1.1.m1.1a\"><mo id=\"S5.T5.2.2.2.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T5.2.2.2.1.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.2.2.2.1.1.m1.1b\"><ci id=\"S5.T5.2.2.2.1.1.m1.1.1.cmml\" xref=\"S5.T5.2.2.2.1.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.2.2.2.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T5.2.2.2.1.1.m1.1d\">\u2193</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.3.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.3.3.3.2.1\">Identity Shifting<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.3.3.3.2.1.m1.1\"><semantics id=\"S5.T5.3.3.3.2.1.m1.1a\"><mo id=\"S5.T5.3.3.3.2.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T5.3.3.3.2.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.3.3.2.1.m1.1b\"><ci id=\"S5.T5.3.3.3.2.1.m1.1.1.cmml\" xref=\"S5.T5.3.3.3.2.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.3.3.2.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T5.3.3.3.2.1.m1.1d\">\u2193</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.4.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.4.3.1\">Benign<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.4.4.4.3.1.m1.1\"><semantics id=\"S5.T5.4.4.4.3.1.m1.1a\"><mo id=\"S5.T5.4.4.4.3.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T5.4.4.4.3.1.m1.1.1.cmml\">\u2193</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.4.4.4.3.1.m1.1b\"><ci id=\"S5.T5.4.4.4.3.1.m1.1.1.cmml\" xref=\"S5.T5.4.4.4.3.1.m1.1.1\">\u2193</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.4.4.4.3.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T5.4.4.4.3.1.m1.1d\">\u2193</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.4.4.5.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S5.T5.4.4.5.1.1\">LLaMA2-7B-chat</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.2\">\u2013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.3\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.4\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.5\">0.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.4.5.6\">7.66</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.4.6.1\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.4.6.2\">84.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.4.6.3\">93.27</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.4.6.4\">4.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.4.6.5\">7.46</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.4.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.4.4.7.1\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T5.4.4.7.1.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.4.4.7.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.7.2.1\" style=\"background-color:#E6E6E6;\">8.27</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.4.4.7.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.7.3.1\" style=\"background-color:#E6E6E6;\">30.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.4.4.7.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.7.4.1\" style=\"background-color:#E6E6E6;\">0.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.4.4.7.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.4.4.7.5.1\" style=\"background-color:#E6E6E6;\">9.94</span></td>\n</tr>\n</table>"
        },
        {
            "id": "A3.T6",
            "type": "table",
            "title": "2405.20192v2_Table6",
            "caption": "Table 6:Experiments on two tasks whose knowledge is not fully acquired by base LLMs.TAIAlags behind vanilla fine-tuning methods by a small margin for Qwen1.5-1.8B. However, for the base model with sufficient knowledge like Qwen1.5-7B,TAIAsurpasses the vanilla fine-tuning methods.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A3.T6.10\">\n<tr class=\"ltx_tr\" id=\"A3.T6.10.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T6.10.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.1.1.1\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T6.10.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.1.2.1\" style=\"font-size:90%;\">Training Data</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A3.T6.10.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.1.3.1\" style=\"font-size:90%;\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T6.10.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.1.4.1\" style=\"font-size:90%;\">SQuAD v2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T6.10.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.1.5.1\" style=\"font-size:90%;\">XSum (R-1/R-2/R-L)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.2\">\n<td class=\"ltx_td ltx_border_t\" id=\"A3.T6.10.2.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.2.2\"><span class=\"ltx_text\" id=\"A3.T6.10.2.2.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T6.10.2.3\"><span class=\"ltx_text\" id=\"A3.T6.10.2.3.1\" style=\"font-size:90%;\">Base Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.2.4\"><span class=\"ltx_text\" id=\"A3.T6.10.2.4.1\" style=\"font-size:90%;\">84.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.2.5\"><span class=\"ltx_text\" id=\"A3.T6.10.2.5.1\" style=\"font-size:90%;\">14.76/3.35/10.05</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.3\">\n<td class=\"ltx_td\" id=\"A3.T6.10.3.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A3.T6.10.3.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T6.10.3.3\"><span class=\"ltx_text\" id=\"A3.T6.10.3.3.1\" style=\"font-size:90%;\">Vanilla</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.3.4.1\" style=\"font-size:90%;\">91.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.3.5\"><span class=\"ltx_text\" id=\"A3.T6.10.3.5.1\" style=\"font-size:90%;\">14.06/2.05/11.36</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.4\">\n<td class=\"ltx_td\" id=\"A3.T6.10.4.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.4.2\"><span class=\"ltx_text\" id=\"A3.T6.10.4.2.1\" style=\"font-size:90%;\">SQuAD v2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T6.10.4.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.4.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.4.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.4.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">91.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.4.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.4.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">18.88/3.73/12.94</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.5\">\n<td class=\"ltx_td\" id=\"A3.T6.10.5.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A3.T6.10.5.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T6.10.5.3\"><span class=\"ltx_text\" id=\"A3.T6.10.5.3.1\" style=\"font-size:90%;\">Vanilla</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.5.4\"><span class=\"ltx_text\" id=\"A3.T6.10.5.4.1\" style=\"font-size:90%;\">72.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.5.5.1\" style=\"font-size:90%;\">34.26/13.04/27.48</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.6.1\"><span class=\"ltx_text\" id=\"A3.T6.10.6.1.1\" style=\"font-size:90%;\">Qwen1.5-1.8B</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.6.2\"><span class=\"ltx_text\" id=\"A3.T6.10.6.2.1\" style=\"font-size:90%;\">XSum</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T6.10.6.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.6.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.6.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.6.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">78.48</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.6.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.6.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">31.32/10.51/24.27</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.7\">\n<td class=\"ltx_td ltx_border_t\" id=\"A3.T6.10.7.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.7.2\"><span class=\"ltx_text\" id=\"A3.T6.10.7.2.1\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T6.10.7.3\"><span class=\"ltx_text\" id=\"A3.T6.10.7.3.1\" style=\"font-size:90%;\">Base Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.7.4\"><span class=\"ltx_text\" id=\"A3.T6.10.7.4.1\" style=\"font-size:90%;\">92.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.7.5\"><span class=\"ltx_text\" id=\"A3.T6.10.7.5.1\" style=\"font-size:90%;\">21.90/6.17/15.17</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.8\">\n<td class=\"ltx_td\" id=\"A3.T6.10.8.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A3.T6.10.8.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T6.10.8.3\"><span class=\"ltx_text\" id=\"A3.T6.10.8.3.1\" style=\"font-size:90%;\">Vanilla</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.8.4\"><span class=\"ltx_text\" id=\"A3.T6.10.8.4.1\" style=\"font-size:90%;\">93.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.8.5\"><span class=\"ltx_text\" id=\"A3.T6.10.8.5.1\" style=\"font-size:90%;\">16.75/2.65/13.15</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.9\">\n<td class=\"ltx_td\" id=\"A3.T6.10.9.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.9.2\"><span class=\"ltx_text\" id=\"A3.T6.10.9.2.1\" style=\"font-size:90%;\">SQuAD v2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A3.T6.10.9.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.9.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.9.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.9.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">95.27</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T6.10.9.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.9.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">25.58/7.66/19.79</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.10\">\n<td class=\"ltx_td\" id=\"A3.T6.10.10.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A3.T6.10.10.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A3.T6.10.10.3\"><span class=\"ltx_text\" id=\"A3.T6.10.10.3.1\" style=\"font-size:90%;\">Vanilla</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.10.4\"><span class=\"ltx_text\" id=\"A3.T6.10.10.4.1\" style=\"font-size:90%;\">77.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T6.10.10.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.10.5.1\" style=\"font-size:90%;\">42.23/19.99/34.78</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T6.10.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T6.10.11.1\"><span class=\"ltx_text\" id=\"A3.T6.10.11.1.1\" style=\"font-size:90%;\">Qwen1.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T6.10.11.2\"><span class=\"ltx_text\" id=\"A3.T6.10.11.2.1\" style=\"font-size:90%;\">XSum</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A3.T6.10.11.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.11.3.1\" style=\"font-size:90%;background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T6.10.11.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T6.10.11.4.1\" style=\"font-size:90%;background-color:#E6E6E6;\">81.79</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T6.10.11.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A3.T6.10.11.5.1\" style=\"font-size:90%;background-color:#E6E6E6;\">38.51/16.49/31.02</span></td>\n</tr>\n</table>"
        },
        {
            "id": "A5.T7",
            "type": "table",
            "title": "2405.20192v2_Table7",
            "caption": "Table 7:Comparison ofTAIAwith vanilla fine-tuning on hallucination resistance. When fine-tuned on datasets with different quality levels,TAIAharvests lower performance drops than vanilla fine-tuning, showing its strong generalization in distilling out hallucinated features.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A5.T7.6\">\n<tr class=\"ltx_tr\" id=\"A5.T7.6.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.1.1.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T7.6.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.1.2.1\">Qwen1.5 1.8B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T7.6.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.1.3.1\">Qwen1.5 7B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T7.6.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.1.4.1\">LLaMA2 7B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T7.6.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.1.5.1\">LLaMA3 8B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T7.6.2.1\">Base Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T7.6.2.2\">4.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T7.6.2.3\">24.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T7.6.2.4\">7.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A5.T7.6.2.5\">26.31</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" id=\"A5.T7.6.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A5.T7.6.3.1.1\">ShareGPT-52K</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.4.1\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.4.2\">1.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.4.3\">1.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.4.4\">1.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.4.5\">3.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T7.6.5.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A5.T7.6.5.1.1\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T7.6.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.5.2.1\">4.37</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T7.6.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.5.3.1\">9.68</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T7.6.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.5.4.1\">4.87</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A5.T7.6.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.5.5.1\">7.64</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\" id=\"A5.T7.6.6.1\"><span class=\"ltx_text ltx_font_italic\" id=\"A5.T7.6.6.1.1\">Alpaca-GPT4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.7.1\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.7.2.1\">5.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.7.3\">10.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.7.4\">7.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A5.T7.6.7.5\">16.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A5.T7.6.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T7.6.8.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"A5.T7.6.8.1.1\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T7.6.8.2\">4.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T7.6.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.8.3.1\">11.46</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T7.6.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.8.4.1\">9.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A5.T7.6.8.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A5.T7.6.8.5.1\">18.33</span></td>\n</tr>\n</table>"
        },
        {
            "id": "A6.T12",
            "type": "table",
            "title": "2405.20192v2_Table12",
            "caption": "Table 12:Model size scaling ofTAIA. We choose 7B, 14B, 32B of Qwen1.5 series as the pre-trained models.TAIAmaintains the best performance based on even larger pre-trained models.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A6.T12.6.1\">\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A6.T12.6.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A6.T12.6.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.1.2.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" id=\"A6.T12.6.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.1.3.1\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"A6.T12.6.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.1.4.1\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A6.T12.6.1.1.5\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.1.5.1\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.2.1\">MATH</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.2.2\">BBH</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.2.3\">CQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.2.4\">LogiQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.2.5\">SVAMP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.2.6\">MMB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.2.7\">MMLU</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"A6.T12.6.1.3.1.1\">Qwen1.5-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.3.2\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.3\">20.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.4\">30.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.5\">78.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.6\">42.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.3.7\">54.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.8\">45.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.3.9\">57.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.3.10\">47.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.4.1\">LoRA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.4.2\">17.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.4.3\">36.09</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.4.4\">77.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.4.5\">37.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.4.6\">57.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.4.7\">44.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.4.8\">54.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.4.9\">46.50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.5.1\">TAIA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.5.2\">24.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.5.3\">43.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.5.4\">77.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.5.5\">41.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.5.6\">67.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.5.7\">46.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.5.8\">57.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.5.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.5.9.1\">51.27</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"A6.T12.6.1.6.1.1\">Qwen1.5-14B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.6.2\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.3\">45.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.4\">43.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.5\">77.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.6\">47.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.6.7\">83.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.8\">51.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.6.9\">66.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.6.10\">59.35</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.7.1\">LoRA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.7.2\">38.74</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.7.3\">41.65</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.7.4\">74.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.7.5\">41.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.7.6\">82.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.7.7\">48.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.7.8\">63.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.7.9\">55.90</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.8.1\">TAIA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.8.2\">55.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.8.3\">46.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.8.4\">77.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.8.5\">46.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.8.6\">83.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.8.7\">52.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.8.8\">65.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.8.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.8.9.1\">60.92</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T12.6.1.9.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"A6.T12.6.1.9.1.1\">Qwen1.5-32B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.9.2\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.9.3\">41.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.9.4\">48.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.9.5\">80.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.9.6\">50.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.9.7\">87.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.9.8\">61.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T12.6.1.9.9\">73.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T12.6.1.9.10\">63.26</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.10.1\">LoRA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.10.2\">39.12</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.10.3\">46.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.10.4\">77.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.10.5\">49.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.10.6\">82.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.10.7\">59.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T12.6.1.10.8\">71.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T12.6.1.10.9\">60.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T12.6.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T12.6.1.11.1\">TAIA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T12.6.1.11.2\">42.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T12.6.1.11.3\">52.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T12.6.1.11.4\">82.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T12.6.1.11.5\">48.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T12.6.1.11.6\">86.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T12.6.1.11.7\">61.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T12.6.1.11.8\">72.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T12.6.1.11.9\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T12.6.1.11.9.1\">63.88</span></td>\n</tr>\n</table>"
        },
        {
            "id": "A6.T13",
            "type": "table",
            "title": "2405.20192v2_Table13",
            "caption": "Table 13:Experiments of the different ranks of Attention/FFN LoRA. The ranks of attention and FFN module are noted as \u2018ar\u2019 and \u2018fr\u2019, respectively. For example, the case \u2018ar4_fr64\u2019 indicates the attention rank is 4, and the FFN rank is 64. The results show that TAIF will have better performance than TAIA only when the attention rank is much greater than the FFN rank.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A6.T13.4.1\">\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A6.T13.4.1.1.1\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"A6.T13.4.1.1.2\"></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"A6.T13.4.1.1.3\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"A6.T13.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.1.4.1\">Knowledge</span></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"A6.T13.4.1.1.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.1.1\">Training Data</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.2.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.3.1\">Train/Infer</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.4.1\">CMMLU</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.5.1\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.6.1\">C-Eval</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.2.7.1\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.1\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.2\">Qwen1.5-1.8B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.3\">-/-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.4\">52.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.5\">43.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.6\">55.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.3.7\">50.62</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.4\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T13.4.1.4.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T13.4.1.4.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.4.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.4.4\">39.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.4.5\">27.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.4.6\">37.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.4.7\">34.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.5\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.5.1\"></td>\n<td class=\"ltx_td\" id=\"A6.T13.4.1.5.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.5.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.5.3.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.5.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.5.4.1\" style=\"background-color:#E6E6E6;\">54.58</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.5.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.5.5.1\" style=\"background-color:#E6E6E6;\">44.47</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.5.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.5.6.1\" style=\"background-color:#E6E6E6;\">55.57</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.5.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.5.7.1\" style=\"background-color:#E6E6E6;\">51.54</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.6\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.6.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.6.2\"><span class=\"ltx_text\" id=\"A6.T13.4.1.6.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T13.4.1.6.2.1.1\">\n<span class=\"ltx_tr\" id=\"A6.T13.4.1.6.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"A6.T13.4.1.6.2.1.1.1.1\">LoRA</span></span>\n<span class=\"ltx_tr\" id=\"A6.T13.4.1.6.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"A6.T13.4.1.6.2.1.1.2.1\">ar4_fr4</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.6.3\">TAIF</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.6.4\">47.06</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.6.5\">42.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.6.6\">45.62</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.6.7\">44.91</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.7\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.7.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T13.4.1.7.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.7.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.7.4\">45.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.7.5\">27.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.7.6\">45.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.7.7\">39.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.8\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.8.1\"></td>\n<td class=\"ltx_td\" id=\"A6.T13.4.1.8.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.8.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.8.3.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.8.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.8.4.1\" style=\"background-color:#E6E6E6;\">54.20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.8.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.8.5.1\" style=\"background-color:#E6E6E6;\">43.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.8.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.8.6.1\" style=\"background-color:#E6E6E6;\">56.32</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.8.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.8.7.1\" style=\"background-color:#E6E6E6;\">51.44</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.9\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.9.1\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.9.2\"><span class=\"ltx_text\" id=\"A6.T13.4.1.9.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T13.4.1.9.2.1.1\">\n<span class=\"ltx_tr\" id=\"A6.T13.4.1.9.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"A6.T13.4.1.9.2.1.1.1.1\">LoRA</span></span>\n<span class=\"ltx_tr\" id=\"A6.T13.4.1.9.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"A6.T13.4.1.9.2.1.1.2.1\">ar4_fr64</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.9.3\">TAIF</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.9.4\">45.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.9.5\">29.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.9.6\">46.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.9.7\">40.53</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.10\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.10.1\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T13.4.1.10.2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.10.3\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.10.4\">45.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.10.5\">28.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.10.6\">42.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T13.4.1.10.7\">38.88</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.11\">\n<td class=\"ltx_td\" id=\"A6.T13.4.1.11.1\"></td>\n<td class=\"ltx_td\" id=\"A6.T13.4.1.11.2\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.11.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.11.3.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.11.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.11.4.1\" style=\"background-color:#E6E6E6;\">51.24</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.11.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.11.5.1\" style=\"background-color:#E6E6E6;\">40.26</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.11.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.11.6.1\" style=\"background-color:#E6E6E6;\">48.74</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T13.4.1.11.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T13.4.1.11.7.1\" style=\"background-color:#E6E6E6;\">46.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T13.4.1.12\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.1\"><span class=\"ltx_text\" id=\"A6.T13.4.1.12.1.1\">Medical Collection</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.2\"><span class=\"ltx_text\" id=\"A6.T13.4.1.12.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"A6.T13.4.1.12.2.1.1\">\n<span class=\"ltx_tr\" id=\"A6.T13.4.1.12.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"A6.T13.4.1.12.2.1.1.1.1\">LoRA</span></span>\n<span class=\"ltx_tr\" id=\"A6.T13.4.1.12.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"A6.T13.4.1.12.2.1.1.2.1\">ar64_fr4</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.3\">TAIF</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.4\">54.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.5\">44.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.6\">55.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T13.4.1.12.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T13.4.1.12.7.1\">51.41</span></td>\n</tr>\n</table>"
        },
        {
            "id": "A6.T14",
            "type": "table",
            "title": "2405.20192v2_Table14",
            "caption": "Table 14:Full results of the ablation experiment on fine-tuning data size. We choose six data scales [1k, 10k, 50k, 100k, 200k, 1.8M] to validateTAIA\u2019s effectiveness.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A6.T14.5.1\">\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A6.T14.5.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.1.1.1\">Data Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A6.T14.5.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.1.2.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" id=\"A6.T14.5.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.1.3.1\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"A6.T14.5.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.1.4.1\">Knowledge</span></td>\n<td class=\"ltx_td ltx_border_tt\" id=\"A6.T14.5.1.1.5\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.1.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.2.1\">BBH</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.3.1\">CQA.</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.4.1\">LogiQA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.5.1\">SVAMP</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.6.1\">MMedB.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.2.7\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.7.1\">MMLU</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.2.8\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.2.8.1\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.1\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.3.2\">Base Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.3\">8.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.4\">26.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.5\">48.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.6\">33.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.3.7\">44.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.8\">32.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.3.9\">42.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.3.10\">33.71</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.4\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T14.5.1.4.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.4.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.4.3\">6.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.4.4\">18.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.4.5\">56.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.4.6\">29.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.4.7\">35.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.4.8\">26.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.4.9\">35.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.4.10\">29.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.1\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.1.1\">1k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.5.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.3.1\" style=\"background-color:#E6E6E6;\">6.50</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.4.1\" style=\"background-color:#E6E6E6;\">21.86</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.5.1\" style=\"background-color:#E6E6E6;\">60.77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.6.1\" style=\"background-color:#E6E6E6;\">31.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.5.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.7.1\" style=\"background-color:#E6E6E6;\">47.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.8.1\" style=\"background-color:#E6E6E6;\">27.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.5.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.5.9.1\" style=\"background-color:#E6E6E6;\">40.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.5.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.5.10.1\" style=\"background-color:#E6E6E6;\">33.76</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.6\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T14.5.1.6.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.6.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.6.3\">7.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.6.4\">18.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.6.5\">51.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.6.6\">34.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.6.7\">52.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.6.8\">37.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.6.9\">44.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.6.10\">35.29</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.1\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.1.1\">10k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.7.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.3.1\" style=\"background-color:#E6E6E6;\">8.34</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.4.1\" style=\"background-color:#E6E6E6;\">31.64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.5.1\" style=\"background-color:#E6E6E6;\">59.79</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.6.1\" style=\"background-color:#E6E6E6;\">33.18</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.7.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.7.1\" style=\"background-color:#E6E6E6;\">49.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.8.1\" style=\"background-color:#E6E6E6;\">39.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.7.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.7.9.1\" style=\"background-color:#E6E6E6;\">44.67</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.7.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.7.10.1\" style=\"background-color:#E6E6E6;\">38.10</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.8\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T14.5.1.8.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.8.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.8.3\">7.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.8.4\">19.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.8.5\">55.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.8.6\">30.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.8.7\">52.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.8.8\">37.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.8.9\">45.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.8.10\">35.45</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.1\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.1.1\">50k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.9.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.3.1\" style=\"background-color:#E6E6E6;\">8.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.4.1\" style=\"background-color:#E6E6E6;\">32.42</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.5.1\" style=\"background-color:#E6E6E6;\">61.67</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.6.1\" style=\"background-color:#E6E6E6;\">32.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.9.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.7.1\" style=\"background-color:#E6E6E6;\">49.10</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.8.1\" style=\"background-color:#E6E6E6;\">39.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.9.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.9.9.1\" style=\"background-color:#E6E6E6;\">45.16</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.9.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.9.10.1\" style=\"background-color:#E6E6E6;\">38.49</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.10\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T14.5.1.10.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.10.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.10.3\">7.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.10.4\">20.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.10.5\">59.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.10.6\">33.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.10.7\">53.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.10.8\">38.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.10.9\">44.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.10.10\">36.85</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.11\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.1\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.1.1\">100k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.11.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.3.1\" style=\"background-color:#E6E6E6;\">8.02</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.4.1\" style=\"background-color:#E6E6E6;\">30.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.5.1\" style=\"background-color:#E6E6E6;\">63.72</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.6.1\" style=\"background-color:#E6E6E6;\">30.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.11.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.7.1\" style=\"background-color:#E6E6E6;\">47.80</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.8.1\" style=\"background-color:#E6E6E6;\">37.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.11.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.11.9.1\" style=\"background-color:#E6E6E6;\">45.57</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.11.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.11.10.1\" style=\"background-color:#E6E6E6;\">37.78</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.12\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T14.5.1.12.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.12.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.12.3\">7.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.12.4\">19.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.12.5\">56.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.12.6\">30.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.12.7\">52.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.12.8\">38.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.12.9\">46.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.12.10\">36.04</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.13\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.1\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.1.1\">200k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.13.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.3.1\" style=\"background-color:#E6E6E6;\">8.44</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.4.1\" style=\"background-color:#E6E6E6;\">26.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.5.1\" style=\"background-color:#E6E6E6;\">60.77</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.6.1\" style=\"background-color:#E6E6E6;\">31.80</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.13.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.7.1\" style=\"background-color:#E6E6E6;\">58.33</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.8.1\" style=\"background-color:#E6E6E6;\">38.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T14.5.1.13.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.13.9.1\" style=\"background-color:#E6E6E6;\">42.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T14.5.1.13.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.13.10.1\" style=\"background-color:#E6E6E6;\">38.03</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.14\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T14.5.1.14.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.14.2\">Vanilla</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.14.3\">7.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.14.4\">15.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.14.5\">75.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.14.6\">34.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.14.7\">65.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.14.8\">38.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T14.5.1.14.9\">44.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T14.5.1.14.10\">40.07</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T14.5.1.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.1\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.1.1\">1.8M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T14.5.1.15.2\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.2.1\" style=\"background-color:#E6E6E6;\">TAIA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.3\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.3.1\" style=\"background-color:#E6E6E6;\">8.08</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.4\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.4.1\" style=\"background-color:#E6E6E6;\">30.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.5\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.5.1\" style=\"background-color:#E6E6E6;\">66.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.6\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.6.1\" style=\"background-color:#E6E6E6;\">33.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T14.5.1.15.7\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.7.1\" style=\"background-color:#E6E6E6;\">58.10</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.8\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.8.1\" style=\"background-color:#E6E6E6;\">39.59</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T14.5.1.15.9\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text\" id=\"A6.T14.5.1.15.9.1\" style=\"background-color:#E6E6E6;\">47.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T14.5.1.15.10\" style=\"background-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T14.5.1.15.10.1\" style=\"background-color:#E6E6E6;\">40.53</span></td>\n</tr>\n</table>"
        },
        {
            "id": "A6.T15",
            "type": "table",
            "title": "2405.20192v2_Table15",
            "caption": "Table 15:Variability ofTAIA.TAIAperforms generally more superior to the vanilla LoRA fine-tuning.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A6.T15.6.1\">\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A6.T15.6.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T15.6.1.1.1.1\">Setting</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"A6.T15.6.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T15.6.1.1.2.1\">Infer Mode</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" id=\"A6.T15.6.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T15.6.1.1.3.1\">Reasoning</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"A6.T15.6.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T15.6.1.1.4.1\">Knowledge</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A6.T15.6.1.1.5\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T15.6.1.1.5.1\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.2.1\">MATH</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.2.2\">BBH</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.2.3\">CQA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.2.4\">LogiQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.2.5\">SVAMP</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.2.6\">MMB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.2.7\">MMLU</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.3\">\n<td class=\"ltx_td ltx_border_t\" id=\"A6.T15.6.1.3.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.3.2\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.3.3\">20.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.3.4\">30.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.3.5\">78.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.3.6\">42.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.3.7\">54.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.3.8\">45.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.3.9\">57.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.3.10\">47.11</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A6.T15.6.1.4.1.1\">Run1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.4.2\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.3\">17.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.4\">36.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.5\">77.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.6\">37.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.4.7\">57.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.8\">44.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.4.9\">54.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.4.10\">46.50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.5.1\">TAIA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.5.2\">24.98</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.5.3\">43.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.5.4\">77.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.5.5\">41.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.5.6\">67.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.5.7\">46.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.5.8\">57.29</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.5.9\">51.27</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A6.T15.6.1.6.1.1\">Run2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.6.2\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.3\">18.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.4\">38.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.5\">75.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.6\">36.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.6.7\">56.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.8\">44.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.6.9\">55.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.6.10\">46.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.7.1\">TAIA</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.7.2\">26.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.7.3\">43.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.7.4\">75.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.7.5\">41.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.7.6\">67.60</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.7.7\">46.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"A6.T15.6.1.7.8\">57.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A6.T15.6.1.7.9\">51.20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A6.T15.6.1.8.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A6.T15.6.1.8.1.1\">Run3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.8.2\">LoRA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.8.3\">18.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.8.4\">38.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.8.5\">76.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.8.6\">36.87</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.8.7\">56.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.8.8\">44.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"A6.T15.6.1.8.9\">55.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A6.T15.6.1.8.10\">46.69</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A6.T15.6.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T15.6.1.9.1\">TAIA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T15.6.1.9.2\">27.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T15.6.1.9.3\">43.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T15.6.1.9.4\">76.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T15.6.1.9.5\">41.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T15.6.1.9.6\">67.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T15.6.1.9.7\">46.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"A6.T15.6.1.9.8\">57.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A6.T15.6.1.9.9\">51.38</td>\n</tr>\n</table>"
        }
    ],
    "metadata": {},
    "pdf_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\pdfs\\2405.20192v2.pdf",
    "HTML_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\htmls\\2405.20192v2.html"
}