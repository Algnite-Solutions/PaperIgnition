{
    "doc_id": "2405.19789v1",
    "title": "Estimating before Debiasing: A Bayesian Approach to Detaching Prior Bias in Federated Semi-Supervised Learning",
    "authors": [
        "Guogang Zhu",
        "Xuefeng Liu",
        "Xinghao Wu",
        "Shaojie Tang",
        "Chao Tang",
        "Jianwei Niu",
        "Hao Su"
    ],
    "categories": [
        "cs.LG",
        "cs.DC"
    ],
    "published_date": "2024-05-30 07:58:01+00:00",
    "abstract": "Federated Semi-Supervised Learning (FSSL) leverages both labeled and\nunlabeled data on clients to collaboratively train a model.In FSSL, the\nheterogeneous data can introduce prediction bias into the model, causing the\nmodel's prediction to skew towards some certain classes. Existing FSSL methods\nprimarily tackle this issue by enhancing consistency in model parameters or\noutputs. However, as the models themselves are biased, merely constraining\ntheir consistency is not sufficient to alleviate prediction bias. In this\npaper, we explore this bias from a Bayesian perspective and demonstrate that it\nprincipally originates from label prior bias within the training data. Building\nupon this insight, we propose a debiasing method for FSSL named FedDB. FedDB\nutilizes the Average Prediction Probability of Unlabeled Data (APP-U) to\napproximate the biased prior.During local training, FedDB employs APP-U to\nrefine pseudo-labeling through Bayes' theorem, thereby significantly reducing\nthe label prior bias. Concurrently, during the model aggregation, FedDB uses\nAPP-U from participating clients to formulate unbiased aggregate weights,\nthereby effectively diminishing bias in the global model. Experimental results\nshow that FedDB can surpass existing FSSL methods. The code is available at\nhttps://github.com/GuogangZhu/FedDB.",
    "text_chunks": [
        {
            "id": "S1",
            "type": "text",
            "title": "1Introduction",
            "caption": "1Introduction",
            "metadata": {},
            "text": "\n1 Introduction\nFederated Learning (FL) McMahan et al. (2017) is a distributed learning paradigm that can facilitate collaborative model training among multiple clients while preserving data privacy.\nPresently, most FL methods are confined to supervised learning (SL) settings, wherein it is presumed that each client maintains a fully labeled dataset.\nNevertheless, in real-world applications, data labeling is notably laborious and time-consuming.\nTherefore, a more realistic case involves each client possessing a mix of unlabeled and labeled data.\nThis specific scenario, known as Federated Semi-Supervised Learning (FSSL), has been explored in various studies Jeong et al. (2021); Lin et al. (2021); Diao et al. (2022) and is garnering increasing interest within the FL research community.\n\nIn this study, we focus on an FSSL setting where the data on each client are class-imbalanced.\nMoreover, it is assumed that both intra-client and inter-client data heterogeneity exist.\nSpecifically, intra-client data heterogeneity implies that both the labeled data and unlabeled data on an individual client originate from diverse distributions.\nInter-client data heterogeneity means that the overall distributions across clients are non-independent and identically distributed (Non-IID).\n\nIn the described scenario, the model\u2019s prediction can skew to some certain classes during the training, i.e., prediction bias.\nFigure 1 presents the experimental results conducted in the above scenario, where the overall distributions of labeled and unlabeled data are balanced.\nIt can be observed that due to class imbalance in the local client, the local model\u2019s predictions gradually skew towards the major classes in the local data.\nMore importantly, this prediction bias cannot be alleviated after model aggregation, even if the overall distributions are balanced.\nInstead, it evolves into a different form of bias due to the influence from other clients.\nThis bias can disrupt the pseudo-labeling process, further creating a \u2018vicious cycle\u2019 between pseudo-labeling and local model training.\n\nExisting FSSL methods attribute the above issue to the divergence across clients caused by heterogeneous data and primarily address it by promoting consistency between model parameters or outputs Zhang et al. (2021); Jiang et al. (2022); Liang et al. (2022). However, as both the local and global models are biased, merely constraining their consistency cannot fundamentally mitigate the model prediction bias.\n\nIn this paper, we delve into the essential reason for the prediction bias in FSSL from a Bayesian perspective.\nBased on Bayes\u2019 rule, the model prediction is as follows:\n\n\ud835\udc91\u2062(\ud835\udc9a|\ud835\udc99)=\ud835\udc91\u2062(\ud835\udc99|\ud835\udc9a)\u2062\ud835\udc91\u2062(\ud835\udc9a)\ud835\udc91\u2062(\ud835\udc99),\ud835\udc91conditional\ud835\udc9a\ud835\udc99\ud835\udc91conditional\ud835\udc99\ud835\udc9a\ud835\udc91\ud835\udc9a\ud835\udc91\ud835\udc99\\bm{p}(\\bm{y}|\\bm{x})=\\frac{\\bm{p}(\\bm{x}|\\bm{y})\\bm{p}(\\bm{y})}{\\bm{p}(\\bm{x}%\n)},bold_italic_p ( bold_italic_y | bold_italic_x ) = divide start_ARG bold_italic_p ( bold_italic_x | bold_italic_y ) bold_italic_p ( bold_italic_y ) end_ARG start_ARG bold_italic_p ( bold_italic_x ) end_ARG ,\n(1)\n\nwhere \ud835\udc91\u2062(\ud835\udc9a|\ud835\udc99)\ud835\udc91conditional\ud835\udc9a\ud835\udc99\\bm{p}(\\bm{y}|\\bm{x})bold_italic_p ( bold_italic_y | bold_italic_x ) is the model\u2019s prediction, \ud835\udc91\u2062(\ud835\udc99|\ud835\udc9a)\ud835\udc91conditional\ud835\udc99\ud835\udc9a\\bm{p}(\\bm{x}|\\bm{y})bold_italic_p ( bold_italic_x | bold_italic_y ) is the class conditional likelihood, \ud835\udc91\u2062(\ud835\udc9a)\ud835\udc91\ud835\udc9a\\bm{p}(\\bm{y})bold_italic_p ( bold_italic_y ) is the label prior.\nAs shown in Figure 2, both the label prior of local labeled data (i.e., \ud835\udc91l\u2062(\ud835\udc9a)subscript\ud835\udc91\ud835\udc59\ud835\udc9a\\bm{p}_{l}(\\bm{y})bold_italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_italic_y )) and unlabeled data (i.e., \ud835\udc91^u\u2062(\ud835\udc9a)subscript^\ud835\udc91\ud835\udc62\ud835\udc9a\\hat{\\bm{p}}_{u}(\\bm{y})over^ start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( bold_italic_y )) are biased. Consequently, the model can gradually absorb these biases during training. These biases are eventually injected into the global model through model aggregation, causing its output priors \ud835\udc91s\u2062(\ud835\udc9a)subscript\ud835\udc91\ud835\udc60\ud835\udc9a\\bm{p}_{s}(\\bm{y})bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_y ) to skew towards certain classes.\nWhen conducting inference on a balanced test dataset (i.e., \ud835\udc91t\u2062(\ud835\udc9a)subscript\ud835\udc91\ud835\udc61\ud835\udc9a\\bm{p}_{t}(\\bm{y})bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y )), the model may suffer severe performance degradation, as \ud835\udc91s\u2062(\ud835\udc9a)\u2260\ud835\udc91t\u2062(\ud835\udc9a)subscript\ud835\udc91\ud835\udc60\ud835\udc9asubscript\ud835\udc91\ud835\udc61\ud835\udc9a\\bm{p}_{s}(\\bm{y})\\neq\\bm{p}_{t}(\\bm{y})bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_y ) \u2260 bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y ).\n\nNevertheless, \ud835\udc91s\u2062(\ud835\udc9a)subscript\ud835\udc91\ud835\udc60\ud835\udc9a\\bm{p}_{s}(\\bm{y})bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_y ) is commonly challenging to estimate.\nOn the one hand, in local clients, the ambiguity of pseudo-labels for unlabeled data makes the label prior bias during local training intractable.\nOn the other hand, in the server, model aggregation combines influences from participating clients, further complicating the estimation of prior bias.\n\nTaking the class-wise accuracy on a balanced test dataset as the ground truth for prior bias, we find that the Average Prediction Probability of Unlabeled Data (APP-U) serves as a robust metric to approximate this bias.\nFigure 3 illustrates the Jensen\u2013Shannon (JS) divergence Lin (1991) between the ground truth bias and either the labeled data distribution or APP-U, where the solid line and shaded area represent the mean and range across clients, respectively.\nInterestingly, it reveals that for both the global and local models, prior bias does not consistently align with the labeled data distribution.\nRather, it shows a stronger correlation with APP-U, indicating that APP-U can effectively quantify the prior bias.\n\nBuilding upon the above insights, we introduce a hierarchical debiasing method for FSSL termed FedDB, to mitigate the prior bias at both the local training and global aggregation stages.\nDuring the local training, FedDB implements debiased pseudo-labeling (DPL) based on Bayes\u2019 theorem, with APP-U serving as the approximation of bias prior. This approach promotes a more balanced pseudo-labeling process for unlabeled data, substantially reducing the label prior bias during local training.\nAt the global aggregation stage, FedDB utilizes APP-U from the participating clients to determine optimal aggregation weights.\nThe above process, termed debiased model aggregation (DMA), effectively mitigates bias within the global model.\nIt should be noted that DPL can be seamlessly integrated with FSSL methods that utilize pseudo-labeling with minimal cost. This demonstrates its substantial potential for practical application of FSSL.\n\nThe main contributions of this paper are as follows:\n\n\u2022\nWe analyze the prediction bias in class-imbalanced FSSL from a Bayesian perspective.\n\n\u2022\nWe propose FedDB, a Bayesian debiasing method for FSSL that uses APP-U as an approximation of prior bias.\n\n\u2022\nWe conduct extensive experiments on multiple datasets to demonstrate the effectiveness of FedDB.\n\n\n"
        },
        {
            "id": "S2",
            "type": "text",
            "title": "2Related Work",
            "caption": "2Related Work",
            "metadata": {},
            "text": "\n2 Related Work\n\n2.1 Federated Learning\nData heterogeneity is a substantial challenge in FL, which can lead to considerable divergence across clients, thereby degrading the model performance Zhao et al. (2018); Li et al. (2020a).\nTo address this issue, various strategies are explored, including reducing the divergence across local models Li et al. (2020b); Acar et al. (2020); Karimireddy et al. (2020), enhancing aggregation schemes Wang et al. (2020); Acar et al. (2020); Reddi et al. (2021), promoting representation consistency across clients Tan et al. (2022); Zhu et al. (2023); Liao et al. (2023), developing personalized models for individual clients Collins et al. (2021); Liu et al. (2023).\nHowever, these methods primarily focus on SL settings, which is impractical as data labeling is laborious and time-consuming.\n\n\n2.2 Semi-Supervised Learning\nSSL aims to mitigate the reliance on labeled data, which prompts various mechanisms to leverage the latent information within unlabeled data.\nPseudo-labeling Lee and others (2013); Wang et al. (2023), also known as self-training, involves assigning pseudo-labels to unlabeled samples with high confidence, enabling their incorporation into the training process.\nConsistency regularization Miyato et al. (2018) introduces arbitrary perturbations to unlabeled samples and promotes the consistent predictions between different views of unlabeled data.\nAdditionally, hybrid methods that amalgamate these approaches are also developed, such as MixMatch Berthelot et al. (2019), FixMatch Sohn et al. (2020).\nRecently, SSL has focused on class imbalance, leading to various studies such as class-rebalancing sampling Wei et al. (2021), and pseudo label sampling Guo and Li (2022). However, simply combining these methods with FL is challenging, as they ignore the collaboration across clients.\n\n\n2.3 Federated Semi-Supervised Learning\nFSSL can be divided into three distinct scenarios Bai et al. (2023):\n(1) Labels-at-Partial-Clients, where\nonly a few clients have full labels, while the rest possess only unlabeled data Liang et al. (2022); Li et al. (2023); (2) Labels-at-Server, where labeled data are only available at the server, with local clients merely having unlabeled data Zhang et al. (2021); Jeong et al. (2021); Diao et al. (2022);\n(3) Labels-at-Clients, where each client has mostly unlabeled data and a few labeled samples Jeong et al. (2021); Bai et al. (2023).\n\nThis paper focuses on the Labels-at-Clients scenario.\nCurrently, several works have been proposed for this scenario.\nFor instance, SemiFed Lin et al. (2021) assigns pseudo-labels to unlabeled data only when multiple models provide consistent predictions.\nFedMatch Jeong et al. (2021) enforces prediction consistency across multiple models.\nHowever, these methods primarily concentrate on encouraging consistency across clients, overlooking the inherent prior biases within the model \u2014 a critical factor leading to performance degradation in FSSL with class imbalance.\n\n"
        },
        {
            "id": "S3",
            "type": "text",
            "title": "3Preliminary and Background",
            "caption": "3Preliminary and Background",
            "metadata": {},
            "text": "\n3 Preliminary and Background\nIn this section, we present the notations used in this paper, followed by a detailed discussion of the framework of FSSL.\n\n3.1 Problem Setting and Notation of FSSL\nWe focus on a FSSL setting for K-class classification task with totally M\ud835\udc40Mitalic_M clients participating in the training.\nEach client m\ud835\udc5amitalic_m maintains a labeled dataset \ud835\udc9flm={(\ud835\udc99n,\ud835\udc9an)}n=1Nlmsuperscriptsubscript\ud835\udc9f\ud835\udc59\ud835\udc5asuperscriptsubscriptsuperscript\ud835\udc99\ud835\udc5bsuperscript\ud835\udc9a\ud835\udc5b\ud835\udc5b1superscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc5a\\mathcal{D}_{l}^{m}=\\{(\\bm{x}^{n},\\bm{y}^{n})\\}_{n=1}^{N_{l}^{m}}caligraphic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = { ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT and an unlabeled dataset \ud835\udc9fum={(\ud835\udc99n)}n=1Numsuperscriptsubscript\ud835\udc9f\ud835\udc62\ud835\udc5asuperscriptsubscriptsuperscript\ud835\udc99\ud835\udc5b\ud835\udc5b1superscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc5a\\mathcal{D}_{u}^{m}=\\{(\\bm{x}^{n})\\}_{n=1}^{N_{u}^{m}}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = { ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT, where Nlmsuperscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc5aN_{l}^{m}italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and Numsuperscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc5aN_{u}^{m}italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT are the counts of labeled and unlabeled samples, respectively (typically, Num\u226bNlmmuch-greater-thansuperscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc5asuperscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc5aN_{u}^{m}\\gg N_{l}^{m}italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT \u226b italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT), \ud835\udc99n\u2208\ud835\udcb3\u2286\u211ddsuperscript\ud835\udc99\ud835\udc5b\ud835\udcb3superscript\u211d\ud835\udc51\\bm{x}^{n}\\in\\mathcal{X}\\subseteq\\mathbb{R}^{d}bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2208 caligraphic_X \u2286 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is the input sampled from a d\ud835\udc51ditalic_d-dimensional space, \ud835\udc9an\u2208\ud835\udcb4\u2286{0,1}Ksuperscript\ud835\udc9a\ud835\udc5b\ud835\udcb4superscript01\ud835\udc3e\\bm{y}^{n}\\in\\mathcal{Y}\\subseteq{\\{0,1\\}}^{K}bold_italic_y start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT \u2208 caligraphic_Y \u2286 { 0 , 1 } start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT is the one-hot label. For clarity, we sometimes omit the superscript denoting the client index in the following contents.\n\nWith a slight abuse of notation, we denote Nlksuperscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc58N_{l}^{k}italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and Nuksuperscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc58N_{u}^{k}italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT as the numbers of samples in class k\ud835\udc58kitalic_k under \ud835\udc9flsubscript\ud835\udc9f\ud835\udc59\\mathcal{D}_{l}caligraphic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and \ud835\udc9fusubscript\ud835\udc9f\ud835\udc62\\mathcal{D}_{u}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT for an arbitrary client, i.e., \u2211k=1KNlk=Nlsuperscriptsubscript\ud835\udc581\ud835\udc3esuperscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc58subscript\ud835\udc41\ud835\udc59\\sum_{k=1}^{K}N_{l}^{k}=N_{l}\u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and \u2211k=1KNuk=Nusuperscriptsubscript\ud835\udc581\ud835\udc3esuperscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc58subscript\ud835\udc41\ud835\udc62\\sum_{k=1}^{K}N_{u}^{k}=N_{u}\u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT. In this paper, we assume that both \ud835\udc9flsubscript\ud835\udc9f\ud835\udc59\\mathcal{D}_{l}caligraphic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and \ud835\udc9fusubscript\ud835\udc9f\ud835\udc62\\mathcal{D}_{u}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT exhibit class imbalance, that is, \u2203i,j\u2208{1,2,\u2026,K}\ud835\udc56\ud835\udc5712\u2026\ud835\udc3e\\exists i,j\\in\\{1,2,\\dots,K\\}\u2203 italic_i , italic_j \u2208 { 1 , 2 , \u2026 , italic_K } for which the ratio NliNljsuperscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc56superscriptsubscript\ud835\udc41\ud835\udc59\ud835\udc57\\frac{N_{l}^{i}}{N_{l}^{j}}divide start_ARG italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_ARG is significantly greater than 1111.\nIn other words, the label prior distribution {pl1,\u2026,plK}superscriptsubscript\ud835\udc5d\ud835\udc591\u2026superscriptsubscript\ud835\udc5d\ud835\udc59\ud835\udc3e\\{p_{l}^{1},\\dots,p_{l}^{K}\\}{ italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_p start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT } shifts from a uniform distribution {1K}Ksuperscript1\ud835\udc3e\ud835\udc3e\\{\\frac{1}{K}\\}^{K}{ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG } start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT.\nThis assumption is similarly applicable for \ud835\udc9fusubscript\ud835\udc9f\ud835\udc62\\mathcal{D}_{u}caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT.\n\nFurthermore, we consider the setting that both intra-client and inter-client data heterogeneity exist in the FL system.\nIntra-client heterogeneity refers to the varied distributions of labeled and unlabeled data within a single client, that is, \u2200m\u2208{1,2,\u2026,M},\ud835\udc9fum\u2260\ud835\udc9flmformulae-sequencefor-all\ud835\udc5a12\u2026\ud835\udc40superscriptsubscript\ud835\udc9f\ud835\udc62\ud835\udc5asuperscriptsubscript\ud835\udc9f\ud835\udc59\ud835\udc5a\\forall m\\in\\{1,2,\\dots,M\\},\\mathcal{D}_{u}^{m}\\neq\\mathcal{D}_{l}^{m}\u2200 italic_m \u2208 { 1 , 2 , \u2026 , italic_M } , caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT \u2260 caligraphic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT.\nInter-client heterogeneity, on the other hand, pertains to the dissimilar mixture distributions of both labeled and unlabeled data across clients, i.e., \u2200i,j\u2208{1,2,\u22ef\u2062M},i\u2260j,it holds that\u2062\ud835\udc9fli+\ud835\udc9fui\u2260\ud835\udc9flj+\ud835\udc9fujformulae-sequencefor-all\ud835\udc56\ud835\udc5712\u22ef\ud835\udc40formulae-sequence\ud835\udc56\ud835\udc57it holds thatsuperscriptsubscript\ud835\udc9f\ud835\udc59\ud835\udc56superscriptsubscript\ud835\udc9f\ud835\udc62\ud835\udc56superscriptsubscript\ud835\udc9f\ud835\udc59\ud835\udc57superscriptsubscript\ud835\udc9f\ud835\udc62\ud835\udc57\\forall i,j\\in\\{1,2,\\cdots M\\},i\\neq j,\\text{it holds that}\\ \\mathcal{D}_{l}^{%\ni}+\\mathcal{D}_{u}^{i}\\neq\\mathcal{D}_{l}^{j}+\\mathcal{D}_{u}^{j}\u2200 italic_i , italic_j \u2208 { 1 , 2 , \u22ef italic_M } , italic_i \u2260 italic_j , it holds that caligraphic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT + caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT \u2260 caligraphic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT + caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT.\n\nThe final objective of FSSL is to learn a global model\nf\u2062(\ud835\udc99;\ud835\udc98):\ud835\udcb3\u2192\ud835\udcb4:\ud835\udc53\ud835\udc99\ud835\udc98\u2192\ud835\udcb3\ud835\udcb4f(\\bm{x};\\bm{w}):\\mathcal{X}\\rightarrow\\mathcal{Y}italic_f ( bold_italic_x ; bold_italic_w ) : caligraphic_X \u2192 caligraphic_Y parameterized by \ud835\udc98\ud835\udc98\\bm{w}bold_italic_w that can generalize well to a balanced test dataset whose label prior distribution is {1K}Ksuperscript1\ud835\udc3e\ud835\udc3e\\{\\frac{1}{K}\\}^{K}{ divide start_ARG 1 end_ARG start_ARG italic_K end_ARG } start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT.\nGiven the input \ud835\udc99nsuperscript\ud835\udc99\ud835\udc5b\\bm{x}^{n}bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we denote its corresponding output logits as \ud835\udc9b\u2062(\ud835\udc99n):=f\u2062(\ud835\udc99n;\ud835\udc98)assign\ud835\udc9bsuperscript\ud835\udc99\ud835\udc5b\ud835\udc53superscript\ud835\udc99\ud835\udc5b\ud835\udc98\\bm{z}(\\bm{x}^{n}):=f(\\bm{x}^{n};\\bm{w})bold_italic_z ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) := italic_f ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; bold_italic_w ), and the normalized prediction probability after softmax layer as \ud835\udc91\u2062(\ud835\udc9a|\ud835\udc99n):=\u03c3\u2062(f\u2062(\ud835\udc9a|\ud835\udc99n;\ud835\udc98))assign\ud835\udc91conditional\ud835\udc9asuperscript\ud835\udc99\ud835\udc5b\ud835\udf0e\ud835\udc53conditional\ud835\udc9asuperscript\ud835\udc99\ud835\udc5b\ud835\udc98\\bm{p}(\\bm{y}|\\bm{x}^{n}):=\\sigma(f(\\bm{y}|\\bm{x}^{n};\\bm{w}))bold_italic_p ( bold_italic_y | bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) := italic_\u03c3 ( italic_f ( bold_italic_y | bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; bold_italic_w ) ), where \u03c3\u2062(\u22c5)\ud835\udf0e\u22c5\\sigma(\\cdot)italic_\u03c3 ( \u22c5 ) is the softmax function.\nThe detailed framework of FSSL is explained as follows.\n\n\n3.2 Framework of FSSL\nDuring each global round, the server first selects a random subset of clients \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S based on the activation rate C\ud835\udc36Citalic_C and broadcasts the global model \ud835\udc98\ud835\udc98\\bm{w}bold_italic_w to these clients. Subsequently, these clients perform local training for E\ud835\udc38Eitalic_E epochs using \ud835\udc98\ud835\udc98\\bm{w}bold_italic_w as initial weights, resulting in the updated local model \ud835\udc98msubscript\ud835\udc98\ud835\udc5a\\bm{w}_{m}bold_italic_w start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Finally, the selected clients upload their local models \ud835\udc98msubscript\ud835\udc98\ud835\udc5a\\bm{w}_{m}bold_italic_w start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT to the server for model aggregation. The training paradigms of labeled and unlabeled data in local clients are as follows.\n\nFor labeled data, the standard cross-entropy loss is applied to the weakly augmented version of samples to promote the discriminative objective, as shown below:\n\n\u2112s=1Nl\u2062\u2211n=1NlH\u2062(\ud835\udc9an,\ud835\udc91\u2062(\ud835\udc9a|\u03b1\u2062(\ud835\udc99n))),subscript\u2112\ud835\udc601subscript\ud835\udc41\ud835\udc59superscriptsubscript\ud835\udc5b1subscript\ud835\udc41\ud835\udc59Hsuperscript\ud835\udc9a\ud835\udc5b\ud835\udc91conditional\ud835\udc9a\ud835\udefcsuperscript\ud835\udc99\ud835\udc5b\\mathcal{L}_{s}=\\frac{1}{N_{l}}\\sum_{n=1}^{N_{l}}\\mathrm{H}(\\bm{y}^{n},\\bm{p}(%\n\\bm{y}|\\alpha(\\bm{x}^{n}))),caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT roman_H ( bold_italic_y start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , bold_italic_p ( bold_italic_y | italic_\u03b1 ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) ) ,\n(2)\n\nwhere \u03b1\u2062(\u22c5)\ud835\udefc\u22c5\\alpha(\\cdot)italic_\u03b1 ( \u22c5 ) is the weak augmentation function, \ud835\udc91\u2062(\ud835\udc9a|\u03b1\u2062(\ud835\udc99n))\ud835\udc91conditional\ud835\udc9a\ud835\udefcsuperscript\ud835\udc99\ud835\udc5b\\bm{p}(\\bm{y}|\\alpha(\\bm{x}^{n}))bold_italic_p ( bold_italic_y | italic_\u03b1 ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) is the prediction probability for \u03b1\u2062(\ud835\udc99n)\ud835\udefcsuperscript\ud835\udc99\ud835\udc5b\\alpha(\\bm{x}^{n})italic_\u03b1 ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ), and H\u2062(\ud835\udc911,\ud835\udc912)Hsubscript\ud835\udc911subscript\ud835\udc912\\mathrm{H}(\\bm{p}_{1},\\bm{p}_{2})roman_H ( bold_italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) is entropy between probability distributions \ud835\udc911subscript\ud835\udc911\\bm{p}_{1}bold_italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and \ud835\udc912subscript\ud835\udc912\\bm{p}_{2}bold_italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.\n\nFor unlabeled data, the samples are pseudo-labeled using the trained model, after which they are incorporated into the training process.\nSpecifically, for a given unlabeled sample \ud835\udc99nsuperscript\ud835\udc99\ud835\udc5b\\bm{x}^{n}bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, the model first generates the probability on its weakly augmented version. Then the pseudo-label is calculated by:\n\n\ud835\udc9a^n=arg\u2061max\u2061(\ud835\udc91\u2062(\ud835\udc9a|\u03b1\u2062(\ud835\udc99n))),superscript^\ud835\udc9a\ud835\udc5b\ud835\udc91conditional\ud835\udc9a\ud835\udefcsuperscript\ud835\udc99\ud835\udc5b\\hat{\\bm{y}}^{n}=\\arg\\max(\\bm{p}(\\bm{y}|\\alpha(\\bm{x}^{n}))),over^ start_ARG bold_italic_y end_ARG start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = roman_arg roman_max ( bold_italic_p ( bold_italic_y | italic_\u03b1 ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) ) ,\n(3)\n\nwhere arg\u2061max\u2061(\u22c5)\u22c5\\arg\\max(\\cdot)roman_arg roman_max ( \u22c5 ) is the function that converts a probability distribution into a one-hot label based on its maximum value.\n\nTo enhance the model generalization, the consistency loss is applied to unlabeled data by minimizing the entropy between the pseudo-label and the prediction of its strong augmented version.\n\nDuring the training, only those unlabeled samples that exhibit high confidence are selected to participate in further training.\nConsequently, the overall optimization objective for the unlabeled data can be expressed as follows:\n\n\u2112u=1Nu\u2062\u2211n=1Nusubscript\u2112\ud835\udc621subscript\ud835\udc41\ud835\udc62superscriptsubscript\ud835\udc5b1subscript\ud835\udc41\ud835\udc62\\displaystyle\\mathcal{L}_{u}=\\frac{1}{N_{u}}\\sum_{n=1}^{N_{u}}caligraphic_L start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUPERSCRIPT\n\ud835\udfd9(max(\ud835\udc91(\ud835\udc9a|\u03b1(\ud835\udc99n)))\u2265\u03c4)\u22c5\\displaystyle\\mathbb{1}(\\max(\\bm{p}(\\bm{y}|\\alpha(\\bm{x}^{n})))\\geq\\tau)\\cdotblackboard_1 ( roman_max ( bold_italic_p ( bold_italic_y | italic_\u03b1 ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) ) \u2265 italic_\u03c4 ) \u22c5\n(4)\n\n\nH\u2062(\ud835\udc9a^n,\ud835\udc91\u2062(\ud835\udc9a|\ud835\udc9c\u2062(\ud835\udc99n))),Hsuperscript^\ud835\udc9a\ud835\udc5b\ud835\udc91conditional\ud835\udc9a\ud835\udc9csuperscript\ud835\udc99\ud835\udc5b\\displaystyle\\mathrm{H}(\\hat{\\bm{y}}^{n},\\bm{p}(\\bm{y}|\\mathcal{A}(\\bm{x}^{n})%\n)),roman_H ( over^ start_ARG bold_italic_y end_ARG start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , bold_italic_p ( bold_italic_y | caligraphic_A ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) ) ,\n(5)\n\nwhere \u03c4\ud835\udf0f\\tauitalic_\u03c4 is the threshold, \ud835\udfd9\u2062(\u22c5)1\u22c5\\mathbb{1}(\\cdot)blackboard_1 ( \u22c5 ) is the indicator function, \ud835\udc9c\u2062(\u22c5)\ud835\udc9c\u22c5\\mathcal{A}(\\cdot)caligraphic_A ( \u22c5 ) is the strong augmentation function.\n\nThe overall optimization objective of local training on clients is expressed as:\n\n\u2112=\u2112s+\u03bb\u2062\u2112u,\u2112subscript\u2112\ud835\udc60\ud835\udf06subscript\u2112\ud835\udc62\\mathcal{L}=\\mathcal{L}_{s}+\\lambda\\mathcal{L}_{u},caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT + italic_\u03bb caligraphic_L start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ,\n(6)\n\nwhere \u03bb\ud835\udf06\\lambdaitalic_\u03bb is used to balance these two loss terms.\n\nAfter local training, the selected clients send the latest local models to the server for model aggregation, as shown below:\n\n\ud835\udc98t+1=1|\ud835\udcaet|\u2062\u2211m\u2208\ud835\udcaet\ud835\udf37m\u22c5\ud835\udc98mt,superscript\ud835\udc98\ud835\udc6111subscript\ud835\udcae\ud835\udc61subscript\ud835\udc5asubscript\ud835\udcae\ud835\udc61\u22c5subscript\ud835\udf37\ud835\udc5asuperscriptsubscript\ud835\udc98\ud835\udc5a\ud835\udc61\\bm{w}^{t+1}=\\frac{1}{\\left|\\mathcal{S}_{t}\\right|}\\sum_{m\\in\\mathcal{S}_{t}}%\n\\bm{\\beta}_{m}\\cdot\\bm{w}_{m}^{t},bold_italic_w start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG | caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | end_ARG \u2211 start_POSTSUBSCRIPT italic_m \u2208 caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT bold_italic_\u03b2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT \u22c5 bold_italic_w start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ,\n(7)\n\nwhere |\ud835\udcaet|subscript\ud835\udcae\ud835\udc61\\left|\\mathcal{S}_{t}\\right|| caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | is the number of selected clients in round t\ud835\udc61titalic_t, \ud835\udc98mtsubscriptsuperscript\ud835\udc98\ud835\udc61\ud835\udc5a\\bm{w}^{t}_{m}bold_italic_w start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is the local model on client m\ud835\udc5amitalic_m in round t\ud835\udc61titalic_t, \ud835\udf37msubscript\ud835\udf37\ud835\udc5a\\bm{\\beta}_{m}bold_italic_\u03b2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is the aggregate weight for \ud835\udc98mtsubscriptsuperscript\ud835\udc98\ud835\udc61\ud835\udc5a\\bm{w}^{t}_{m}bold_italic_w start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, \ud835\udc98t+1superscript\ud835\udc98\ud835\udc611\\bm{w}^{t+1}bold_italic_w start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT is the global model in round t+1\ud835\udc611t+1italic_t + 1.\n\n"
        },
        {
            "id": "S4",
            "type": "text",
            "title": "4FedDB: Detaching Prior Bias in FSSL",
            "caption": "4FedDB: Detaching Prior Bias in FSSL",
            "metadata": {},
            "text": "\n4 FedDB: Detaching Prior Bias in FSSL\nThis section details the framework of FedDB and its two key techniques: debiased pseudo-labeling (DPL) and debiased model aggregation (DMA).\n\n4.1 Framework Overview of FedDB\nFigure\u00a04 illustrates the framework of FedDB. During the training, each global round consists of the following steps:\n\n(1)\nThe server selects a subset of clients for training and broadcasts the global model to these clients;\n\n(2)\nThe clients perform inference on unlabeled data and calculate APP-U to estimate the prior bias;\n\n(3)\nThe clients perform DPL on unlabeled data using APP-U;\n\n(4)\nThe clients train the model utilizing both labeled data and pseudo-labeled data;\n\n(5)\nThe clients upload local models and APP-U to the server. The server performs DMA using APP-U from clients;\n\n(6)\nRepeating steps 1-5 until the global model converges.\n\n\n\n\n4.2 Prior Bias Estimation\nIn this paper, we consider an FSSL setting where both the labeled data and unlabeled data are class imbalanced.\nIn such a case, the model\u2019s predictions can skew towards certain classes, owing to the biased label prior in the training data.\nThis skew contradicts the training objective of FSSL, which is to achieve uniform performance across all classes.\n\nTo investigate the impact of class imbalance on model training in FSSL, we conduct preliminary experiments using the CIFAR10 dataset.\nWe establish a scenario with 10 clients, each participating in model training in every round.\nThe number of labeled and unlabeled samples is set to 4000400040004000 and 46000460004600046000, respectively.\nThe class imbalance is created by the Dirichlet distribution, as declared in Section 5.\n\nAs shown in Figure 1, both the local and global models exhibit a biased prediction towards certain classes.\nHowever, estimating the above bias in FSSL is challenging due to the data heterogeneity and imprecision in pseudo-labeling.\nBy extensive experiments, we discover that the prior bias can be effectively approximated by the Average Prediction Probability on Unlabeled Data (APP-U).\nSpecifically, for client m\ud835\udc5amitalic_m, the APP-U, denoted by \ud835\udc91\u00afmsubscript\u00af\ud835\udc91\ud835\udc5a\\overline{\\bm{p}}_{m}over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, can be calculated by:\n\n\ud835\udc91\u00afm=\u2211n=1Num\ud835\udc91\u2062(\ud835\udc9a|\u03b1\u2062(\ud835\udc99un))Num,subscript\u00af\ud835\udc91\ud835\udc5asuperscriptsubscript\ud835\udc5b1superscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc5a\ud835\udc91conditional\ud835\udc9a\ud835\udefcsuperscriptsubscript\ud835\udc99\ud835\udc62\ud835\udc5bsuperscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc5a\\overline{\\bm{p}}_{m}=\\frac{{\\textstyle\\sum_{n=1}^{N_{u}^{m}}}\\bm{p}(\\bm{y}|%\n\\alpha(\\bm{x}_{u}^{n}))}{N_{u}^{m}},over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT bold_italic_p ( bold_italic_y | italic_\u03b1 ( bold_italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_ARG ,\n(8)\n\nwhere Numsuperscriptsubscript\ud835\udc41\ud835\udc62\ud835\udc5aN_{u}^{m}italic_N start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT denotes the total number of unlabeled samples on client m\ud835\udc5amitalic_m, \ud835\udc91\u2062(\ud835\udc9a|\u03b1\u2062(\ud835\udc99un))\ud835\udc91conditional\ud835\udc9a\ud835\udefcsuperscriptsubscript\ud835\udc99\ud835\udc62\ud835\udc5b\\bm{p}(\\bm{y}|\\alpha(\\bm{x}_{u}^{n}))bold_italic_p ( bold_italic_y | italic_\u03b1 ( bold_italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ) is the prediction probability of the weak augmentation of sample \ud835\udc99unsuperscriptsubscript\ud835\udc99\ud835\udc62\ud835\udc5b\\bm{x}_{u}^{n}bold_italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.\n\nWe adopt JS divergence as a metric to quantify the disparity between two distributions.\nA larger JS divergence indicates a greater disparity between the distributions.\nTaking the class-wise accuracy on a balanced test dataset as the ground truth bias, we calculate the JS divergence between it and either APP-U or the labeled data distribution. As shown in Figure 3, for both local and global models, the JS divergence between APP-U and the ground truth is significantly smaller than that between the labeled data distribution and the ground truth. This demonstrates the effectiveness of APP-U as a metric for quantifying prior bias in FSSL.\n\n\n4.3 Debiased Pseudo-Labeling\nIn this subsection, we detail the procedure of DPL.\nGiven an FL model parameterized by \ud835\udc98\ud835\udc98\\bm{w}bold_italic_w, we first obtain the prediction probability \ud835\udc91s\u2062(\ud835\udc9a|\ud835\udc99)subscript\ud835\udc91\ud835\udc60conditional\ud835\udc9a\ud835\udc99\\bm{p}_{s}(\\bm{y}|\\bm{x})bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_y | bold_italic_x ) by applying a softmax function to unnormalized logits, as illustrated below:\n\n\ud835\udc91s\u2062(y|\ud835\udc99)=e\ud835\udc9b\u2062(\ud835\udc99)\u2062[y]\u2211k=1Ke\ud835\udc9b\u2062(\ud835\udc99)\u2062[k],subscript\ud835\udc91\ud835\udc60conditional\ud835\udc66\ud835\udc99superscript\ud835\udc52\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc66superscriptsubscript\ud835\udc581\ud835\udc3esuperscript\ud835\udc52\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc58\\bm{p}_{s}(y|\\bm{x})=\\frac{e^{\\bm{z}(\\bm{x})[y]}}{{\\textstyle\\sum_{k=1}^{K}}e^%\n{\\bm{z}(\\bm{x})[k]}},bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y | bold_italic_x ) = divide start_ARG italic_e start_POSTSUPERSCRIPT bold_italic_z ( bold_italic_x ) [ italic_y ] end_POSTSUPERSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT bold_italic_z ( bold_italic_x ) [ italic_k ] end_POSTSUPERSCRIPT end_ARG ,\n(9)\n\nwhere \ud835\udc9b\u2062(\ud835\udc99)\u2062[y]\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc66\\bm{z}(\\bm{x})[y]bold_italic_z ( bold_italic_x ) [ italic_y ] is the y\ud835\udc66yitalic_y-th unnormalized logit.\n\nBy applying the Bayes\u2019 theorem to \ud835\udc91s\u2062(y|\ud835\udc99)subscript\ud835\udc91\ud835\udc60conditional\ud835\udc66\ud835\udc99\\bm{p}_{s}(y|\\bm{x})bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y | bold_italic_x ), we obtain:\n\n\ud835\udc91s\u2062(y|\ud835\udc99)=\ud835\udc91s\u2062(y)\u2062\ud835\udc91s\u2062(\ud835\udc99|y)\u2211k=1K\ud835\udc91s\u2062(k)\u2062\ud835\udc91s\u2062(\ud835\udc99|k).subscript\ud835\udc91\ud835\udc60conditional\ud835\udc66\ud835\udc99subscript\ud835\udc91\ud835\udc60\ud835\udc66subscript\ud835\udc91\ud835\udc60conditional\ud835\udc99\ud835\udc66superscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udc91\ud835\udc60\ud835\udc58subscript\ud835\udc91\ud835\udc60conditional\ud835\udc99\ud835\udc58\\bm{p}_{s}(y|\\bm{x})=\\frac{\\bm{p}_{s}(y)\\bm{p}_{s}(\\bm{x}|y)}{{\\textstyle\\sum_%\n{k=1}^{K}}\\bm{p}_{s}(k)\\bm{p}_{s}(\\bm{x}|k)}.bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y | bold_italic_x ) = divide start_ARG bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y ) bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_x | italic_y ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_k ) bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_x | italic_k ) end_ARG .\n(10)\n\n\nDue to the class imbalance in our FSSL settings, the prior distribution \ud835\udc91s\u2062(k)subscript\ud835\udc91\ud835\udc60\ud835\udc58\\bm{p}_{s}(k)bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_k ), as outputted by the model, is biased towards certain majority classes. This leads to a biased prediction probability \ud835\udc91s\u2062(y|\ud835\udc99)subscript\ud835\udc91\ud835\udc60conditional\ud835\udc66\ud835\udc99\\bm{p}_{s}(y|\\bm{x})bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y | bold_italic_x ), causing the model to be overconfident in these majority classes.\nThe objective of DPL is to seek a conditional probability \ud835\udc91t\u2062(y|\ud835\udc99)subscript\ud835\udc91\ud835\udc61conditional\ud835\udc66\ud835\udc99\\bm{p}_{t}(y|\\bm{x})bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y | bold_italic_x ) that is robust across all classes, given the estimation of the model\u2019s biased prior \ud835\udc91\u00af\u00af\ud835\udc91\\overline{\\bm{p}}over\u00af start_ARG bold_italic_p end_ARG, as defined in Eq. (8).\n\nFollowing previous studies Tian et al. (2020); Kairouz et al. (2021); Hong et al. (2021), we assume that the class conditional likelihoods are the same in both the biased and debiased predictions, i.e., \ud835\udc91t\u2062(\ud835\udc99|y)=\ud835\udc91s\u2062(\ud835\udc99|y)subscript\ud835\udc91\ud835\udc61conditional\ud835\udc99\ud835\udc66subscript\ud835\udc91\ud835\udc60conditional\ud835\udc99\ud835\udc66\\bm{p}_{t}(\\bm{x}|y)=\\bm{p}_{s}(\\bm{x}|y)bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x | italic_y ) = bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_x | italic_y ).\nBy rearranging Eq. (9) and Eq. (10), we have:\n\n\nln\u2061(\ud835\udc91t\u2062(y)\u2062\ud835\udc91t\u2062(\ud835\udc99|y))=subscript\ud835\udc91\ud835\udc61\ud835\udc66subscript\ud835\udc91\ud835\udc61conditional\ud835\udc99\ud835\udc66absent\\displaystyle\\ln(\\bm{p}_{t}(y)\\bm{p}_{t}(\\bm{x}|y))=roman_ln ( bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y ) bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x | italic_y ) ) =\n\ud835\udc9b\u2062(\ud835\udc99)\u2062[y]+ln\u2061(\ud835\udc91t\u2062(y))\u2212ln\u2061(\ud835\udc91s\u2062(y))\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc66subscript\ud835\udc91\ud835\udc61\ud835\udc66subscript\ud835\udc91\ud835\udc60\ud835\udc66\\displaystyle\\bm{z}(\\bm{x})[y]+\\ln(\\bm{p}_{t}(y))-\\ln(\\bm{p}_{s}(y))bold_italic_z ( bold_italic_x ) [ italic_y ] + roman_ln ( bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y ) ) - roman_ln ( bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y ) )\n(11)\n\n\n+ln\u2061(\u2211k=1K\ud835\udc91s\u2062(k)\u2062\ud835\udc91s\u2062(\ud835\udc99|k))superscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udc91\ud835\udc60\ud835\udc58subscript\ud835\udc91\ud835\udc60conditional\ud835\udc99\ud835\udc58\\displaystyle+\\ln({\\textstyle\\sum_{k=1}^{K}}\\bm{p}_{s}(k)\\bm{p}_{s}(\\bm{x}|k))+ roman_ln ( \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_k ) bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_x | italic_k ) )\n\n\n\u2212ln\u2061(\u2211k=1Ke\ud835\udc9b\u2062(\ud835\udc99)\u2062[k]).superscriptsubscript\ud835\udc581\ud835\udc3esuperscript\ud835\udc52\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc58\\displaystyle-\\ln({\\textstyle\\sum_{k=1}^{K}}e^{\\bm{z}(\\bm{x})[k]}).- roman_ln ( \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT bold_italic_z ( bold_italic_x ) [ italic_k ] end_POSTSUPERSCRIPT ) .\n\n\n\nRecalling that:\n\n\ud835\udc9b\u2062(\ud835\udc99)\u2062[y]=ln\u2061\ud835\udc91s\u2062(y|\ud835\udc99)+ln\u2062\u2211k=1K\ud835\udc91s\u2062(k)\u2062\ud835\udc91s\u2062(\ud835\udc99|k).\ud835\udc9b\ud835\udc99delimited-[]\ud835\udc66subscript\ud835\udc91\ud835\udc60conditional\ud835\udc66\ud835\udc99superscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udc91\ud835\udc60\ud835\udc58subscript\ud835\udc91\ud835\udc60conditional\ud835\udc99\ud835\udc58\\bm{z}(\\bm{x})[y]=\\ln\\bm{p}_{s}(y|\\bm{x})+\\ln{{\\textstyle\\sum_{k=1}^{K}}\\bm{p}%\n_{s}(k)\\bm{p}_{s}(\\bm{x}|k)}.bold_italic_z ( bold_italic_x ) [ italic_y ] = roman_ln bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y | bold_italic_x ) + roman_ln \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_k ) bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( bold_italic_x | italic_k ) .\n(12)\n\n\nWe derive the following debiased posterior probability:\n\n\n\ud835\udc91t\u2062(y|\ud835\udc99)subscript\ud835\udc91\ud835\udc61conditional\ud835\udc66\ud835\udc99\\displaystyle\\bm{p}_{t}(y|\\bm{x})bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y | bold_italic_x )\n=\ud835\udc91t\u2062(y)\u2062\ud835\udc91t\u2062(\ud835\udc99|y)\u2211k=1K\ud835\udc91t\u2062(k)\u2062\ud835\udc91t\u2062(\ud835\udc99|k)absentsubscript\ud835\udc91\ud835\udc61\ud835\udc66subscript\ud835\udc91\ud835\udc61conditional\ud835\udc99\ud835\udc66superscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udc91\ud835\udc61\ud835\udc58subscript\ud835\udc91\ud835\udc61conditional\ud835\udc99\ud835\udc58\\displaystyle=\\frac{\\bm{p}_{t}(y)\\bm{p}_{t}(\\bm{x}|y)}{\\sum_{k=1}^{K}\\bm{p}_{t%\n}(k)\\bm{p}_{t}(\\bm{x}|k)}= divide start_ARG bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y ) bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x | italic_y ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_k ) bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x | italic_k ) end_ARG\n(13)\n\n\n=\ud835\udc91s\u2062(y|\ud835\udc99)\u2062\ud835\udc91t\u2062(y)/\ud835\udc91s\u2062(y)\u2211k=1K\ud835\udc91s\u2062(k|\ud835\udc99)\u2062\ud835\udc91t\u2062(k)/\ud835\udc91s\u2062(k),absentsubscript\ud835\udc91\ud835\udc60conditional\ud835\udc66\ud835\udc99subscript\ud835\udc91\ud835\udc61\ud835\udc66subscript\ud835\udc91\ud835\udc60\ud835\udc66superscriptsubscript\ud835\udc581\ud835\udc3esubscript\ud835\udc91\ud835\udc60conditional\ud835\udc58\ud835\udc99subscript\ud835\udc91\ud835\udc61\ud835\udc58subscript\ud835\udc91\ud835\udc60\ud835\udc58\\displaystyle=\\frac{\\bm{p}_{s}(y|\\bm{x})\\bm{p}_{t}(y)/\\bm{p}_{s}(y)}{{%\n\\textstyle\\sum_{k=1}^{K}}\\bm{p}_{s}(k|\\bm{x})\\bm{p}_{t}(k)/\\bm{p}_{s}(k)},= divide start_ARG bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y | bold_italic_x ) bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y ) / bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_y ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_k | bold_italic_x ) bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_k ) / bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_k ) end_ARG ,\n\n\n\nwhere \ud835\udc91t\u2062(k)subscript\ud835\udc91\ud835\udc61\ud835\udc58\\bm{p}_{t}(k)bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_k ) is a uniform distribution that is robust for all classes.\nBy applying the estimated bias \ud835\udc91\u00af\u00af\ud835\udc91\\overline{\\bm{p}}over\u00af start_ARG bold_italic_p end_ARG as the approximation of the prior bias \ud835\udc91ssubscript\ud835\udc91\ud835\udc60\\bm{p}_{s}bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT,\nwe can obtain the debiased prediction probability of unlabeled data as follows:\n\n\ud835\udc91^=\ud835\udc91\u2062(\ud835\udc9a|\ud835\udc99)/\ud835\udc91\u00af\u2211k=1K\ud835\udc91\u2062(k|\ud835\udc99)/\ud835\udc91\u00afk.^\ud835\udc91\ud835\udc91conditional\ud835\udc9a\ud835\udc99\u00af\ud835\udc91superscriptsubscript\ud835\udc581\ud835\udc3e\ud835\udc91conditional\ud835\udc58\ud835\udc99subscript\u00af\ud835\udc91\ud835\udc58\\hat{\\bm{p}}=\\frac{\\bm{p}(\\bm{y}|\\bm{x})/\\overline{\\bm{p}}}{{\\textstyle\\sum_{k%\n=1}^{K}}\\bm{p}(k|\\bm{x})/\\overline{\\bm{p}}_{k}}.over^ start_ARG bold_italic_p end_ARG = divide start_ARG bold_italic_p ( bold_italic_y | bold_italic_x ) / over\u00af start_ARG bold_italic_p end_ARG end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_p ( italic_k | bold_italic_x ) / over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG .\n(14)\n\n\nIntuitively, Eq. (14) serves as a regularization term that smooths the prediction probabilities of the majority classes and sharpens these of the minority classes, which can alleviate the prior bias introduced by the heterogeneous data.\nThe detailed procedures of DPL are shown in Algorithm 1.\n\n\n4.4 Debiased Model Aggregation\nThe objective of DMA is to computing aggregation weights that enable the model to perform uniformly across all classes.\nDuring each local updating round, the activated clients send their accumulated APP-U \ud835\udc91\u00afmsubscript\u00af\ud835\udc91\ud835\udc5a\\overline{\\bm{p}}_{m}over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and their latest models \ud835\udc98msubscript\ud835\udc98\ud835\udc5a\\bm{w}_{m}bold_italic_w start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT to the server. Then we can get the aggregated APP-U as follows:\n\n\ud835\udc91\u00afa\u2062g\u2062g\u2062r=\u2211m\u2208\ud835\udcaet\ud835\udf37m\u2062\ud835\udc91\u00afm,subscript\u00af\ud835\udc91\ud835\udc4e\ud835\udc54\ud835\udc54\ud835\udc5fsubscript\ud835\udc5asubscript\ud835\udcae\ud835\udc61subscript\ud835\udf37\ud835\udc5asubscript\u00af\ud835\udc91\ud835\udc5a\\overline{\\bm{p}}_{aggr}={\\textstyle\\sum_{m\\in\\mathcal{S}_{t}}}\\bm{\\beta}_{m}%\n\\overline{\\bm{p}}_{m},over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_a italic_g italic_g italic_r end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_m \u2208 caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT bold_italic_\u03b2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ,\n(15)\n\nwhere \ud835\udf37msubscript\ud835\udf37\ud835\udc5a\\bm{\\beta}_{m}bold_italic_\u03b2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT denotes the aggregation weight for client m\ud835\udc5amitalic_m. To achieve a more balanced model, we expect \ud835\udc91\u00afa\u2062g\u2062g\u2062rsubscript\u00af\ud835\udc91\ud835\udc4e\ud835\udc54\ud835\udc54\ud835\udc5f\\overline{\\bm{p}}_{aggr}over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_a italic_g italic_g italic_r end_POSTSUBSCRIPT to be more uniform, leading to the following optimization objective:\n\n\n\nmin\ud835\udf37\u2061\u2112a\u2062g\u2062g\u2062r=\u2211m=1M(\ud835\udc91\u00afa\u2062g\u2062g\u2062r\u2212\ud835\udc91t)2subscript\ud835\udf37subscript\u2112\ud835\udc4e\ud835\udc54\ud835\udc54\ud835\udc5fsuperscriptsubscript\ud835\udc5a1\ud835\udc40superscriptsubscript\u00af\ud835\udc91\ud835\udc4e\ud835\udc54\ud835\udc54\ud835\udc5fsubscript\ud835\udc91\ud835\udc612\\displaystyle\\min_{\\bm{\\beta}}\\mathcal{L}_{aggr}=\\sqrt{\\textstyle\\sum_{m=1}^{M%\n}(\\overline{\\bm{p}}_{aggr}-\\bm{p}_{t})^{2}}roman_min start_POSTSUBSCRIPT bold_italic_\u03b2 end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_a italic_g italic_g italic_r end_POSTSUBSCRIPT = square-root start_ARG \u2211 start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT ( over\u00af start_ARG bold_italic_p end_ARG start_POSTSUBSCRIPT italic_a italic_g italic_g italic_r end_POSTSUBSCRIPT - bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG\n(16)\n\n\ns.t.\u2062\u2211m\u2208\ud835\udcaet\ud835\udf37m=1,s.t.subscript\ud835\udc5asubscript\ud835\udcae\ud835\udc61subscript\ud835\udf37\ud835\udc5a1\\displaystyle\\text{s.t.}\\textstyle\\sum_{m\\in\\mathcal{S}_{t}}\\bm{\\beta}_{m}=1,s.t. \u2211 start_POSTSUBSCRIPT italic_m \u2208 caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT bold_italic_\u03b2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 1 ,\n\n\nwhere \ud835\udc91t={1K}Ksubscript\ud835\udc91\ud835\udc61superscript1\ud835\udc3e\ud835\udc3e\\bm{p}_{t}=\\{\\frac{1}{K}\\}^{K}bold_italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = { divide start_ARG 1 end_ARG start_ARG italic_K end_ARG } start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT is the uniform distribution over K\ud835\udc3eKitalic_K classes, identical to the test dataset.\nIn FedDB, we utilize the gradient descent algorithm to solve the above optimization problem.\n\nAfter obtaining the aggregation weights \ud835\udf37\ud835\udf37\\bm{\\beta}bold_italic_\u03b2, we aggregate client models and update the global model as follows:\n\n\ud835\udc98t+1=\u2211m\u2208\ud835\udcaet\ud835\udf37m\u22c5\ud835\udc98mt,superscript\ud835\udc98\ud835\udc611subscript\ud835\udc5asubscript\ud835\udcae\ud835\udc61\u22c5subscript\ud835\udf37\ud835\udc5asuperscriptsubscript\ud835\udc98\ud835\udc5a\ud835\udc61\\bm{w}^{t+1}=\\textstyle\\sum_{m\\in\\mathcal{S}_{t}}\\bm{\\beta}_{m}\\cdot\\bm{w}_{m}%\n^{t},bold_italic_w start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_m \u2208 caligraphic_S start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT bold_italic_\u03b2 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT \u22c5 bold_italic_w start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ,\n(17)\n\nwhere \ud835\udc98mtsubscriptsuperscript\ud835\udc98\ud835\udc61\ud835\udc5a\\bm{w}^{t}_{m}bold_italic_w start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is the local model of client m\ud835\udc5amitalic_m at last round, \ud835\udc98t+1superscript\ud835\udc98\ud835\udc611\\bm{w}^{t+1}bold_italic_w start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT is the global model. \ud835\udc98t+1superscript\ud835\udc98\ud835\udc611\\bm{w}^{t+1}bold_italic_w start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT is then broadcast to the activated client for further updates.\nThe processes of DMA and FedDB are presented in Algorithms 2 and 3, respectively.\n\n\n"
        },
        {
            "id": "S5",
            "type": "text",
            "title": "5Experiments",
            "caption": "5Experiments",
            "metadata": {},
            "text": "\n5 Experiments\nThis section details the experimental results in various settings to demonstrate the effectiveness of FedDB.\n\n5.1 Experimental Setup\nDatasets.\nWe evaluate FedDB on three benchmark datasets, including CIFAR10, SVHN, and CIFAR100.\nInitially, a balanced labeled dataset is separated from the original training dataset, with the residual data designated as the unlabeled dataset.\nWhen distributing these training data to clients, we sample data from a Dirichlet distribution \ud835\udc92\u223cDir\u2062(\u03b4\u2062\ud835\udc91)similar-to\ud835\udc92Dir\ud835\udeff\ud835\udc91\\bm{q}\\sim\\text{Dir}(\\delta\\bm{p})bold_italic_q \u223c Dir ( italic_\u03b4 bold_italic_p ), where \ud835\udc91\ud835\udc91\\bm{p}bold_italic_p is the class-wise prior distribution and \u03b4\ud835\udeff\\deltaitalic_\u03b4 is a parameter that modulates the heterogeneity among clients.\nA higher value of \u03b4\ud835\udeff\\deltaitalic_\u03b4 correlates with reduced data heterogeneity.\nTo enrich the unlabeled dataset, we add the samples from the labeled dataset to the unlabeled dataset after discarding their labels.\nWe conduct experiments in IID setting and Non-IID settings with \u03b4={0.1,0.3}\ud835\udeff0.10.3\\delta=\\{0.1,0.3\\}italic_\u03b4 = { 0.1 , 0.3 }.\nIn the IID setting, the total number of labeled samples is set to 4000,1000,1000040001000100004000,1000,100004000 , 1000 , 10000 for CIFAR10, SVHN and CIFAR100, respectively. For Non-IID setting, the total number of labeled data is set to 4000400040004000 for CIFAR10 and SVHN, and 10000100001000010000 for CIFAR100.\nThe test dataset from the original dataset is used for model evaluation.\n\nBenchmark Methods.\nWe compare FedDB against the following benchmark methods:\n\n\u2022\nFedAvg McMahan et al. (2017): The FedAvg method is applied in a constrained scenario where each client utilizes only the small labeled dataset for training.\n\n\u2022\nFixMatch Sohn et al. (2020): This method is a basic adaptation of FixMatch within FedAvg framework.\n\n\u2022\nFedMatch Jeong et al. (2021): FedMatch introduces the inter-client consistency loss to maximize the agreement between local models.\n\n\u2022\nFedRGD Zhang et al. (2021): It mitigates the model bias by reducing gradient divergence among clients.\n\n\u2022\nSemiFL Diao et al. (2022): SemiFL adopts alternate training between server and clients. Here, we adopts its client-side training due to the lack of training samples on the server in our scenario.\n\n\u2022\nMethods combining DPL. We also conduct experiments that integrate DPL with benchmark methods. These hybrid methods are denoted as Method-FedDPL.\n\n\n\nImplementation Details.\nWe primarily follow the experimental settings adopted in prior works of FSSL Jeong et al. (2021).\nThere are a total of 100100100100 clients participating in the training, with 10101010 active clients (C=0.1)\ud835\udc360.1(C=0.1)( italic_C = 0.1 ) engaged in each global round.\nThe local training epoch is set to E=5\ud835\udc385E=5italic_E = 5 and the epoch for updating the model aggregation weights is set to Ea\u2062g\u2062g\u2062r=100subscript\ud835\udc38\ud835\udc4e\ud835\udc54\ud835\udc54\ud835\udc5f100E_{aggr}=100italic_E start_POSTSUBSCRIPT italic_a italic_g italic_g italic_r end_POSTSUBSCRIPT = 100.\nAll experiments are executed for 800800800800 global rounds.\nWe employ Wide ResNet28x2 in our experiments.\nThe SGD optimizer is adopted for model training, operating at learning rates \u03b7=0.03\ud835\udf020.03\\eta=0.03italic_\u03b7 = 0.03 for local updating and \u03b7a\u2062g\u2062g\u2062r=1.0subscript\ud835\udf02\ud835\udc4e\ud835\udc54\ud835\udc54\ud835\udc5f1.0\\eta_{aggr}=1.0italic_\u03b7 start_POSTSUBSCRIPT italic_a italic_g italic_g italic_r end_POSTSUBSCRIPT = 1.0 for aggregation, complemented by a momentum of 0.90.90.90.9.\nDue to the limited number of samples on clients, we feed all training data simultaneously to the model during local training.\nThe confidence threshold for pseudo-labeling is set to \u03c4=0.95\ud835\udf0f0.95\\tau=0.95italic_\u03c4 = 0.95.\nThe data augmentation operation is consistent with those described in FixMatch Sohn et al. (2020).\nAll experiments are repeated for 4444 times and we report the mean and standard deviation of the best accuracy during training.\n\n\n5.2 Results on Benchmark Datasets\nThe experimental results are presented in Tables 1 - 3, where values inside the parentheses represent the mean, and values outside the parentheses represent the standard deviation of multiple experiments.\nIt can be observed that with the same number of labeled samples, the accuracy of all methods decreases as \u03b4\ud835\udeff\\deltaitalic_\u03b4 decreases, demonstrating that data heterogeneity is a key factor harming model performance.\nFedAvg, despite its simplicity, serves as a reliable benchmark method, particularly as the dataset difficulty increases (e.g., CIFAR100).\nThis issue is also noted by Diao et al. (2022).\nThis demonstrates that improperly incorporating unlabeled data into training can negatively impact the model\u2019s training.\nCompared with other FSSL methods, FedDB enhances test accuracy, demonstrating the effectiveness of FedDB in the FSSL scenario.\nThe same conclusion can also be drawn from Figure 5.\n\n\n\n5.3 Effectiveness of DPL\nAs illustrated in Table 4, employing DPL results in substantial gains for FedDB.\nFurthermore, DPL can be regarded as a convenient plug-in that can be easily integrated into existing FSSL methods utilizing pseudo-labeling.\nAs shown in Tables 1 - 3, introducing DPL to existing FSSL methods effectively enhances their performance.\nFigure 6 displays the accuracy of pseudo-labels during training. It indicates that DPL effectively enhances the accuracy of these pseudo-labels, which in turn benefits FSSL training.\nFigure 7 presents the ratio of pseudo-labeled samples in the unlabeled data.\nHowever, introducing DPL does not consistently improve the ratio of pseudo-labeled samples, as the model in FSSL is challenging to train, making it difficult for samples to be pseudo-labeled.\n\n\n\n5.4 Effectiveness of DMA\nAs shown in Table 4, DMA generally contributes positively to FedDB in most scenarios. However, its impact differs among various datasets.\nMore specifically, DMA consistently results in improved outcomes on the CIFAR10 and CIFAR100 datasets.\nConversely, on the SVHN dataset, DMA can lead to performance decline in certain scenarios.\nUpon detailed analysis, we ascribe this issue to the imbalanced distribution of the SVHN dataset, which contravenes the objective of FSSL that seeks for a balanced model.\n\n"
        },
        {
            "id": "S6",
            "type": "text",
            "title": "6Conclusion",
            "caption": "6Conclusion",
            "metadata": {},
            "text": "\n6 Conclusion\nIn this paper, we propose FedDB to detach prior bias in FSSL with class imbalance.\nAt the local training level, FedDB debiases the pseudo-labeling using APP-U based on Bayes\u2019 theorem, encouraging a more balanced training data during the training.\nAt the global aggregation level, FedDB leverages APP-U across different clients to derive optimal aggregation weights, aiming to debias the global model.\nExtensive experiments have shown the effectiveness of FedDB.\n"
        },
        {
            "id": "Sx1",
            "type": "text",
            "title": "Acknowledgements",
            "caption": "Acknowledgements",
            "metadata": {},
            "text": "\nAcknowledgements\nThis work was supported by the National Natural Science\nFoundation of China under Grants 62372028 and 62372027.\n"
        }
    ],
    "figure_chunks": [
        {
            "id": "S1.F1",
            "type": "figure",
            "title": "2405.19789v1_Figure1",
            "caption": "Figure 1:Class-wise test accuracy on a balanced test dataset, along with the labeled data distribution on an individual client. (a) Test accuracy of local model, (b) Test accuracy of global model. The class indexes are ranked based on the labeled data distribution.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure1.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S1.F2",
            "type": "figure",
            "title": "2405.19789v1_Figure2",
            "caption": "Figure 2:Prior bias in class-imbalanced FSSL.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure2.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S1.F3",
            "type": "figure",
            "title": "2405.19789v1_Figure3",
            "caption": "Figure 3:JS divergence between the ground truth bias and either the labeled data distribution or APP-U on clients. (a) Results on the local model, (b) Results on the global model.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure3.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S4.F4",
            "type": "figure",
            "title": "2405.19789v1_Figure4",
            "caption": "Figure 4:Framework overview of FedDB.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure4.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F5",
            "type": "figure",
            "title": "2405.19789v1_Figure5",
            "caption": "Figure 5:Convergence curve on CIFAR100. (a) IID, (b)Non-IID with\u03b4=0.3\ud835\udeff0.3\\delta=0.3italic_\u03b4 = 0.3.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure5.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F6",
            "type": "figure",
            "title": "2405.19789v1_Figure6",
            "caption": "Figure 6:Accuracy of pseudo labels on CIFAR100. (a) IID, (b)Non-IID with\u03b4=0.3\ud835\udeff0.3\\delta=0.3italic_\u03b4 = 0.3.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure6.png",
            "alt_text": "Refer to caption"
        },
        {
            "id": "S5.F7",
            "type": "figure",
            "title": "2405.19789v1_Figure7",
            "caption": "Figure 7:Ratio of unlabeled samples that are finally assigned with pseudo-labels on CIFAR100. (a) IID, (b)Non-IID with\u03b4=0.3\ud835\udeff0.3\\delta=0.3italic_\u03b4 = 0.3.",
            "metadata": {},
            "image_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\imgs\\2405.19789v1_Figure7.png",
            "alt_text": "Refer to caption"
        }
    ],
    "table_chunks": [
        {
            "id": "S5.T1",
            "type": "table",
            "title": "2405.19789v1_Table1",
            "caption": "Table 1:Experimental results in the IID setting.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T1.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.1.1.1.1.1\">Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.1.1.1.1.2\">CIFAR10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.1.1.1.1.3\">SVHN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.1.1.1.1.4\">CIFAR100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.2.2.1\">FedAvg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.2.2.2\">58.42(0.61)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.2.2.3\">25.10(0.76)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.2.2.4\">32.00(0.80)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.3.1\">FixMatch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.3.2\">65.80(2.72)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.3.3\">87.44(1.35)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.3.3.4\">24.72(0.73)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.4.1\">FedMatch</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.4.2\">39.63(1.66)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.4.3\">25.09(5.40)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.4.4.4\">9.44(0.66)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.5.1\">FedRGD</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.5.2\">63.27(1.47)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.5.3\">81.04(2.43)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.5.5.4\">14.45(0.42)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.6.1\">SemiFL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.6.2\">57.24(7.96)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.6.3\">85.58(10.03)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.6.6.4\">22.61(3.07)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.7.7.1\">FixMatch-FedDPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.7.7.2\">66.97(2.84)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.7.7.3\">88.00(0.67)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.1.7.7.4\">26.44(1.73)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.8.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.8.8.1\">FedMatch-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.8.8.2\">43.06(3.16)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.8.8.3\">25.90(3.12)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.8.8.4\">9.47(0.79)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.9.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.9.9.1\">FedRGD-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.9.9.2\">64.75(1.20)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.9.9.3\">81.24(5.36)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.9.9.4\">17.17(0.98)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.10.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.10.10.1\">SemiFL-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.10.10.2\">68.46(3.61)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.10.10.3\">86.77(1.79)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.10.10.4\">27.67(0.89)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.11.11.1\">FedDB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.11.11.2\">67.32(2.31)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.11.11.3\">86.75(0.90)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T1.1.1.11.11.4\">26.71(0.87)</td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T2",
            "type": "table",
            "title": "2405.19789v1_Table2",
            "caption": "Table 2:Experimental results in the Non-IID setting with\u03b4=0.3\ud835\udeff0.3\\delta=0.3italic_\u03b4 = 0.3.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T2.3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.1\">Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.2\">CIFAR10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.3\">SVHN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.3.1.1.1.4\">CIFAR100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.2.2.1\">FedAvg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.2.2.2\">47.72(1.95)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.2.2.3\">69.44(6.21)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.2.2.4\">31.34(0.36)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.3.3.1\">FixMatch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.3.3.2\">50.99(2.49)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.3.3.3\">86.61(0.19)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.3.3.4\">25.47(0.46)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.4.4.1\">FedMatch</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.4.4.2\">38.64(2.49)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.4.4.3\">26.04(4.85)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.4.4.4\">8.77(0.57)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.5.5.1\">FedRGD</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.5.5.2\">51.45(2.39)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.5.5.3\">86.89(3.21)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.5.5.4\">14.83(0.34)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.6.6.1\">SemiFL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.6.6.2\">50.07(1.05)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.6.6.3\">76.11(6.3)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.6.6.4\">26.40(0.81)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.7.7.1\">FixMatch-FedDPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.7.7.2\">53.92(3.41)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.7.7.3\">85.87(0.51)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.3.1.7.7.4\">28.47(0.13)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.8.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.8.8.1\">FedMatch-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.8.8.2\">39.17(2.10)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.8.8.3\">27.02(3.13)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.8.8.4\">8.87(0.11)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.9.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.9.9.1\">FedRGD-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.9.9.2\">51.57(1.67)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.9.9.3\">87.00(1.31)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.9.9.4\">19.94(0.75)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.10.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.10.10.1\">SemiFL-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.10.10.2\">55.42(2.57)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.10.10.3\">87.61(0.91)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.3.1.10.10.4\">28.29(0.73)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.3.1.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.11.11.1\">FedDB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.11.11.2\">55.00(1.17)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.11.11.3\">85.99(0.49)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T2.3.1.11.11.4\">29.28(0.51)</td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T3",
            "type": "table",
            "title": "2405.19789v1_Table3",
            "caption": "Table 3:Experimental results in the Non-IID setting with\u03b4=0.1\ud835\udeff0.1\\delta=0.1italic_\u03b4 = 0.1.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T3.3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.1.1.1.1\">Dataset</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.1.1.1.2\">CIFAR10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.1.1.1.3\">SVHN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.1.1.1.4\">CIFAR100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.2.2.1\">FedAvg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.2.2.2\">33.53(1.9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.2.2.3\">32.21(1.52)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.2.2.4\">28.78(0.53)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.3.3.1\">FixMatch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.3.3.2\">35.14(1.53)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.3.3.3\">74.31(2.07)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.3.3.4\">25.90(1.06)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.4.1\">FedMatch</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.4.2\">31.12(2.69)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.4.3\">12.66(3.34)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.4.4.4\">7.50(0.99)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.5.1\">FedRGD</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.5.2\">35.33(3.73)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.5.3\">38.20(5.64)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.5.5.4\">18.04(1.59)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.6.1\">SemiFL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.6.2\">33.72(1.87)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.6.3\">72.76(6.19)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.6.6.4\">25.82(0.44)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.7.1\">FixMatch-FedDPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.7.2\">37.13(3.22)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.7.3\">76.29(1.00)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.3.1.7.7.4\">27.76(0.85)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.8.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.8.8.1\">FedMatch-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.8.8.2\">32.26(2.75)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.8.8.3\">16.94(1.28)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.8.8.4\">7.66(0.43)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.9.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.9.1\">FedRGD-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.9.2\">35.59(3.49)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.9.3\">38.76(2.67)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.9.9.4\">18.98(0.58)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.10.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.10.1\">SemiFL-FedDPL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.10.2\">37.84(2.33)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.10.3\">74.54(7.51)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.1.10.10.4\">27.62(1.00)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.1.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.11.11.1\">FedDB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.11.11.2\">37.95(2.21)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.11.11.3\">76.20(1.31)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.3.1.11.11.4\">27.99(1.28)</td>\n</tr>\n</tbody>\n</table>"
        },
        {
            "id": "S5.T4",
            "type": "table",
            "title": "2405.19789v1_Table4",
            "caption": "Table 4:Ablation studies on CIFAR10, SVHN, and CIFAR100.",
            "metadata": {},
            "table_html": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.2.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.3.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T4.2.2.3.1.1\" style=\"width:28.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.2.2.3.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T4.2.2.3.1.1.1.1\">IID</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.2.2.3.1.2\">DPL</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T4.2.2.3.1.3\">DMA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.2.2.3.1.4\">CIFAR10</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.2.2.3.1.5\">SVHN</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.2.2.3.1.6\">CIFAR100</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.4.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.4.1.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.4.1.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.4\">65.80(2.72)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.5\">87.44(1.35)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.4.1.6\">24.72(0.73)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.5.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.5.2.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.5.2.2\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.2.2.5.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.5.2.4\">66.97(2.84)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.5.2.5\">88.00(0.67)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.5.2.6\">26.44(1.73)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.6.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.6.3.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.6.3.2\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.2.2.6.3.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.6.3.4\">67.32(2.31)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.6.3.5\">86.75(0.90)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.6.3.6\">26.71(0.87)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.1.1\" style=\"width:28.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S5.T4.1.1.1.1.1.1\"><math alttext=\"\\delta=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.1.1.1.1.1.1.m1.1\"><semantics id=\"S5.T4.1.1.1.1.1.1.m1.1a\"><mrow id=\"S5.T4.1.1.1.1.1.1.m1.1.1\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T4.1.1.1.1.1.1.m1.1.1.2\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.2.cmml\">\u03b4</mi><mo id=\"S5.T4.1.1.1.1.1.1.m1.1.1.1\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T4.1.1.1.1.1.1.m1.1.1.3\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.1.1.1.m1.1b\"><apply id=\"S5.T4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1\"><eq id=\"S5.T4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T4.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.2\">\ud835\udeff</ci><cn id=\"S5.T4.1.1.1.1.1.1.m1.1.1.3.cmml\" type=\"float\" xref=\"S5.T4.1.1.1.1.1.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.1.1.1.m1.1c\">\\delta=0.3</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.1.1.1.1.1.1.m1.1d\">italic_\u03b4 = 0.3</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.1.2\">DPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.1.3\">DMA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.1.4\">CIFAR10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.1.5\">SVHN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.1.1.6\">CIFAR100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.7.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.7.4.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.7.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.7.4.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.7.4.4\">50.99(2.49)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.7.4.5\">86.61(0.19)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.7.4.6\">25.47(0.46)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.8.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.8.5.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.8.5.2\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.2.2.8.5.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.8.5.4\">53.92(3.41)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.8.5.5\">85.87(0.51)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.8.5.6\">28.47(0.13)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.9.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.9.6.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.9.6.2\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.2.2.9.6.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.9.6.4\">55.00(1.17)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.9.6.5\">85.99(0.49)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.9.6.6\">29.28(0.51)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.2.1\" style=\"width:28.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S5.T4.2.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S5.T4.2.2.2.1.1.1\"><math alttext=\"\\delta=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.2.2.2.1.1.1.m1.1\"><semantics id=\"S5.T4.2.2.2.1.1.1.m1.1a\"><mrow id=\"S5.T4.2.2.2.1.1.1.m1.1.1\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T4.2.2.2.1.1.1.m1.1.1.2\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.2.cmml\">\u03b4</mi><mo id=\"S5.T4.2.2.2.1.1.1.m1.1.1.1\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T4.2.2.2.1.1.1.m1.1.1.3\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.2.2.2.1.1.1.m1.1b\"><apply id=\"S5.T4.2.2.2.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1\"><eq id=\"S5.T4.2.2.2.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T4.2.2.2.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.2\">\ud835\udeff</ci><cn id=\"S5.T4.2.2.2.1.1.1.m1.1.1.3.cmml\" type=\"float\" xref=\"S5.T4.2.2.2.1.1.1.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.2.2.2.1.1.1.m1.1c\">\\delta=0.1</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.2.2.2.1.1.1.m1.1d\">italic_\u03b4 = 0.1</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.2\">DPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.2.3\">DMA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.4\">CIFAR10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.5\">SVHN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.2.6\">CIFAR100</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.10.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.10.7.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.10.7.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.10.7.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.10.7.4\">35.14(1.53)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.10.7.5\">74.31(2.07)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.2.2.10.7.6\">25.90(1.06)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.11.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r\" id=\"S5.T4.2.2.11.8.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.11.8.2\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T4.2.2.11.8.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.11.8.4\">37.13(3.22)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.11.8.5\">76.29(1.00)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.2.2.11.8.6\">27.76(0.85)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2.12.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r\" id=\"S5.T4.2.2.12.9.1\" style=\"width:28.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.12.9.2\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T4.2.2.12.9.3\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.12.9.4\">37.95(2.21)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.12.9.5\">76.20(1.31)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.2.2.12.9.6\">27.99(1.28)</td>\n</tr>\n</tbody>\n</table>"
        }
    ],
    "metadata": {},
    "pdf_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\pdfs\\2405.19789v1.pdf",
    "HTML_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\htmls\\2405.19789v1.html"
}