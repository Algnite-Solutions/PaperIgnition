{
    "doc_id": "2405.20542v1",
    "title": "On the Connection Between Non-negative Matrix Factorization and Latent Dirichlet Allocation",
    "authors": [
        "Benedikt Geiger",
        "Peter J. Park"
    ],
    "categories": [
        "cs.LG",
        "stat.ML",
        "62H30, 15A23, 62H22, 62F15, 68W40"
    ],
    "published_date": "2024-05-30 23:54:17+00:00",
    "abstract": "Non-negative matrix factorization with the generalized Kullback-Leibler\ndivergence (NMF) and latent Dirichlet allocation (LDA) are two popular\napproaches for dimensionality reduction of non-negative data. Here, we show\nthat NMF with $\\ell_1$ normalization constraints on the columns of both\nmatrices of the decomposition and a Dirichlet prior on the columns of one\nmatrix is equivalent to LDA. To show this, we demonstrate that explicitly\naccounting for the scaling ambiguity of NMF by adding $\\ell_1$ normalization\nconstraints to the optimization problem allows a joint update of both matrices\nin the widely used multiplicative updates (MU) algorithm. When both of the\nmatrices are normalized, the joint MU algorithm leads to probabilistic latent\nsemantic analysis (PLSA), which is LDA without a Dirichlet prior. Our approach\nof deriving joint updates for NMF also reveals that a Lasso penalty on one\nmatrix together with an $\\ell_1$ normalization constraint on the other matrix\nis insufficient to induce any sparsity.",
    "text_chunks": [
        {
            "id": "S1",
            "type": "text",
            "title": "1Introduction",
            "caption": "1Introduction",
            "metadata": {},
            "text": "\n1 Introduction\nNon-negative matrix factorization (NMF) and latent Dirichlet allocation (LDA) are two prominent techniques for unsupervised machine learning and data analysis. Both approaches decompose non-negative data into an interpretable parts-based representation and have been used successfully across a wide range of areas including topic modeling, image and audio signal processing, recommendation systems, and bioinformatics [24, 46, 17]. However, despite their conceptual similarities and widespread use, the relationship between NMF and LDA remains underexplored. Although partial results exist, prior work is written from a probabilistic perspective, and empirical comparisons often fall short to appreciate the depth of their connection.\n\nNon-negative matrix factorization X\u2248W\u2062H\ud835\udc4b\ud835\udc4a\ud835\udc3bX\\approx WHitalic_X \u2248 italic_W italic_H with an appropriately chosen loss function is commonly viewed as a constrained optimization problem, with linear algebra algorithms employed to solve it. In the case of the widely used Kullback\u2013Leibler (KL) divergence loss, the most popular approach for solving the optimization problem is the so-called multiplicative updates approach [17]. Originally derived by Richardson [39] and Lucy [31] in the context of deconvolution, it was rediscovered and applied to NMF in the seminal work of Lee and Seung [28, 27]. The algorithm alternates between updating each of the matrices W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H of the factorization in a multiplicative manner. Latent Dirichlet allocation was introduced as a generative statistical model of text data to learn the underlying topics, topic proportions of documents, and probabilistic topic assignments of individual words [4]. As a bag-of-words model, LDA assumes that the order of the words in each document does not play a role, and only the word-document count matrix X\ud835\udc4bXitalic_X of a corpus is used during training and inference. The generative process of LDA extends that of its predecessor probabilistic latent semantic analysis (PLSA) [23, 22] by adding a Dirichlet prior to the topic proportions. Thus, a solution of PLSA can also be interpreted as a maximum a posteriori estimate of LDA under a uniform Dirichlet prior [18].\n\nIn addition to these connections to LDA, PLSA is well-known to be closely related to NMF. For normalized inputs X\ud835\udc4bXitalic_X, fixed points of the PLSA algorithm can be mapped to fixed points of the MU algorithm of NMF with the KL divergence, and vice versa [16]. The identical result has also been shown for global maxima of the PLSA likelihood and global minima of the KL loss of NMF [12]. Noticing similarities between the update rules and underlying generative processes of NMF, PLSA, and LDA dates back to earlier work on unified frameworks for probabilistic matrix decomposition [5, 6]. Various probabilistic latent variable models with joint parameter updates closely resembling the update rules of NMF have also been studied in [41, 42, 47]. However, an important detail frequently overlooked in the literature is that PLSA jointly updates the topic and topic proportion matrices, whereas the MU algorithm of NMF sequentially updates one matrix given the other.\n\nIn this paper, we reveal a deeper connection between NMF and topic models like PLSA and LDA. We solve NMF with additional normalization constraints, and show that this leads to algorithms resembling or identical to the PLSA algorithm. Our main contributions are the following:\n\n\n1.\nFor NMF with the KL divergence, adding normalization constraints to the columns of one or both matrices of the decomposition makes it possible to derive joint update rules for both matrices. The obtained algorithms save one matrix multiplication per iteration.\n\n2.\nNMF with the KL divergence and additional normalization constraints on the columns of both matrices is equivalent to PLSA. Both the objective function and the canonical joint update rules are identical. In contrast to the existing literature, we establish an algorithmic equivalence, and our result neither requires normalizing the input matrix X\ud835\udc4bXitalic_X nor transforming the trained model parameters.\n\n3.\nWith a Dirichlet prior on the columns of the matrix corresponding to the topic proportions, NMF with the KL divergence and additional normalization constraints on the columns of both matrices is equivalent to LDA. Both the objective function and the canonical variational inference algorithm are identical.\n\n4.\nJoint update equations can be derived to speed up and better understand variations of NMF. We demonstrate this by revisiting sparse NMF with the KL divergence [30].\n\n\nJoint update rules for NMF have recently been introduced in the more general case of \u03b2\ud835\udefd\\betaitalic_\u03b2-divergence loss functions [33]. Both in [33] and in our work, the results are based on a majorization-minimization scheme in which an upper bound of the objective function is constructed and minimized in every iteration [43]. The approach differs from previous work by jointly considering both matrices of the decomposition instead of constructing an upper bound for one of the two matrices at a time [15, 36]. In contrast to [33], however, the additional normalizations allow us to derive a closed-form expression for the optimum of the upper bound.\n\nThat NMF with Dirichlet priors on the columns of both matrices of the decomposition leads to LDA with Dirichlet smoothing on the topics was briefly mentioned in [49, 48] and suggested in [9, Section 3.5]. By revisiting the connection between PLSA and NMF, our perspective emphasizes the role of the normalization constraints rather than the Dirichlet distribution giving rise to the connections between generative models and algorithms.\n"
        },
        {
            "id": "S2",
            "type": "text",
            "title": "2Related work",
            "caption": "2Related work",
            "metadata": {},
            "text": "\n2 Related work\nBayesian NMF.\nNon-negative matrix factorization can often be interpreted in a statistical framework [14]. For example, minimizing the KL divergence between the data matrix X\ud835\udc4bXitalic_X and its reconstruction W\u2062H\ud835\udc4a\ud835\udc3bWHitalic_W italic_H is equivalent to maximizing the likelihood of W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H in an element-wise Poisson noise model X\u223cPoisson\u2061(W\u2062H)similar-to\ud835\udc4bPoisson\ud835\udc4a\ud835\udc3bX\\sim\\operatorname{Poisson}(WH)italic_X \u223c roman_Poisson ( italic_W italic_H ). Similar connections between the loss function and the likelihood of an appropriately chosen generative model also exist for alternative reconstruction errors [44]. In other words, the probabilistic point of view on NMF is based on the underlying statistical assumptions of the chosen reconstruction error. Through priors on W\ud835\udc4aWitalic_W or H\ud835\udc3bHitalic_H, this statistical framework allows one to induce desired properties of the factorization. Bayesian extensions of NMF have, for instance, been applied to control the sparsity or smoothness of the parameters [8, 45, 13], automatically select the factorization rank [44, 21], obtain uncertainty estimates [40], or model bounded support data [32].\n\nIn the case of NMF with the KL divergence, the most commonly used prior is the Gamma distribution [7, 45, 8, 14, 11, 35, 37]. The main reason for this choice is the analytical convenience resulting from the Gamma distribution being the conjugate prior of the Poisson distribution. The obtained Gamma\u2013Poisson model with fixed prior parameters has been shown to be related to LDA: When W\ud835\udc4aWitalic_W is column-normalized and Gamma priors with identical rate parameters are placed on the columns of H\ud835\udc3bHitalic_H, the generative model extends LDA by also modeling the document length [6, Lemma 1]. In fact, we strengthen this connection and prove that the canonical variational inference algorithm of this Gamma\u2013Poisson model is identical to the LDA algorithm from [4] (see Appendix\u00a0E). This algorithmic equivalence can be understood as the Bayesian counterpart to the connection between NMF with a normalization constraint on W\ud835\udc4aWitalic_W and PLSA.\n\n"
        },
        {
            "id": "S3",
            "type": "text",
            "title": "3Preliminaries",
            "caption": "3Preliminaries",
            "metadata": {},
            "text": "\n3 Preliminaries\nThroughout the paper, our terminology draws inspiration from the topic modeling literature. That is, when considering the non-negative matrix decomposition X\u2248W\u2062H\ud835\udc4b\ud835\udc4a\ud835\udc3bX\\approx WHitalic_X \u2248 italic_W italic_H, we use the terms (unnormalized) topic matrix to refer to W\ud835\udc4aWitalic_W and topic weights to refer to H\ud835\udc3bHitalic_H. When additional normalization constraints are in place, we use (normalized) topic matrix and topic proportions instead. We use the notation \u211d+subscript\u211d\\mathbb{R}_{+}blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT to refer to the non-negative real numbers, and for any positive integer M\ud835\udc40Mitalic_M, we denote the M\u22121\ud835\udc401M-1italic_M - 1 dimensional standard simplex by \u0394M\u22121={y\u2208\u211d+M|\u2211mym=1}subscript\u0394\ud835\udc401conditional-set\ud835\udc66superscriptsubscript\u211d\ud835\udc40subscript\ud835\udc5asubscript\ud835\udc66\ud835\udc5a1\\Delta_{M-1}=\\{y\\in\\mathbb{R}_{+}^{M}\\,|\\,\\sum_{m}y_{m}=1\\}roman_\u0394 start_POSTSUBSCRIPT italic_M - 1 end_POSTSUBSCRIPT = { italic_y \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT | \u2211 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 1 }. When using the abbreviation NMF, we refer to non-negative matrix factorization with the KL divergence unless noted otherwise.\n\nIn the context of topic modelling, the goal of NMF, PLSA, and LDA is to identify the underlying topics in a corpus of documents and to represent each document as an additive combination of the learned topics. All three methods are bag-of-words models\u2014they assume that learning the decomposition does not require taking the order of words in the documents into account. Therefore, the input of the algorithms is the term-document matrix X\u2208\u211d+V\u00d7D\ud835\udc4bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}^{V\\times D}_{+}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT, where each entry xv\u2062dsubscript\ud835\udc65\ud835\udc63\ud835\udc51x_{vd}italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT is the number of times the term v\ud835\udc63vitalic_v occurs in document d\ud835\udc51ditalic_d. The number of topics K\ud835\udc3eKitalic_K is a hyperparameter of all three approaches.\n\n3.1 Non-negative matrix factorization (NMF)\nNMF decomposes the term-document matrix X\ud835\udc4bXitalic_X into two non-negative matrices W\u2208\u211d+V\u00d7K\ud835\udc4asuperscriptsubscript\u211d\ud835\udc49\ud835\udc3eW\\in\\mathbb{R}_{+}^{V\\times K}italic_W \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT and H\u2208\u211d+K\u00d7D\ud835\udc3bsuperscriptsubscript\u211d\ud835\udc3e\ud835\udc37H\\in\\mathbb{R}_{+}^{K\\times D}italic_H \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT. The columns of W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H can then be interpreted as the unnormalized topics and topic weights of the documents, respectively, and these parameters are learned by minimizing an appropriate \u201cdistance\u201c D(X||WH)D(X\\,||\\,WH)italic_D ( italic_X | | italic_W italic_H ) between the data matrix and its reconstruction. A common choice is the (generalized) Kullback\u2013Leibler divergence\n\nDKL(X||WH)=\u2211v,dxv\u2062dlogxv\u2062d(W\u2062H)v\u2062d\u2212xv\u2062d+(WH)v\u2062d,\\displaystyle D_{\\text{KL}}(X\\,||\\,WH)=\\sum_{v,d}x_{vd}\\log\\frac{x_{vd}}{(WH)_%\n{vd}}-x_{vd}+(WH)_{vd},italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) = \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT + ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ,\n(1)\n\nwhich leads to the optimization problem\n\nminW\u2208\u211d+V\u00d7K,H\u2208\u211d+K\u00d7DDKL(X||WH).\\displaystyle\\min_{W\\in\\mathbb{R}_{+}^{V\\times K},\\,H\\in\\mathbb{R}_{+}^{K%\n\\times D}}D_{\\text{KL}}(X\\,||\\,WH).roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT , italic_H \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) .\n(2)\n\nAlgorithms to solve (2) typically alternate between updating each of the matrices W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H while fixing the other one [19]. The motivation for this approach is two-fold [17]. First, the factorization X\u2248W\u2062H\ud835\udc4b\ud835\udc4a\ud835\udc3bX\\approx WHitalic_X \u2248 italic_W italic_H is symmetric to XT\u2248HT\u2062WTsuperscript\ud835\udc4b\ud835\udc47superscript\ud835\udc3b\ud835\udc47superscript\ud835\udc4a\ud835\udc47X^{T}\\approx H^{T}W^{T}italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2248 italic_H start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_W start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. An update rule for one of the matrices thus immediately yields an update rule for the other. Second, the optimization problems induced by fixing each of the two matrices in (2) are convex. The most common algorithm consists of multiplicatively updating each matrix iteratively until convergence (see Algorithm\u00a01). It was popularized by Lee and Seung, who demonstrated the potential of learning parts-based representations and proved the monotonic convergence of the objective function [27, 28, 46]. The key idea of their derivation is to construct a well-behaved function G\u2062(H,H\u2032)\ud835\udc3a\ud835\udc3bsuperscript\ud835\udc3b\u2032G(H,H^{\\prime})italic_G ( italic_H , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) for F(H)=DKL(X||WH)F(H)=D_{\\text{KL}}(X\\,||\\,WH)italic_F ( italic_H ) = italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) such that\n\nG\u2062(H,H\u2032)\u2265F\u2062(H)for all\u00a0\u2062H,H\u2032,G\u2062(H\u2032,H\u2032)=F\u2062(H\u2032)for all\u00a0\u2062H\u2032.formulae-sequence\ud835\udc3a\ud835\udc3bsuperscript\ud835\udc3b\u2032\ud835\udc39\ud835\udc3bfor all\u00a0\ud835\udc3bsuperscript\ud835\udc3b\u2032\ud835\udc3asuperscript\ud835\udc3b\u2032superscript\ud835\udc3b\u2032\ud835\udc39superscript\ud835\udc3b\u2032for all\u00a0superscript\ud835\udc3b\u2032\\begin{split}G(H,H^{\\prime})&\\geq F(H)\\quad\\text{for all }H,\\,H^{\\prime},\\\\\nG(H^{\\prime},H^{\\prime})&=F(H^{\\prime})\\quad\\text{for all }H^{\\prime}.\\end{split}start_ROW start_CELL italic_G ( italic_H , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) end_CELL start_CELL \u2265 italic_F ( italic_H ) for all italic_H , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_G ( italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) end_CELL start_CELL = italic_F ( italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) for all italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT . end_CELL end_ROW\n(3)\n\nOnce such an auxiliary function G\ud835\udc3aGitalic_G is found, the objective function F\ud835\udc39Fitalic_F can be shown to be non-increasing by updating an iterate H(n)superscript\ud835\udc3b\ud835\udc5bH^{(n)}italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT to the argument of the minimum of H\u21a6G\u2062(H,H(n))maps-to\ud835\udc3b\ud835\udc3a\ud835\udc3bsuperscript\ud835\udc3b\ud835\udc5bH\\mapsto G(H,H^{(n)})italic_H \u21a6 italic_G ( italic_H , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) (see Appendix\u00a0A). While the perspective on (1) and the construction of the auxiliary function in [28] via Jensen\u2019s inequality is entirely non-probabilistic, there is also a probabilistic framework for Algorithm\u00a01 [8, 14]. Assuming conditional independence of the counts Xv\u2062dsubscript\ud835\udc4b\ud835\udc63\ud835\udc51X_{vd}italic_X start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT, the model Xv\u2062d\u223cPoisson(WH)v\u2062dX_{vd}\\sim\\operatorname{Poisson}(WH)_{vd}italic_X start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT leads to a log-likelihood identical to the negative KL divergence (1) (up to a constant). By augmenting the generative process to a latent variable model via\n\nzv\u2062k\u2062d\u223cPoisson\u2061(wv\u2062k\u2062hk\u2062d),Xv\u2062d=\u2211kzv\u2062k\u2062d,formulae-sequencesimilar-tosubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51Poissonsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc4b\ud835\udc63\ud835\udc51subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51z_{vkd}\\sim\\operatorname{Poisson}(w_{vk}h_{kd}),\\quad X_{vd}=\\sum_{k}z_{vkd},italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) , italic_X start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ,\n(4)\n\na standard expectation-maximization (EM) [10] procedure yields a joint auxiliary function G\ud835\udc3aGitalic_G for the model parameters (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ), i.e.,\n\nG\u2062((W,H),(W\u2032,H\u2032))\u2265DKL(X||WH)for all\u00a0(W,H),(W\u2032,H\u2032),G\u2062((W\u2032,H\u2032),(W\u2032,H\u2032))=DKL(X||W\u2032H\u2032)for all\u00a0(W\u2032,H\u2032).\\begin{split}G((W,H),(W^{\\prime},H^{\\prime}))&\\geq D_{\\text{KL}}(X\\,||\\,WH)%\n\\quad\\text{for all }(W,H),\\,(W^{\\prime},H^{\\prime}),\\\\\nG((W^{\\prime},H^{\\prime}),(W^{\\prime},H^{\\prime}))&=D_{\\text{KL}}(X\\,||\\,W^{%\n\\prime}H^{\\prime})\\quad\\text{for all }(W^{\\prime},H^{\\prime}).\\end{split}start_ROW start_CELL italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) ) end_CELL start_CELL \u2265 italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) for all ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) , end_CELL end_ROW start_ROW start_CELL italic_G ( ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) , ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) ) end_CELL start_CELL = italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) for all ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) . end_CELL end_ROW\n(5)\n\nWith\n\n\u03d5v\u2062k\u2062d(n)\u2254wv\u2062k(n)\u2062hk\u2062d(n)(W(n)\u2062H(n))v\u2062d\u2254superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsuperscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58\ud835\udc5bsuperscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5bsubscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51\\phi_{vkd}^{(n)}\\coloneqq\\frac{w_{vk}^{(n)}h_{kd}^{(n)}}{(W^{(n)}H^{(n)})_{vd}}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT \u2254 divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG\n(6)\n\nand modulo terms independent of the model parameters, the auxiliary function is given by\n\nG\u2062((W,H),(W(n),H(n)))=\u2212\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d(n)+\u2211v,d(W\u2062H)v\u2062d,\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5bsubscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc63\ud835\udc51subscript\ud835\udc4a\ud835\udc3b\ud835\udc63\ud835\udc51G((W,H),(W^{(n)},H^{(n)}))=-\\sum_{v,k,d}x_{vd}\\phi_{vkd}^{(n)}\\log\\frac{w_{vk}%\nh_{kd}}{\\phi_{vkd}^{(n)}}+\\sum_{v,d}(WH)_{vd},italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) ) = - \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG + \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ,\n(7)\n\ncf.\u00a0[8, eq.\u00a0(18)]. Although a closed-form joint optimum does not exist, G\ud835\udc3aGitalic_G induces auxiliary functions for both individual matrices via\n\nG~\u2062(W,W(n))=G\u2062((W,H(n)),(W(n),H(n))),G~\u2062(H,H(n))=G\u2062((W(n),H),(W(n),H(n))).formulae-sequence~\ud835\udc3a\ud835\udc4asuperscript\ud835\udc4a\ud835\udc5b\ud835\udc3a\ud835\udc4asuperscript\ud835\udc3b\ud835\udc5bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b~\ud835\udc3a\ud835\udc3bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc3asuperscript\ud835\udc4a\ud835\udc5b\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\\begin{split}\\widetilde{G}(W,W^{(n)})&=G((W,H^{(n)}),(W^{(n)},H^{(n)})),\\\\\n\\widetilde{G}(H,H^{(n)})&=G((W^{(n)},H),(W^{(n)},H^{(n)})).\\end{split}start_ROW start_CELL over~ start_ARG italic_G end_ARG ( italic_W , italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) end_CELL start_CELL = italic_G ( ( italic_W , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) ) , end_CELL end_ROW start_ROW start_CELL over~ start_ARG italic_G end_ARG ( italic_H , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) end_CELL start_CELL = italic_G ( ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) ) . end_CELL end_ROW\n(8)\n\nAlternating between optimizing each of the induced auxiliary functions recovers the multiplicative updates from Lee and Seung, revealing that Algorithm\u00a01 is in fact a generalized EM algorithm.\n\nIn practice, it is often desirable to increase the interpretability of the matrices and to eliminate the scaling ambiguity of the factorization caused by W\u2062H=W\u2062D\u2062D\u22121\u2062H\ud835\udc4a\ud835\udc3b\ud835\udc4a\ud835\udc37superscript\ud835\udc371\ud835\udc3bWH=WDD^{-1}Hitalic_W italic_H = italic_W italic_D italic_D start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_H for any invertible diagonal matrix D\u2208\u211d+K\u00d7K\ud835\udc37subscriptsuperscript\u211d\ud835\udc3e\ud835\udc3eD\\in\\mathbb{R}^{K\\times K}_{+}italic_D \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT. A common strategy to achieve both is to normalize the columns of W\ud835\udc4aWitalic_W to probability distributions and to scale the rows of H\ud835\udc3bHitalic_H accordingly after each iteration or after convergence.\n\n\n3.2 Probabilistic latent semantic analysis (PLSA)\nIn PLSA [23], the generative process of the term-document matrix is unfolded in the sense that the words are generated individually. The scheme for each of the n=1,\u2026,Nd\ud835\udc5b1\u2026subscript\ud835\udc41\ud835\udc51n=1,\\ldots,N_{d}italic_n = 1 , \u2026 , italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT words in document d\ud835\udc51ditalic_d is the following:\n\n1.\nSample a latent topic zn\u2062dsubscript\ud835\udc67\ud835\udc5b\ud835\udc51z_{nd}italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT with probability p\u2062(zn\u2062d=k)=hk\u2062d\ud835\udc5dsubscript\ud835\udc67\ud835\udc5b\ud835\udc51\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51p(z_{nd}=k)=h_{kd}italic_p ( italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_k ) = italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT.\n\n2.\nSample a word \u03bdn\u2062dsubscript\ud835\udf08\ud835\udc5b\ud835\udc51\\nu_{nd}italic_\u03bd start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT with probability p\u2062(\u03bdn\u2062d=v|zn\u2062d=k)=wv\u2062k\ud835\udc5dsubscript\ud835\udf08\ud835\udc5b\ud835\udc51conditional\ud835\udc63subscript\ud835\udc67\ud835\udc5b\ud835\udc51\ud835\udc58subscript\ud835\udc64\ud835\udc63\ud835\udc58p(\\nu_{nd}=v|z_{nd}=k)=w_{vk}italic_p ( italic_\u03bd start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_v | italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_k ) = italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT.\n\nThe resulting log-likelihood is given by\n\nlogp(X|W,H)=\u2211v,dxv\u2062dlog(WH)v\u2062d+c(X),\\log p(X|W,H)=\\sum_{v,d}x_{vd}\\log(WH)_{vd}+c(X),roman_log italic_p ( italic_X | italic_W , italic_H ) = \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT + italic_c ( italic_X ) ,\n(9)\n\nwhere the constant c\u2062(X)\ud835\udc50\ud835\udc4bc(X)italic_c ( italic_X ) arises from bagging the sequence of words, cf.\u00a0[6, Section III.A]. In the following, we will always ignore this constant because it only depends on the observed counts and does not affect model training. The relevant part of the log-likelihood (9) therefore reduces to\n\n\u2112PLSA(X|W,H)\u2254\u2211v,dxv\u2062dlog(WH)v\u2062d.\\mathcal{L}^{\\text{PLSA}}(X|W,H)\\coloneqq\\sum_{v,d}x_{vd}\\log(WH)_{vd}.caligraphic_L start_POSTSUPERSCRIPT PLSA end_POSTSUPERSCRIPT ( italic_X | italic_W , italic_H ) \u2254 \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT .\n(10)\n\nThe algorithm to fit the model parameters is an EM algorithm, described in [23]. With the conditional distributions of the latent topic assignments for model parameters (W(n),H(n))superscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b(W^{(n)},H^{(n)})( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) equal to\n\np\u2062(zn\u2062d=k|\u03bdn\u2062d=v)(n)=\u03d5v\u2062k\u2062d(n)\u221dwv\u2062k(n)\u2062hk\u2062d(n),\ud835\udc5dsuperscriptsubscript\ud835\udc67\ud835\udc5b\ud835\udc51conditional\ud835\udc58subscript\ud835\udf08\ud835\udc5b\ud835\udc51\ud835\udc63\ud835\udc5bsuperscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bproportional-tosuperscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58\ud835\udc5bsuperscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5b\\displaystyle p(z_{nd}=k|\\nu_{nd}=v)^{(n)}=\\phi_{vkd}^{(n)}\\propto w_{vk}^{(n)%\n}h_{kd}^{(n)},italic_p ( italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_k | italic_\u03bd start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_v ) start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT = italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT \u221d italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ,\n(11)\n\nthe E-step of the derivation naturally leads to an auxiliary function of the negative objective function (10) given by\n\nG\u2062((W,H),(W(n),H(n)))=\u2212\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d(n).\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5bsubscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bG((W,H),(W^{(n)},H^{(n)}))=-\\sum_{v,k,d}x_{vd}\\phi_{vkd}^{(n)}\\log\\frac{w_{vk}%\nh_{kd}}{\\phi_{vkd}^{(n)}}.italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) ) = - \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG .\n(12)\n\nThe minimization of G\ud835\udc3aGitalic_G during the M-step yields Algorithm\u00a02. While the algorithm is remarkably similar to Algorithm\u00a01 and the only difference seems to be the normalizations, it is important to note that all model parameters are updated jointly in PLSA. The conditional distributions of the topic assignments \u03d5v\u2062k\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\phi_{vkd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT are not recomputed after updating one of the matrices.\n\n\n3.3 Latent Dirichlet allocation (LDA)\nLDA was developed to overcome the issue that the number of model parameters of PLSA grows linearly with the number of documents on which the model is trained [4, 3]. To this end, LDA introduces a model parameter \u03b1\u2208\u211d>0K\ud835\udefcsubscriptsuperscript\u211d\ud835\udc3eabsent0\\alpha\\in\\mathbb{R}^{K}_{>0}italic_\u03b1 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT and a Dirichlet prior h\u223cDir\u2061(\u03b1)similar-to\u210eDir\ud835\udefch\\sim\\operatorname{Dir}(\\alpha)italic_h \u223c roman_Dir ( italic_\u03b1 ) that turn the topic proportions into latent variables. The generative process for each document d\ud835\udc51ditalic_d then becomes\n\n1.\nSample the topic proportions hd\u223cDir\u2061(\u03b1).similar-tosubscript\u210e\ud835\udc51Dir\ud835\udefch_{d}\\sim\\operatorname{Dir}(\\alpha).italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT \u223c roman_Dir ( italic_\u03b1 ) .\n\n2.\nFor each of the n=1,\u2026,Nd\ud835\udc5b1\u2026subscript\ud835\udc41\ud835\udc51n=1,\\ldots,N_{d}italic_n = 1 , \u2026 , italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT words in document d\ud835\udc51ditalic_d:\n\n(a)\nSample a latent topic zn\u2062dsubscript\ud835\udc67\ud835\udc5b\ud835\udc51z_{nd}italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT with probability p\u2062(zn\u2062d=k)=hk\u2062d\ud835\udc5dsubscript\ud835\udc67\ud835\udc5b\ud835\udc51\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51p(z_{nd}=k)=h_{kd}italic_p ( italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_k ) = italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT,\n\n(b)\nSample a word \u03bdn\u2062dsubscript\ud835\udf08\ud835\udc5b\ud835\udc51\\nu_{nd}italic_\u03bd start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT with probability p\u2062(\u03bdn\u2062d=v|zn\u2062d=k)=wv\u2062k\ud835\udc5dsubscript\ud835\udf08\ud835\udc5b\ud835\udc51conditional\ud835\udc63subscript\ud835\udc67\ud835\udc5b\ud835\udc51\ud835\udc58subscript\ud835\udc64\ud835\udc63\ud835\udc58p(\\nu_{nd}=v|z_{nd}=k)=w_{vk}italic_p ( italic_\u03bd start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_v | italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT = italic_k ) = italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT.\n\n\n\nThis results in the log-likelihood\n\nlog\u2061p\u2062(xd|W,\u03b1)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefc\\displaystyle\\log p(x_{d}|W,\\alpha)roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 )\n=log\u2062\u222bp\u2062(h|\u03b1)\u2062\u220fv(W\u2062h)vxv\u2062d\u2062d\u2062h,absent\ud835\udc5dconditional\u210e\ud835\udefcsubscriptproduct\ud835\udc63superscriptsubscript\ud835\udc4a\u210e\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\ud835\udc51\u210e\\displaystyle=\\log\\int p(h|\\alpha)\\prod_{v}(Wh)_{v}^{x_{vd}}\\,dh,= roman_log \u222b italic_p ( italic_h | italic_\u03b1 ) \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_d italic_h ,\n(13)\n\nwhere p\u2062(h|\u03b1)\ud835\udc5dconditional\u210e\ud835\udefcp(h|\\alpha)italic_p ( italic_h | italic_\u03b1 ) denotes the density of the Dirichlet distribution. The introduction of the Dirichlet prior causes the posterior distribution of the latent variables (hd,zd)subscript\u210e\ud835\udc51subscript\ud835\udc67\ud835\udc51(h_{d},z_{d})( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) to be intractable. Blei et al.\u00a0therefore resort to variational inference (VI), which is one of the approximate inference algorithms applicable to the LDA model [25, 4, 1]. Following their mean-field approach, the true posterior is approximated with a variational distribution q\ud835\udc5eqitalic_q given by\n\nq\u2062(h,z|\u03b2,\u03d5)=\u220fdqd\u2062(hd,zd|\u03b2d,\u03d5d),qd\u2062(hd,zd|\u03b2d,\u03d5d)=q\u2062(hd|\u03b2d)\u2062\u220fnq\u2062(zn\u2062d|\u03d5n\u2062d),formulae-sequence\ud835\udc5e\u210econditional\ud835\udc67\ud835\udefditalic-\u03d5subscriptproduct\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\ud835\udc5econditionalsubscript\u210e\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptproduct\ud835\udc5b\ud835\udc5econditionalsubscript\ud835\udc67\ud835\udc5b\ud835\udc51subscriptitalic-\u03d5\ud835\udc5b\ud835\udc51\\displaystyle q(h,z|\\beta,\\phi)=\\prod_{d}q_{d}(h_{d},z_{d}|\\beta_{d},\\phi_{d})%\n,\\qquad q_{d}(h_{d},z_{d}|\\beta_{d},\\phi_{d})=q(h_{d}|\\beta_{d})\\prod_{n}q(z_{%\nnd}|\\phi_{nd}),italic_q ( italic_h , italic_z | italic_\u03b2 , italic_\u03d5 ) = \u220f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) \u220f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT | italic_\u03d5 start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT ) ,\n(14)\n\nwhere \u03b2dsubscript\ud835\udefd\ud835\udc51\\beta_{d}italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and \u03d5n\u2062dsubscriptitalic-\u03d5\ud835\udc5b\ud835\udc51\\phi_{nd}italic_\u03d5 start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT are parameters of a Dirichlet and categorical distribution, respectively. In addition to the model parameters W\ud835\udc4aWitalic_W and \u03b1\ud835\udefc\\alphaitalic_\u03b1, these so-called variational parameters are iteratively optimized by the algorithm. Instead of maximizing the exact log-likelihood, the idea of variational inference is to optimize a lower bound given by\n\n\u2112LDA\u2062(X|W,\u03b1,\u03b2,\u03d5)=\ud835\udd3cq\u2062[log\u2061p\u2062(X,h,z|W,\u03b1)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(h,z|\u03b2,\u03d5)].superscript\u2112LDAconditional\ud835\udc4b\ud835\udc4a\ud835\udefc\ud835\udefditalic-\u03d5subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5d\ud835\udc4b\u210econditional\ud835\udc67\ud835\udc4a\ud835\udefcsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5e\u210econditional\ud835\udc67\ud835\udefditalic-\u03d5\\mathcal{L}^{\\text{LDA}}(X|W,\\alpha,\\beta,\\phi)=\\mathbb{E}_{q}\\!\\left[\\log p(X%\n,h,z|W,\\alpha)\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(h,z|\\beta,\\phi)\\right].caligraphic_L start_POSTSUPERSCRIPT LDA end_POSTSUPERSCRIPT ( italic_X | italic_W , italic_\u03b1 , italic_\u03b2 , italic_\u03d5 ) = blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_X , italic_h , italic_z | italic_W , italic_\u03b1 ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h , italic_z | italic_\u03b2 , italic_\u03d5 ) ] .\n(15)\n\nHere and in the following, \ud835\udd3cq\u2062[\u22c5]subscript\ud835\udd3c\ud835\udc5edelimited-[]\u22c5\\mathbb{E}_{q}\\!\\left[\\cdot\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ \u22c5 ] denotes the expectation with respect to the variational distribution. Similar to PLSA, the key insight that reveals algebraic connections to NMF is that the optimal variational approximation of the posterior of the topic assignments zn\u2062dsubscript\ud835\udc67\ud835\udc5b\ud835\udc51z_{nd}italic_z start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT does not depend on the position n\ud835\udc5bnitalic_n, but only on the word \u03bdn\u2062dsubscript\ud835\udf08\ud835\udc5b\ud835\udc51\\nu_{nd}italic_\u03bd start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT at position n\ud835\udc5bnitalic_n, i.e., \u03d5n\u2062dsubscriptitalic-\u03d5\ud835\udc5b\ud835\udc51\\phi_{nd}italic_\u03d5 start_POSTSUBSCRIPT italic_n italic_d end_POSTSUBSCRIPT can be reindexed. Together with\n\nh~k\u2062d\u2062(\u03b2d)\u2254exp\u2061(\ud835\udd3cq\u2062(hd|\u03b2d)\u2062[log\u2061hk\u2062d]),\u2254subscript~\u210e\ud835\udc58\ud835\udc51subscript\ud835\udefd\ud835\udc51subscript\ud835\udd3c\ud835\udc5econditionalsubscript\u210e\ud835\udc51subscript\ud835\udefd\ud835\udc51delimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\widetilde{h}_{kd}(\\beta_{d})\\coloneqq\\exp\\big{(}\\mathbb{E}_{q(h_{d}|\\beta_{d}%\n)}[\\log h_{kd}]\\big{)},over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ( italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) \u2254 roman_exp ( blackboard_E start_POSTSUBSCRIPT italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] ) ,\n(16)\n\nwhich can be computed explicitly using the digamma function, the variational lower bound (15) reduces to\n\n\u2112LDA\u2062(X|W,\u03b1,\u03b2,\u03d5)=\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d\u2062log\u2061wv\u2062k\u2062h~k\u2062d\u03d5v\u2062k\u2062d+\u2211d(log\u2061\u0393\u2062(\u2211k\u03b1k)\u2212log\u2061\u0393\u2062(\u2211k\u03b2k\u2062d))+\u2211k,d(log\u2061\u0393\u2062(\u03b2k\u2062d)\u2212log\u2061\u0393\u2062(\u03b1k)+(\u03b1k\u2212\u03b2k\u2062d)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d]),superscript\u2112LDAconditional\ud835\udc4b\ud835\udc4a\ud835\udefc\ud835\udefditalic-\u03d5subscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc51\u0393subscript\ud835\udc58subscript\ud835\udefc\ud835\udc58\u0393subscript\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefd\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udefc\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle\\begin{split}\\mathcal{L}^{\\text{LDA}}(X|W,\\alpha,\\beta,\\phi)&=%\n\\sum_{v,k,d}x_{vd}\\phi_{vkd}\\log\\frac{w_{vk}\\widetilde{h}_{kd}}{\\phi_{vkd}}+%\n\\sum_{d}\\Big{(}\\log\\Gamma(\\textstyle{\\sum}_{k}\\alpha_{k})-\\log\\Gamma(%\n\\textstyle{\\sum}_{k}\\beta_{kd})\\Big{)}\\\\\n&\\phantom{=}+\\displaystyle\\sum_{k,d}\\Big{(}\\log\\Gamma(\\beta_{kd})-\\log\\Gamma(%\n\\alpha_{k})+(\\alpha_{k}-\\beta_{kd})\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]%\n\\Big{)},\\end{split}start_ROW start_CELL caligraphic_L start_POSTSUPERSCRIPT LDA end_POSTSUPERSCRIPT ( italic_X | italic_W , italic_\u03b1 , italic_\u03b2 , italic_\u03d5 ) end_CELL start_CELL = \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG + \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( roman_log roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - roman_log roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL + \u2211 start_POSTSUBSCRIPT italic_k , italic_d end_POSTSUBSCRIPT ( roman_log roman_\u0393 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - roman_log roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] ) , end_CELL end_ROW\n(17)\n\ncf.\u00a0[4, A.1, eq.\u00a0(15), (16)]. The VI algorithm is a variational EM procedure. It alternates between optimizing the approximation of the posterior distribution of the latent variables for fixed model parameters (variational E-step) and optimizing the model parameters for a fixed variational distribution (variational M-step). That is, (17) is optimized with respect to (\u03b2,\u03d5)\ud835\udefditalic-\u03d5(\\beta,\\phi)( italic_\u03b2 , italic_\u03d5 ) during the variational E-step and with respect to (W,\u03b1)\ud835\udc4a\ud835\udefc(W,\\alpha)( italic_W , italic_\u03b1 ) during the variational M-step. Since there is no closed-form optimum for the parameter of the Dirichlet prior \u03b1\ud835\udefc\\alphaitalic_\u03b1, we assume that it is fixed for simplicity. This procedure results in Algorithm\u00a03.\n\n"
        },
        {
            "id": "S4",
            "type": "text",
            "title": "4Algorithms for NMF with normalization constraints",
            "caption": "4Algorithms for NMF with normalization constraints",
            "metadata": {},
            "text": "\n4 Algorithms for NMF with normalization constraints\nWe now consider the NMF optimization problem (2) with additional normalization constraints on the topics or both the topics and the topic weights. We describe the connections between the original formulation and the sum-constrained optimization problems, and derive NMF algorithms with joint update rules. These new NMF algorithms are based on the observation that a constrained optimization of the joint auxiliary function (7) is possible.\n\nMore precisely, the two optimization problems we consider are NMF with normalized topics\n\nminW\u2208\u0394V\u22121K,H\u2208\u211d+K\u00d7DDKL(X||WH),\\displaystyle\\min_{W\\in\\Delta_{V-1}^{K},\\,H\\in\\mathbb{R}_{+}^{K\\times D}}D_{%\n\\text{KL}}(X||WH),roman_min start_POSTSUBSCRIPT italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , italic_H \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) ,\n(18)\n\nas well as NMF with both normalized topics and topic proportions\n\nminW\u2208\u0394V\u22121K,H\u2208\u0394K\u22121DDKL(X||WH).\\displaystyle\\min_{W\\in\\Delta_{V-1}^{K},\\,H\\in\\Delta_{K-1}^{D}}D_{\\text{KL}}(X%\n||WH).roman_min start_POSTSUBSCRIPT italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , italic_H \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) .\n(19)\n\nAt first glance, it might seem unnatural to normalize both matrices W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H without adjusting the input matrix X\ud835\udc4bXitalic_X. However, with both constraints, we have \u2211v(W\u2062H)v\u2062d=1subscript\ud835\udc63subscript\ud835\udc4a\ud835\udc3b\ud835\udc63\ud835\udc511\\sum_{v}(WH)_{vd}=1\u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = 1, and the objective function of (19) becomes\n\n\u2212\u2211v,dxv\u2062dlog(WH)v\u2062d=\u2212\u2211d(\u2211v\u2032xv\u2032\u2062d)\u2211vxv\u2062d\u2211v\u2032xv\u2032\u2062dlog(Whd)v.-\\sum_{v,d}x_{vd}\\log(WH)_{vd}=-\\sum_{d}\\Big{(}\\sum_{v^{\\prime}}x_{v^{\\prime}d%\n}\\Big{)}\\sum_{v}\\frac{x_{vd}}{\\sum_{v^{\\prime}}x_{v^{\\prime}d}}\\log(Wh_{d})_{v}.- \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = - \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( \u2211 start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_d end_POSTSUBSCRIPT ) \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT divide start_ARG italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_d end_POSTSUBSCRIPT end_ARG roman_log ( italic_W italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT .\n(20)\n\nThe inner sum corresponds to the Kullback\u2013Leibler divergence between the empirical distribution induced by the observed counts (xv\u2062d)vsubscriptsubscript\ud835\udc65\ud835\udc63\ud835\udc51\ud835\udc63(x_{vd})_{v}( italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and the model distribution (W\u2062hd)vsubscript\ud835\udc4asubscript\u210e\ud835\udc51\ud835\udc63(Wh_{d})_{v}( italic_W italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT [22, eq.\u00a0(10)]. In particular, optimization problem (19) and PLSA have the same objective function. Both models project the samples onto the simplex spanned by the topics based on the KL divergence. The following lemmas clarify the connections between all three NMF problems with different normalization constraints.\n\nLemma 4.1.\n\nLet (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of the standard NMF optimization problem (2) and let \u03bbk=\u2211vwv\u2062ksubscript\ud835\udf06\ud835\udc58subscript\ud835\udc63subscript\ud835\udc64\ud835\udc63\ud835\udc58\\lambda_{k}=\\sum_{v}w_{vk}italic_\u03bb start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT. Then (W~,H~)~\ud835\udc4a~\ud835\udc3b(\\widetilde{W},\\widetilde{H})( over~ start_ARG italic_W end_ARG , over~ start_ARG italic_H end_ARG ) with w~v\u2062k=wv\u2062k/\u03bbksubscript~\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udf06\ud835\udc58\\widetilde{w}_{vk}=w_{vk}/\\lambda_{k}over~ start_ARG italic_w end_ARG start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT = italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT / italic_\u03bb start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and h~k\u2062d=\u03bbk\u2062hk\u2062dsubscript~\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51\\widetilde{h}_{kd}=\\lambda_{k}h_{kd}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT is a solution of NMF with a normalization constraint on W\ud835\udc4aWitalic_W (18). Conversely, every solution of (18) is a solution of (2).\n\n\nLemma 4.2.\n\nLet (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of NMF with a normalization constraint on W\ud835\udc4aWitalic_W (18) and let \u03bbd=\u2211vxv\u2062dsubscript\ud835\udf06\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\lambda_{d}=\\sum_{v}x_{vd}italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. Then (W,H~)\ud835\udc4a~\ud835\udc3b(W,\\widetilde{H})( italic_W , over~ start_ARG italic_H end_ARG ) with h~k\u2062d=hk\u2062d/\u03bbdsubscript~\u210e\ud835\udc58\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51\\widetilde{h}_{kd}=h_{kd}/\\lambda_{d}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT / italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is a solution of NMF with a normalization constraint on both W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H (19).\nConversely, let (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of (19). Then (W,H~)\ud835\udc4a~\ud835\udc3b(W,\\widetilde{H})( italic_W , over~ start_ARG italic_H end_ARG ) with h~k\u2062d=\u03bbd\u2062hk\u2062dsubscript~\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51\\widetilde{h}_{kd}=\\lambda_{d}h_{kd}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT is a solution of (18).\n\nLemma\u00a04.1 follows directly from W\u2062H=W\u2062D\u2062D\u22121\u2062H\ud835\udc4a\ud835\udc3b\ud835\udc4a\ud835\udc37superscript\ud835\udc371\ud835\udc3bWH=WDD^{-1}Hitalic_W italic_H = italic_W italic_D italic_D start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_H for any invertible non-negative diagonal matrix D\ud835\udc37Ditalic_D and the proof of Lemma\u00a04.2 is deferred to Appendix\u00a0B. Taken together, these two results show that a solution of one of the three NMF problems (2), (18) or (19) can be readily converted into a solution of one of the other optimization problems. In fact, one can also see that an analogous result holds for the fixed points of their MU algorithms (see below).\n\nContrary to intuition, the additional normalization constraints simplify the NMF optimization problem. Recall that the joint auxiliary function (7) of DKL(X||WH)D_{\\text{KL}}(X\\,||\\,WH)italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) is given by\n\nG\u2062((W,H),(W(n),H(n)))\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\\displaystyle G((W,H),(W^{(n)},H^{(n)}))italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) )\n=\u2212\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d(n)+\u2211v,d(W\u2062H)v\u2062d.absentsubscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc63\ud835\udc51subscript\ud835\udc4a\ud835\udc3b\ud835\udc63\ud835\udc51\\displaystyle=-\\sum_{v,k,d}x_{vd}\\phi_{vkd}^{(n)}\\log\\frac{w_{vk}h_{kd}}{\\phi_%\n{vkd}^{(n)}}+\\sum_{v,d}(WH)_{vd}.= - \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG + \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT .\n\nUnder the constraints wk\u2208\u0394V\u22121subscript\ud835\udc64\ud835\udc58subscript\u0394\ud835\udc491w_{k}\\in\\Delta_{V-1}italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT, the second summand reduces to \u2211k,dhk\u2062dsubscript\ud835\udc58\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51\\sum_{k,d}h_{kd}\u2211 start_POSTSUBSCRIPT italic_k , italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT and G\ud835\udc3aGitalic_G can be jointly optimized in all model parameters (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ). Using Lagrange multipliers, we obtain the joint multiplicative update equations given in Algorithm\u00a04 (see Appendix\u00a0B for details). When additionally imposing hd\u2208\u0394K\u22121subscript\u210e\ud835\udc51subscript\u0394\ud835\udc3e1h_{d}\\in\\Delta_{K-1}italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT, the relevant summands of the auxiliary function are identical to the auxiliary function (12) of PLSA. Accordingly, the canonical multiplicative algorithm for (19) is the PLSA algorithm, cf.\u00a0Algorithms\u00a06 and\u00a02. The two models are equivalent both in the sense that their optimization problems are the same and in the sense that their MU algorithms coincide.\n\nAlthough it is also possible to derive update rules that alternate between updating W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H using the induced auxiliary functions (8), the joint update equations have one major advantage. They do not require W\u2062H\ud835\udc4a\ud835\udc3bWHitalic_W italic_H to be recomputed after updating one of the two matrices, which reduces the number of matrix multiplication per iteration from four to three. From a theoretical perspective, the joint updates turn NMF from a generalized EM algorithm into an EM algorithm. Another interesting observation is that the generative process (4) of NMF does not explicitly model the latent topic assignments of individual words in documents. However, the correspondence to PLSA and its generative process shows that \u03d5v\u2062k\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\phi_{vkd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT is in fact the posterior probability that a word v\ud835\udc63vitalic_v in document d\ud835\udc51ditalic_d has been generated by topic k\ud835\udc58kitalic_k. For given data X\ud835\udc4bXitalic_X, the two generative processes of NMF with normalization constraints and PLSA are equivalent and they reveal different perspectives on the factorization. On the one hand, the Poisson noise model of NMF is more concise and motivates to not store \u03d5v\u2062k\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\phi_{vkd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT explicitly during model training. On the other hand, the atomic generative process of PLSA adds a layer of interpretability to the model parameters. More details on the connections between various generative models are given in Appendix\u00a0D, and we also refer the interested reader to [6].\n\n"
        },
        {
            "id": "S5",
            "type": "text",
            "title": "5Connection between NMF and LDA",
            "caption": "5Connection between NMF and LDA",
            "metadata": {},
            "text": "\n5 Connection between NMF and LDA\nThe previous section revealed the equivalence of NMF with additional normalization constraints and PLSA. Since the only difference between PLSA and LDA is the Dirichlet prior on the topic proportions, it is natural to wonder whether NMF with normalization constraints and a Dirichlet prior on the columns of H\ud835\udc3bHitalic_H is equivalent to LDA. This turns out to be the case.\n\nWith the normalization constraints wk\u2208\u0394V\u22121subscript\ud835\udc64\ud835\udc58subscript\u0394\ud835\udc491w_{k}\\in\\Delta_{V-1}italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT on the topic matrix W\ud835\udc4aWitalic_W, the following Dirichlet\u2013Poisson generative model yields, up to a data-dependent constant, the same log-likelihood and variational lower bound as LDA:\n\n1.\nSample the topic proportions hd\u223cDir\u2061(\u03b1)similar-tosubscript\u210e\ud835\udc51Dir\ud835\udefch_{d}\\sim\\operatorname{Dir}(\\alpha)italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT \u223c roman_Dir ( italic_\u03b1 ).\n\n2.\nSample the topic contributions zv\u2062k\u2062d\u223cPoisson\u2061(wv\u2062k\u2062hk\u2062d)similar-tosubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51Poissonsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51z_{vkd}\\sim\\operatorname{Poisson}(w_{vk}h_{kd})italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ).\n\n3.\nXv\u2062d=\u2211kzv\u2062k\u2062dsubscript\ud835\udc4b\ud835\udc63\ud835\udc51subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51X_{vd}=\\sum_{k}z_{vkd}italic_X start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT.\n\nDenoting the slice z:\u2063:dsubscript\ud835\udc67::absent\ud835\udc51z_{::d}italic_z start_POSTSUBSCRIPT : : italic_d end_POSTSUBSCRIPT of the latent tensor z\ud835\udc67zitalic_z as zdsubscript\ud835\udc67\ud835\udc51z_{d}italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and the fiber zv:dsubscript\ud835\udc67:\ud835\udc63\ud835\udc51z_{v:d}italic_z start_POSTSUBSCRIPT italic_v : italic_d end_POSTSUBSCRIPT as zv\u2062dsubscript\ud835\udc67\ud835\udc63\ud835\udc51z_{vd}italic_z start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT, we apply a mean-field variational inference approach and approximate the true posterior of the latent variables with a variational distribution given by\n\nq\u2062(h,z|\u03b2,\u03d5)=\u220fdqd\u2062(hd,zd|\u03b2d,\u03d5d),qd\u2062(hd,zd|\u03b2d,\u03d5d)=q\u2062(hd|\u03b2d)\u2062\u220fvq\u2062(zv\u2062d|xv\u2062d,\u03d5v\u2062d),formulae-sequence\ud835\udc5e\u210econditional\ud835\udc67\ud835\udefditalic-\u03d5subscriptproduct\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\ud835\udc5econditionalsubscript\u210e\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptproduct\ud835\udc63\ud835\udc5econditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc51\\displaystyle q(h,z|\\beta,\\phi)=\\prod_{d}q_{d}(h_{d},z_{d}|\\beta_{d},\\phi_{d})%\n,\\qquad q_{d}(h_{d},z_{d}|\\beta_{d},\\phi_{d})=q(h_{d}|\\beta_{d})\\prod_{v}q(z_{%\nvd}|x_{vd},\\phi_{vd}),italic_q ( italic_h , italic_z | italic_\u03b2 , italic_\u03d5 ) = \u220f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ) ,\n(21)\n\nwhere \u03b2dsubscript\ud835\udefd\ud835\udc51\\beta_{d}italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and \u03d5v\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc51\\phi_{vd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT are Dirichlet and multinomial parameters, respectively. With the joint likelihood of the observed and latent variables of a single document being equal to\n\np\u2062(xd,hd,zd|W,\u03b1)=(\u220fv\ud835\udfd9\u2211kzv\u2062k\u2062d=xv\u2062d)\u2062p\u2062(hd|\u03b1)\u2062\u220fv,kp\u2062(zv\u2062k\u2062d|W,hd),\ud835\udc5dsubscript\ud835\udc65\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4a\ud835\udefcsubscriptproduct\ud835\udc63subscript1subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51\ud835\udc5dconditionalsubscript\u210e\ud835\udc51\ud835\udefcsubscriptproduct\ud835\udc63\ud835\udc58\ud835\udc5dconditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51p(x_{d},h_{d},z_{d}|W,\\alpha)=\\Big{(}\\prod_{v}\\mathbbm{1}_{\\sum_{k}z_{vkd}=x_{%\nvd}}\\Big{)}p(h_{d}|\\alpha)\\prod_{v,k}p(z_{vkd}|W,h_{d}),italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 ) = ( \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) italic_p ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b1 ) \u220f start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_p ( italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ,\n(22)\n\nthe relevant terms of the variational lower bound of the Dirichlet\u2013Poisson model with normalized topic matrix W\ud835\udc4aWitalic_W are identical to the variational lower bound (17) of LDA, and the VI algorithms are identical too (see Appendix\u00a0C). Similar to the connection between NMF with normalization constraints and PLSA, the two different generative processes reveal different perspectives on the factorization. On the one hand, the Dirichlet\u2013Poisson model of NMF is more concice and motivates to not store the variational parameters \u03d5italic-\u03d5\\phiitalic_\u03d5 explicitly during model training\u2014a computational trick that is already used in efficient LDA implementations [20, 38], cf.\u00a0Algorithms\u00a05 and\u00a03.\nOn the other hand, the multinomial generative process of LDA adds a layer of interpretability to \u03d5italic-\u03d5\\phiitalic_\u03d5.\n\n"
        },
        {
            "id": "S6",
            "type": "text",
            "title": "6Sparse NMF",
            "caption": "6Sparse NMF",
            "metadata": {},
            "text": "\n6 Sparse NMF\nIn sparse NMF, a penalty on H\ud835\udc3bHitalic_H is added to decrease the number of topics used in the reconstruction of each document. This is especially useful when the number of topics is high, as it can prevent overfitting and improve the interpretability of W\ud835\udc4aWitalic_W [26]. For a hyperparameter \u03bb>0\ud835\udf060\\lambda>0italic_\u03bb > 0, the objective function of \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-penalized NMF is given by\n\nDKL(X||WH)+\u03bb\u2225H\u22251.D_{\\text{KL}}(X\\,||\\,WH)+\\lambda\\|H\\|_{1}.italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\n(23)\n\nAn important technical difficulty of sparse NMF is that the scaling ambiguity of the matrix product W\u2062H\ud835\udc4a\ud835\udc3bWHitalic_W italic_H in combination with a penalty on H\ud835\udc3bHitalic_H would cause H\ud835\udc3bHitalic_H to converge to zero and W\ud835\udc4aWitalic_W to blow up unless further constraints are imposed. For this reason, the columns of W\ud835\udc4aWitalic_W are usually required to be normalized, and the optimization problem becomes\n\nminW\u2208\u0394V\u22121K,H\u2208\u211d+K\u00d7DDKL(X||WH)+\u03bb\u2225H\u22251.\\displaystyle\\min_{W\\in\\Delta_{V-1}^{K},\\,H\\in\\mathbb{R}_{+}^{K\\times D}}D_{%\n\\text{KL}}(X\\,||\\,WH)+\\lambda\\|H\\|_{1}.roman_min start_POSTSUBSCRIPT italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , italic_H \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\n(24)\n\nWith the \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraints on the columns of W\ud835\udc4aWitalic_W in place, the results from Section\u00a04 can be readily applied to obtain joint update rules to solve (24). Similar to before, the objective function value can be shown to be non-increasing under the following joint updates:\n\nwv\u2062k(n+1)\u221dwv\u2062k(n)\u2062\u2211dxv\u2062d\u2062hk\u2062d(n)(W(n)\u2062H(n))v\u2062d,hk\u2062d(n+1)=11+\u03bb\u2062hk\u2062d(n)\u2062\u2211vxv\u2062d\u2062wv\u2062k(n)(W(n)\u2062H(n))v\u2062d.formulae-sequenceproportional-tosubscriptsuperscript\ud835\udc64\ud835\udc5b1\ud835\udc63\ud835\udc58subscriptsuperscript\ud835\udc64\ud835\udc5b\ud835\udc63\ud835\udc58subscript\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptsuperscript\u210e\ud835\udc5b\ud835\udc58\ud835\udc51subscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51subscriptsuperscript\u210e\ud835\udc5b1\ud835\udc58\ud835\udc5111\ud835\udf06subscriptsuperscript\u210e\ud835\udc5b\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptsuperscript\ud835\udc64\ud835\udc5b\ud835\udc63\ud835\udc58subscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51\\displaystyle w^{(n+1)}_{vk}\\propto w^{(n)}_{vk}\\sum_{d}x_{vd}\\frac{h^{(n)}_{%\nkd}}{(W^{(n)}H^{(n)})_{vd}},\\qquad h^{(n+1)}_{kd}=\\frac{1}{1+\\lambda}h^{(n)}_{%\nkd}\\sum_{v}x_{vd}\\frac{w^{(n)}_{vk}}{(W^{(n)}H^{(n)})_{vd}}.italic_w start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u221d italic_w start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT divide start_ARG italic_h start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG , italic_h start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_h start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT divide start_ARG italic_w start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG .\n(25)\n\nThe only difference between \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-penalized NMF and NMF without a penalty (18) is that the topic weights H\ud835\udc3bHitalic_H are scaled by a factor of 11+\u03bb11\ud835\udf06\\frac{1}{1+\\lambda}divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG, cf.\u00a0Algorithm\u00a04. Surprisingly, however, this holds true not only for the first, but for every iteration of the algorithm. Our observation is not a consequence of the algorithmic approach, but a property of the optimization problem itself:\n\nLemma 6.1.\n\nLet (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of NMF with an \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraint on W\ud835\udc4aWitalic_W (18) and let \u03bb>0\ud835\udf060\\lambda>0italic_\u03bb > 0. Then (W,11+\u03bb\u2062H)\ud835\udc4a11\ud835\udf06\ud835\udc3b(W,\\tfrac{1}{1+\\lambda}H)( italic_W , divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_H ) is a solution of Lasso-penalized NMF with an \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraint (24). Conversely, let (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of (24). Then (W,(1+\u03bb)\u2062H)\ud835\udc4a1\ud835\udf06\ud835\udc3b(W,(1+\\lambda)H)( italic_W , ( 1 + italic_\u03bb ) italic_H ) is a solution of (18).\n\nA proof by contradiction is provided in Appendix\u00a0F. The Lasso penalty being insufficient to induce sparse topic weights H\ud835\udc3bHitalic_H has, to the best of our knowledge, not been reported in the literature [34]. Interestingly, however, replacing the \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraints on the columns of W\ud835\udc4aWitalic_W with \u21132subscript\u21132\\ell_{2}roman_\u2113 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT normalization constraints and using the heuristic gradient-based algorithm from [26] leads to sparsity [29, Section IV]. In fact, any normalization constraints on the columns of W\ud835\udc4aWitalic_W and penalty terms on H\ud835\udc3bHitalic_H can be proven to act as competing regularizers on the two matrices (see Appendix\u00a0F). Together with Lemma\u00a06.1, this demonstrates that the effect on the sparsity of the model parameters not only depends on the penalty on H\ud835\udc3bHitalic_H, but also on the choice of the normalization constraint on W\ud835\udc4aWitalic_W and the reconstruction error.\n"
        },
        {
            "id": "S7",
            "type": "text",
            "title": "7Conclusion",
            "caption": "7Conclusion",
            "metadata": {},
            "text": "\n7 Conclusion\nThis paper has examined NMF with normalization constraints on the columns of one or both matrices of the decomposition. By taking these constraints into account explicitly during the majorization-minimization scheme, it became possible to derive joint update rules that speed up model training significantly. The joint update rules further bridge the gap between NMF optimization problems and algorithms from the topic modeling literature such as PLSA and LDA. Leveraging these connections, we have demonstrated that different latent structures of equivalent generative models can highlight complementary aspects of the same model.\n\nThe existing theoretical and applied research on other aspects and extensions of NMF or LDA is beyond the scope of our work. However, we anticipate that the outlined connections are not limited to the classical full-batch variational inference algorithms of these base models. Regarding NMF, directly resolving the scaling ambiguity in the case of the Kullback\u2013Leibler divergence allowed us to re-examine the Lasso penalty for sparse NMF and might also be helpful in answering questions on the convergence of the model parameters [2]. An interesting but ambitious direction would be to extend these ideas to more general loss functions.\n"
        },
        {
            "id": "Sx1",
            "type": "text",
            "title": "Acknowledgments",
            "caption": "Acknowledgments",
            "metadata": {},
            "text": "\nAcknowledgments\nThis work was funded by the National Institutes of Health (grant number R01CA269805). All authors declare no conflict of interest.\n\nWe would like to express our gratitude to Marinka Zitnik for her advice throughout the project. Moreover, we thank Christian Fiedler, Teng Gao, Allen Lynch, and Dominik Glodzik for their helpful feedback on the manuscript.\n"
        },
        {
            "id": "A1",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix A Auxiliary functions for NMF\nWe give a brief overview of the fundamental idea of majorization-minimization algorithms and the construction of auxiliary functions for NMF with the KL divergence. This section is based on [43, 28, 17].\n\nConsider the optimization problem\n\nminY\u2208\ud835\udcb4\u2061F\u2062(Y)subscript\ud835\udc4c\ud835\udcb4\ud835\udc39\ud835\udc4c\\min_{Y\\in\\mathcal{Y}}F(Y)roman_min start_POSTSUBSCRIPT italic_Y \u2208 caligraphic_Y end_POSTSUBSCRIPT italic_F ( italic_Y )\n\nfor \ud835\udcb4\u2286\u211dM\ud835\udcb4superscript\u211d\ud835\udc40\\mathcal{Y}\\subseteq\\mathbb{R}^{M}caligraphic_Y \u2286 blackboard_R start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT and a function F:\ud835\udcb4\u2192\u211d.:\ud835\udc39\u2192\ud835\udcb4\u211dF\\colon\\mathcal{Y}\\rightarrow\\mathbb{R}.italic_F : caligraphic_Y \u2192 blackboard_R . Within the context of the majorization-minimization framework, the strategy to minimize F\ud835\udc39Fitalic_F consists of two-steps. First, an auxiliary function G:\ud835\udcb4\u00d7\ud835\udcb4\u2192\u211d:\ud835\udc3a\u2192\ud835\udcb4\ud835\udcb4\u211dG\\colon\\mathcal{Y}\\times\\mathcal{Y}\\rightarrow\\mathbb{R}italic_G : caligraphic_Y \u00d7 caligraphic_Y \u2192 blackboard_R with\n\nG\u2062(Y,Y\u2032)\u2265F\u2062(Y),\ud835\udc3a\ud835\udc4csuperscript\ud835\udc4c\u2032\ud835\udc39\ud835\udc4c\\displaystyle G(Y,Y^{\\prime})\\geq F(Y),italic_G ( italic_Y , italic_Y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) \u2265 italic_F ( italic_Y ) ,\n\n\nG\u2062(Y\u2032,Y\u2032)=F\u2062(Y\u2032)\ud835\udc3asuperscript\ud835\udc4c\u2032superscript\ud835\udc4c\u2032\ud835\udc39superscript\ud835\udc4c\u2032\\displaystyle G(Y^{\\prime},Y^{\\prime})=F(Y^{\\prime})italic_G ( italic_Y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) = italic_F ( italic_Y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT )\n\nfor all Y,Y\u2032\u2208\ud835\udcb4\ud835\udc4csuperscript\ud835\udc4c\u2032\ud835\udcb4Y,Y^{\\prime}\\in\\mathcal{Y}italic_Y , italic_Y start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 caligraphic_Y is constructed. The majorizer G\ud835\udc3aGitalic_G of F\ud835\udc39Fitalic_F is then minimized in every iteration, that is,\n\nY(n+1)=argminY\u2208\ud835\udcb4G\u2062(Y,Y(n)).superscript\ud835\udc4c\ud835\udc5b1subscriptargmin\ud835\udc4c\ud835\udcb4\ud835\udc3a\ud835\udc4csuperscript\ud835\udc4c\ud835\udc5bY^{(n+1)}=\\operatorname*{argmin}_{Y\\in\\mathcal{Y}}G(Y,Y^{(n)}).italic_Y start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT = roman_argmin start_POSTSUBSCRIPT italic_Y \u2208 caligraphic_Y end_POSTSUBSCRIPT italic_G ( italic_Y , italic_Y start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) .\n\nIndeed, this procedure yields a non-increasing objective function value due to\n\nF\u2062(Y(n+1))\u2264G\u2062(Y(n+1),Y(n))\u2264G\u2062(Y(n),Y(n))=F\u2062(Y(n)).\ud835\udc39superscript\ud835\udc4c\ud835\udc5b1\ud835\udc3asuperscript\ud835\udc4c\ud835\udc5b1superscript\ud835\udc4c\ud835\udc5b\ud835\udc3asuperscript\ud835\udc4c\ud835\udc5bsuperscript\ud835\udc4c\ud835\udc5b\ud835\udc39superscript\ud835\udc4c\ud835\udc5bF(Y^{(n+1)})\\leq G(Y^{(n+1)},Y^{(n)})\\leq G(Y^{(n)},Y^{(n)})=F(Y^{(n)}).italic_F ( italic_Y start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT ) \u2264 italic_G ( italic_Y start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) \u2264 italic_G ( italic_Y start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_Y start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) = italic_F ( italic_Y start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) .\n\nIn the case of NMF with the KL divergence, the goal is to decompose X\u2208\u211d+V\u00d7D\ud835\udc4bsuperscriptsubscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}_{+}^{V\\times D}italic_X \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT into the product of two non-negative matrices W\u2208\u211d+V\u00d7K\ud835\udc4asuperscriptsubscript\u211d\ud835\udc49\ud835\udc3eW\\in\\mathbb{R}_{+}^{V\\times K}italic_W \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT and H\u2208\u211d+K\u00d7D\ud835\udc3bsuperscriptsubscript\u211d\ud835\udc3e\ud835\udc37H\\in\\mathbb{R}_{+}^{K\\times D}italic_H \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT by minimizing the reconstruction error\n\nDKL(X||WH)=\u2211v,dxv\u2062dlogxv\u2062d(W\u2062H)v\u2062d\u2212xv\u2062d+(WH)v\u2062d.D_{\\text{KL}}(X\\,||\\,WH)=\\sum_{v,d}x_{vd}\\log\\frac{x_{vd}}{(WH)_{vd}}-x_{vd}+(%\nWH)_{vd}.italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) = \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG - italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT + ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT .\n\nNote that it is equivalent to minimize\n\nF(W,H)=\u2212\u2211v,dxv\u2062dlog(WH)v\u2062d+(WH)v\u2062d.F(W,H)=-\\sum_{v,d}x_{vd}\\log(WH)_{vd}+(WH)_{vd}.italic_F ( italic_W , italic_H ) = - \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT + ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT .\n\nFor simplicity, we will therefore often refer to DKLsubscript\ud835\udc37KLD_{\\text{KL}}italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT and F\ud835\udc39Fitalic_F interchangeably. An auxiliary function of DKLsubscript\ud835\udc37KLD_{\\text{KL}}italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT can now be constructed using the following two well-known results.\n\nLemma A.1 ([28, Lemma 3]).\n\nConsider the function F(W,H)=\u2212log(WH)v\u2062dF(W,H)=-\\log(WH)_{vd}italic_F ( italic_W , italic_H ) = - roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT for two indices v,d\ud835\udc63\ud835\udc51v,ditalic_v , italic_d. Then the function G\ud835\udc3aGitalic_G defined by\n\n\u03d5v\u2062k\u2062d\u2032subscriptsuperscriptitalic-\u03d5\u2032\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle\\phi^{\\prime}_{vkd}italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT\n\u2254wv\u2062k\u2032\u2062hk\u2062d\u2032(W\u2032\u2062H\u2032)v\u2062d,\u2254absentsubscriptsuperscript\ud835\udc64\u2032\ud835\udc63\ud835\udc58subscriptsuperscript\u210e\u2032\ud835\udc58\ud835\udc51subscriptsuperscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032\ud835\udc63\ud835\udc51\\displaystyle\\coloneqq\\frac{w^{\\prime}_{vk}h^{\\prime}_{kd}}{(W^{\\prime}H^{%\n\\prime})_{vd}},\u2254 divide start_ARG italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG ,\n\n\nG\u2062((W,H),(W\u2032,H\u2032))\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032\\displaystyle G((W,H),(W^{\\prime},H^{\\prime}))italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) )\n=\u2212\u2211k\u03d5v\u2062k\u2062d\u2032\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d\u2032absentsubscript\ud835\udc58subscriptsuperscriptitalic-\u03d5\u2032\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscriptsuperscriptitalic-\u03d5\u2032\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle=-\\sum_{k}\\phi^{\\prime}_{vkd}\\log\\frac{w_{vk}h_{kd}}{\\phi^{\\prime%\n}_{vkd}}= - \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG\n\nis an auxiliary function of F.\ud835\udc39F.italic_F .\n\nProof.\nThe equality G\u2062((W\u2032,H\u2032),(W\u2032,H\u2032))=F\u2062(W\u2032,H\u2032)\ud835\udc3asuperscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032superscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032\ud835\udc39superscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032G((W^{\\prime},H^{\\prime}),(W^{\\prime},H^{\\prime}))=F(W^{\\prime},H^{\\prime})italic_G ( ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) , ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) ) = italic_F ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) is easy to verify and Jensen\u2019s inequality implies\n\nlog(WH)v\u2062d=log\u2211k\u03d5v\u2062k\u2062d\u2032wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d\u2032\u2265\u2211k\u03d5v\u2062k\u2062d\u2032logwv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d\u2032.\\log(WH)_{vd}=\\log\\sum_{k}\\phi^{\\prime}_{vkd}\\frac{w_{vk}h_{kd}}{\\phi^{\\prime}%\n_{vkd}}\\geq\\sum_{k}\\phi^{\\prime}_{vkd}\\log\\frac{w_{vk}h_{kd}}{\\phi^{\\prime}_{%\nvkd}}.roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = roman_log \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG \u2265 \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG .\n\n\u220e\n\n\nLemma A.2.\n\nConsider finitely many function Fi:\ud835\udcb4\u2192\u211d:subscript\ud835\udc39\ud835\udc56\u2192\ud835\udcb4\u211dF_{i}\\colon\\mathcal{Y}\\rightarrow\\mathbb{R}italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : caligraphic_Y \u2192 blackboard_R with auxiliary functions Gi,subscript\ud835\udc3a\ud835\udc56G_{i},italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , and let ci\u2208\u211d+subscript\ud835\udc50\ud835\udc56subscript\u211dc_{i}\\in\\mathbb{R}_{+}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT. Then G=\u2211ici\u2062Gi\ud835\udc3asubscript\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc3a\ud835\udc56G=\\sum_{i}c_{i}G_{i}italic_G = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_G start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is an auxiliary function of F=\u2211ici\u2062Fi.\ud835\udc39subscript\ud835\udc56subscript\ud835\udc50\ud835\udc56subscript\ud835\udc39\ud835\udc56F=\\sum_{i}c_{i}F_{i}.italic_F = \u2211 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_F start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .\n\nProof.\nFollows from the definition of auxiliary functions.\n\u220e\n\n\nCorollary A.3 ([28, Lemma 3]).\n\nConsider the function\n\nF(W,H)=\u2212\u2211v,dxv\u2062dlog(WH)v\u2062d+(WH)v\u2062d.F(W,H)=-\\sum_{v,d}x_{vd}\\log(WH)_{vd}+(WH)_{vd}.italic_F ( italic_W , italic_H ) = - \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT + ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT .\n\nThen the function G\ud835\udc3aGitalic_G defined by\n\n\u03d5v\u2062k\u2062d\u2032subscriptsuperscriptitalic-\u03d5\u2032\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle\\phi^{\\prime}_{vkd}italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT\n=wv\u2062k\u2032\u2062hk\u2062d\u2032(W\u2032\u2062H\u2032)v\u2062d,absentsubscriptsuperscript\ud835\udc64\u2032\ud835\udc63\ud835\udc58subscriptsuperscript\u210e\u2032\ud835\udc58\ud835\udc51subscriptsuperscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032\ud835\udc63\ud835\udc51\\displaystyle=\\frac{w^{\\prime}_{vk}h^{\\prime}_{kd}}{(W^{\\prime}H^{\\prime})_{vd%\n}},= divide start_ARG italic_w start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG ,\n\n\nG\u2062((W,H),(W\u2032,H\u2032))\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\u2032superscript\ud835\udc3b\u2032\\displaystyle G((W,H),(W^{\\prime},H^{\\prime}))italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) )\n=\u2212\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d\u2032\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d\u2032+\u2211v,d(W\u2062H)v\u2062dabsentsubscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptsuperscriptitalic-\u03d5\u2032\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscriptsuperscriptitalic-\u03d5\u2032\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc51subscript\ud835\udc4a\ud835\udc3b\ud835\udc63\ud835\udc51\\displaystyle=-\\sum_{v,k,d}x_{vd}\\phi^{\\prime}_{vkd}\\log\\frac{w_{vk}h_{kd}}{%\n\\phi^{\\prime}_{vkd}}+\\sum_{v,d}(WH)_{vd}= - \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG + \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT\n\nis an auxiliary function of F\ud835\udc39Fitalic_F.\n\nProof.\nApply Lemma\u00a0A.1 and Lemma\u00a0A.2.\n\u220e\n\n"
        },
        {
            "id": "A2",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix B NMF with normalization constraints\nIn this section, we first proof 4.2, which states that global optima of NMF with a normalization constraint on W\ud835\udc4aWitalic_W (18) and NMF with a normalization constraint on both W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H (19) are identical up to a scaling of their topic weights by the total word counts \u2211vxv\u2062dsubscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\sum_{v}x_{vd}\u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. Second, we provide details to the derivation of the joint update rules of the two NMF problems with normalization constraints. Lastly, we show that the connection from 4.2 extends to iterates and fixed points of their MU algorithms.\n\nLemma 4.2.\n\nLet (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of NMF with a normalization constraint on W\ud835\udc4aWitalic_W (18) and let \u03bbd=\u2211vxv\u2062dsubscript\ud835\udf06\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\lambda_{d}=\\sum_{v}x_{vd}italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. Then (W,H~)\ud835\udc4a~\ud835\udc3b(W,\\widetilde{H})( italic_W , over~ start_ARG italic_H end_ARG ) with h~k\u2062d=hk\u2062d/\u03bbdsubscript~\u210e\ud835\udc58\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51\\widetilde{h}_{kd}=h_{kd}/\\lambda_{d}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT / italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is a solution of NMF with a normalization constraint on both W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H (19).\nConversely, let (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of (19). Then (W,H~)\ud835\udc4a~\ud835\udc3b(W,\\widetilde{H})( italic_W , over~ start_ARG italic_H end_ARG ) with h~k\u2062d=\u03bbd\u2062hk\u2062dsubscript~\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51\\widetilde{h}_{kd}=\\lambda_{d}h_{kd}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT is a solution of (18).\n\nProof.\nThe main ingredient of the proof is that solutions of NMF with the KL divergence (2) preserve the column sums of X\ud835\udc4bXitalic_X. That is, for any solution (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) of (2), we have\n\n\u2211v(W\u2062H)v\u2062d=\u2211vxv\u2062dsubscript\ud835\udc63subscript\ud835\udc4a\ud835\udc3b\ud835\udc63\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\sum_{v}(WH)_{vd}=\\sum_{v}x_{vd}\u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT\n(26)\n\n[17, Theorem 6.9]. Now, let (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of NMF with an additional normalization constraint on W\ud835\udc4aWitalic_W. Due to Lemma\u00a04.1, it is also a solution of the standard NMF problem (2). From (26) and wk\u2208\u0394V\u22121subscript\ud835\udc64\ud835\udc58subscript\u0394\ud835\udc491w_{k}\\in\\Delta_{V-1}italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT, it follows that \u2211khk\u2062d=\u2211vxv\u2062d=\u03bbd.subscript\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udf06\ud835\udc51\\sum_{k}h_{kd}=\\sum_{v}x_{vd}=\\lambda_{d}.\u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT . Hence, the candidate solution (W,H~)\ud835\udc4a~\ud835\udc3b(W,\\widetilde{H})( italic_W , over~ start_ARG italic_H end_ARG ) defined via h~k\u2062d=hk\u2062d/\u03bbdsubscript~\u210e\ud835\udc58\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51\\widetilde{h}_{kd}=h_{kd}/\\lambda_{d}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT / italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT indeed fulfills the additional normalization constraint, and we also have\n\nDKL(X||WH)=\u2211v,dxv\u2062dlogxv\u2062d(W\u2062H)v\u2062d.D_{\\text{KL}}(X\\,||\\,WH)=\\sum_{v,d}x_{vd}\\log\\frac{x_{vd}}{(WH)_{vd}}.italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) = \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W italic_H ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG .\n\nThe result can now be shown using a standard proof by contraction.\n\u220e\n\nIn the following, we provide details to the constrained joint optimization of the joint auxiliary function G\ud835\udc3aGitalic_G of DKL(X||WH)D_{\\text{KL}}(X\\,||\\,WH)italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ). Recall from (7) that G\ud835\udc3aGitalic_G is given by\n\n\u03d5v\u2062k\u2062d(n)superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5b\\displaystyle\\phi_{vkd}^{(n)}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT\n=wv\u2062k(n)\u2062hk\u2062d(n)(W(n)\u2062H(n))v\u2062d,absentsuperscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58\ud835\udc5bsuperscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5bsubscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51\\displaystyle=\\frac{w_{vk}^{(n)}h_{kd}^{(n)}}{(W^{(n)}H^{(n)})_{vd}},= divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG ,\n\n\nG\u2062((W,H),(W(n),H(n)))\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\\displaystyle G((W,H),(W^{(n)},H^{(n)}))italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) )\n=\u2212\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d(n)+wv\u2062k\u2062hk\u2062d,absentsubscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=-\\sum_{v,k,d}x_{vd}\\phi_{vkd}^{(n)}\\log\\frac{w_{vk}h_{kd}}{\\phi_%\n{vkd}^{(n)}}+w_{vk}h_{kd},= - \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG + italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ,\n\nso the derivatives of G\ud835\udc3aGitalic_G are\n\n\u2202wv\u2062kG\u2062((W,H),(W(n),H(n)))subscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\\displaystyle\\partial_{w_{vk}}G((W,H),(W^{(n)},H^{(n)}))\u2202 start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) )\n=\u22121wv\u2062k\u2062\u2211dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)+\u2211dhk\u2062d,absent1subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=-\\frac{1}{w_{vk}}\\sum_{d}x_{vd}\\phi_{vkd}^{(n)}+\\sum_{d}h_{kd},= - divide start_ARG 1 end_ARG start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ,\n\n\n\u2202hk\u2062dG\u2062((W,H),(W(n),H(n)))subscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\\displaystyle\\partial_{h_{kd}}G((W,H),(W^{(n)},H^{(n)}))\u2202 start_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) )\n=\u22121hk\u2062d\u2062\u2211vxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)+\u2211vwv\u2062k.absent1subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc63subscript\ud835\udc64\ud835\udc63\ud835\udc58\\displaystyle=-\\frac{1}{h_{kd}}\\sum_{v}x_{vd}\\phi_{vkd}^{(n)}+\\sum_{v}w_{vk}.= - divide start_ARG 1 end_ARG start_ARG italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT + \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT .\n\nWith the additional normalization constraints wk\u2208\u0394V\u22121subscript\ud835\udc64\ud835\udc58subscript\u0394\ud835\udc491w_{k}\\in\\Delta_{V-1}italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT, the method of Lagrange multipliers implies that there exist \u03bbk\u2208\u211dsubscript\ud835\udf06\ud835\udc58\u211d\\lambda_{k}\\in\\mathbb{R}italic_\u03bb start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 blackboard_R with\n\n1wv\u2062k\u2062\u2211dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u2212\u2211dhk\u2062d=\u03bbk,1subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc58\\frac{1}{w_{vk}}\\sum_{d}x_{vd}\\phi_{vkd}^{(n)}-\\sum_{d}h_{kd}=\\lambda_{k},divide start_ARG 1 end_ARG start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT - \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ,\n\nwhich yields\n\nwv\u2062k\u221d\u2211dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n).proportional-tosubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bw_{vk}\\propto\\sum_{d}x_{vd}\\phi_{vkd}^{(n)}.italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u221d \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT .\n(27)\n\nSetting the derivative of G\ud835\udc3aGitalic_G with respect to hk\u2062dsubscript\u210e\ud835\udc58\ud835\udc51h_{kd}italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT equal to zero and again utilizing wk\u2208\u0394V\u22121subscript\ud835\udc64\ud835\udc58subscript\u0394\ud835\udc491w_{k}\\in\\Delta_{V-1}italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT, we immediately obtain\n\nhk\u2062d=\u2211vxv\u2062d\u2062\u03d5v\u2062k\u2062d(n).subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bh_{kd}=\\sum_{v}x_{vd}\\phi_{vkd}^{(n)}.italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT .\n(28)\n\nIf we additionally require the columns of H\ud835\udc3bHitalic_H to be normalized, i.e., hd\u2208\u0394K\u22121subscript\u210e\ud835\udc51subscript\u0394\ud835\udc3e1h_{d}\\in\\Delta_{K-1}italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT, the method of Lagrange multipliers implies that there exist \u03bbd\u2208\u211dsubscript\ud835\udf06\ud835\udc51\u211d\\lambda_{d}\\in\\mathbb{R}italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT \u2208 blackboard_R with\n\n1hk\u2062d\u2062\u2211vxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u22121=\u03bbd,1subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5b1subscript\ud835\udf06\ud835\udc51\\frac{1}{h_{kd}}\\sum_{v}x_{vd}\\phi_{vkd}^{(n)}-1=\\lambda_{d},divide start_ARG 1 end_ARG start_ARG italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT - 1 = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ,\n\nwhich yields\n\nhk\u2062d\u221d\u2211vxv\u2062d\u2062\u03d5v\u2062k\u2062d(n).proportional-tosubscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bh_{kd}\\propto\\sum_{v}x_{vd}\\phi_{vkd}^{(n)}.italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT \u221d \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT .\n(29)\n\n\nNotice that the normalization in (29) just scales (28) by the total word counts \u2211vxv\u2062dsubscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\sum_{v}x_{vd}\u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. More precisely, denoting the n\ud835\udc5bnitalic_n-th iterate of the NMF with a normalization constraint on W\ud835\udc4aWitalic_W Algorithm\u00a04 by (W(n),NMF,H(n),NMF)superscript\ud835\udc4a\ud835\udc5bNMFsuperscript\ud835\udc3b\ud835\udc5bNMF(W^{(n),\\text{NMF}},H^{(n),\\text{NMF}})( italic_W start_POSTSUPERSCRIPT ( italic_n ) , NMF end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) , NMF end_POSTSUPERSCRIPT ) and the n\ud835\udc5bnitalic_n-th iterate of NMF with a normalization constraint on both W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H / PLSA Algorithm\u00a06 by (W(n),PLSA,H(n),PLSA)superscript\ud835\udc4a\ud835\udc5bPLSAsuperscript\ud835\udc3b\ud835\udc5bPLSA(W^{(n),\\text{PLSA}},H^{(n),\\text{PLSA}})( italic_W start_POSTSUPERSCRIPT ( italic_n ) , PLSA end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) , PLSA end_POSTSUPERSCRIPT ), we obtain the following result.\n\nLemma B.1.\n\nLet X\u2208\u211d+V\u00d7D\ud835\udc4bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}^{V\\times D}_{+}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and let \u03bbd=\u2211vxv\u2062dsubscript\ud835\udf06\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\lambda_{d}=\\sum_{v}x_{vd}italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. Let W(0)\u2208\u0394V\u22121K,H(0)\u2208\u0394K\u22121Dformulae-sequencesuperscript\ud835\udc4a0superscriptsubscript\u0394\ud835\udc491\ud835\udc3esuperscript\ud835\udc3b0superscriptsubscript\u0394\ud835\udc3e1\ud835\udc37W^{(0)}\\in\\Delta_{V-1}^{K},\\,H^{(0)}\\in\\Delta_{K-1}^{D}italic_W start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT be normalized initial values. Then for all n\u22651\ud835\udc5b1n\\geq 1italic_n \u2265 1\n\nW(n),NMFsuperscript\ud835\udc4a\ud835\udc5bNMF\\displaystyle W^{(n),\\text{NMF}}italic_W start_POSTSUPERSCRIPT ( italic_n ) , NMF end_POSTSUPERSCRIPT\n=W(n),PLSA,absentsuperscript\ud835\udc4a\ud835\udc5bPLSA\\displaystyle=W^{(n),\\text{PLSA}},= italic_W start_POSTSUPERSCRIPT ( italic_n ) , PLSA end_POSTSUPERSCRIPT ,\n\n\nhk\u2062d(n),NMFsuperscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5bNMF\\displaystyle h_{kd}^{(n),\\text{NMF}}italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) , NMF end_POSTSUPERSCRIPT\n=\u03bbd\u2062hk\u2062d(n),PLSA,absentsubscript\ud835\udf06\ud835\udc51superscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5bPLSA\\displaystyle=\\lambda_{d}h_{kd}^{(n),\\text{PLSA}},= italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) , PLSA end_POSTSUPERSCRIPT ,\n\n\n\u2211khk\u2062d(n),NMFsubscript\ud835\udc58superscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5bNMF\\displaystyle\\sum_{k}h_{kd}^{(n),\\text{NMF}}\u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) , NMF end_POSTSUPERSCRIPT\n=\u03bbd.absentsubscript\ud835\udf06\ud835\udc51\\displaystyle=\\lambda_{d}.= italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT .\n\n\n\nProof.\nUsing \u2211k,vxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)=\u2211vxv\u2062d=\u03bbdsubscript\ud835\udc58\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udf06\ud835\udc51\\sum_{k,v}x_{vd}\\phi_{vkd}^{(n)}=\\sum_{v}x_{vd}=\\lambda_{d}\u2211 start_POSTSUBSCRIPT italic_k , italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, the statement follows by induction.\n\u220e\n\n\nCorollary B.2.\n\nLet X\u2208\u211d+V\u00d7D\ud835\udc4bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}^{V\\times D}_{+}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and let \u03bbd=\u2211vxv\u2062dsubscript\ud835\udf06\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\lambda_{d}=\\sum_{v}x_{vd}italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. Let (W\u2217,NMF,H\u2217,NMF)superscript\ud835\udc4aNMFsuperscript\ud835\udc3bNMF(W^{*,\\text{NMF}},H^{*,\\text{NMF}})( italic_W start_POSTSUPERSCRIPT \u2217 , NMF end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2217 , NMF end_POSTSUPERSCRIPT ) be a fixed point of NMF with a normalization constraint on W\ud835\udc4aWitalic_W Algorithm\u00a04. Then (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) with W=W\u2217,NMF\ud835\udc4asuperscript\ud835\udc4aNMFW=W^{*,\\text{NMF}}italic_W = italic_W start_POSTSUPERSCRIPT \u2217 , NMF end_POSTSUPERSCRIPT and hk\u2062d=hk\u2062d\u2217,NMF/\u03bbdsubscript\u210e\ud835\udc58\ud835\udc51superscriptsubscript\u210e\ud835\udc58\ud835\udc51NMFsubscript\ud835\udf06\ud835\udc51h_{kd}=h_{kd}^{*,\\text{NMF}}/\\lambda_{d}italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 , NMF end_POSTSUPERSCRIPT / italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is a fixed point of NMF with a normalization constraint on both W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H / PLSA Algorithm\u00a06. Conversely, let (W\u2217,PLSA,H\u2217,PLSA)superscript\ud835\udc4aPLSAsuperscript\ud835\udc3bPLSA(W^{*,\\text{PLSA}},H^{*,\\text{PLSA}})( italic_W start_POSTSUPERSCRIPT \u2217 , PLSA end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2217 , PLSA end_POSTSUPERSCRIPT ) be a fixed point of Algorithm\u00a06. Then (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) with W=W\u2217,PLSA\ud835\udc4asuperscript\ud835\udc4aPLSAW=W^{*,\\text{PLSA}}italic_W = italic_W start_POSTSUPERSCRIPT \u2217 , PLSA end_POSTSUPERSCRIPT and hk\u2062d=\u03bbd\u2062hk\u2062d\u2217,PLSAsubscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51superscriptsubscript\u210e\ud835\udc58\ud835\udc51PLSAh_{kd}=\\lambda_{d}h_{kd}^{*,\\text{PLSA}}italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 , PLSA end_POSTSUPERSCRIPT is a fixed point of Algorithm\u00a04.\n\nProof.\nFollows directly from Lemma\u00a0B.1 and \u2211khk\u2062d\u2217,NMF=\u2211vxv\u2062d=\u03bbdsubscript\ud835\udc58superscriptsubscript\u210e\ud835\udc58\ud835\udc51NMFsubscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udf06\ud835\udc51\\sum_{k}h_{kd}^{*,\\text{NMF}}=\\sum_{v}x_{vd}=\\lambda_{d}\u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u2217 , NMF end_POSTSUPERSCRIPT = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT for any fixed point of Algorithm\u00a04.\n\u220e\n\n"
        },
        {
            "id": "A3",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix C Dirichlet\u2013Poisson model\nIn this section, we derive the variational lower bound of the Dirichlet\u2013Poisson model with a normalized topic matrix W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT described in Section\u00a05. Recall the generative process of the word counts of a single document d\ud835\udc51ditalic_d:\n\n1.\nSample the topic proportions hd\u223cDir\u2061(\u03b1)similar-tosubscript\u210e\ud835\udc51Dir\ud835\udefch_{d}\\sim\\operatorname{Dir}(\\alpha)italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT \u223c roman_Dir ( italic_\u03b1 ).\n\n2.\nSample the topic contributions zv\u2062k\u2062d\u223cPoisson\u2061(wv\u2062k\u2062hk\u2062d)similar-tosubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51Poissonsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51z_{vkd}\\sim\\operatorname{Poisson}(w_{vk}h_{kd})italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ).\n\n3.\nXv\u2062d=\u2211kzv\u2062k\u2062dsubscript\ud835\udc4b\ud835\udc63\ud835\udc51subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51X_{vd}=\\sum_{k}z_{vkd}italic_X start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT.\n\nUnder this model, the joint likelihood of the observed counts xdsubscript\ud835\udc65\ud835\udc51x_{d}italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and the latent variables (hd,zd)subscript\u210e\ud835\udc51subscript\ud835\udc67\ud835\udc51(h_{d},z_{d})( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is given by\n\np\u2062(xd,hd,zd|W,\u03b1)=(\u220fv\ud835\udfd9\u2211kzv\u2062k\u2062d=xv\u2062d)\u2062p\u2062(hd|\u03b1)\u2062\u220fv,kp\u2062(zv\u2062k\u2062d|W,hd),\ud835\udc5dsubscript\ud835\udc65\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4a\ud835\udefcsubscriptproduct\ud835\udc63subscript1subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51\ud835\udc5dconditionalsubscript\u210e\ud835\udc51\ud835\udefcsubscriptproduct\ud835\udc63\ud835\udc58\ud835\udc5dconditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51p(x_{d},h_{d},z_{d}|W,\\alpha)=\\Big{(}\\prod_{v}\\mathbbm{1}_{\\sum_{k}z_{vkd}=x_{%\nvd}}\\Big{)}p(h_{d}|\\alpha)\\prod_{v,k}p(z_{vkd}|W,h_{d}),italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 ) = ( \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) italic_p ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b1 ) \u220f start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_p ( italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ,\n\nwhere\n\np\u2062(zv\u2062k\u2062d|W,hd)=(wv\u2062k\u2062hk\u2062d)zv\u2062k\u2062d\u2062e\u2212wv\u2062k\u2062hk\u2062dzv\u2062k\u2062d!\ud835\udc5dconditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51superscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51superscript\ud835\udc52subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51p(z_{vkd}|W,h_{d})=(w_{vk}h_{kd})^{z_{vkd}}\\frac{e^{-w_{vk}h_{kd}}}{z_{vkd}!}italic_p ( italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG italic_e start_POSTSUPERSCRIPT - italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! end_ARG\n\nis the probability mass function of the Poisson distribution. Identically to LDA, the introduction of the Dirichlet prior on the topic proportions results in an intractable posterior distribution of the latent variables. We therefore resort to variational inference and approximate the true posterior with a variational distribution q\ud835\udc5eqitalic_q given by\n\nq\u2062(h,z|\u03b2,\u03d5)=\u220fdqd\u2062(hd,zd|\u03b2d,\u03d5d),qd\u2062(hd,zd|\u03b2d,\u03d5d)=q\u2062(hd|\u03b2d)\u2062\u220fvq\u2062(zv\u2062d|xv\u2062d,\u03d5v\u2062d),formulae-sequence\ud835\udc5e\u210econditional\ud835\udc67\ud835\udefditalic-\u03d5subscriptproduct\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\ud835\udc5econditionalsubscript\u210e\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptproduct\ud835\udc63\ud835\udc5econditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc51\\displaystyle q(h,z|\\beta,\\phi)=\\prod_{d}q_{d}(h_{d},z_{d}|\\beta_{d},\\phi_{d})%\n,\\qquad q_{d}(h_{d},z_{d}|\\beta_{d},\\phi_{d})=q(h_{d}|\\beta_{d})\\prod_{v}q(z_{%\nvd}|x_{vd},\\phi_{vd}),italic_q ( italic_h , italic_z | italic_\u03b2 , italic_\u03d5 ) = \u220f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ) ,\n\nwhere \u03b2dsubscript\ud835\udefd\ud835\udc51\\beta_{d}italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and \u03d5v\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc51\\phi_{vd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT are Dirichlet and multinomial parameters, respectively. We will now compute the individual terms of the per-document variational lower bound\n\nlog\u2061p\u2062(xd|W,\u03b1)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefc\\displaystyle\\log p(x_{d}|W,\\alpha)roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 )\n\u2265\ud835\udd3cq\u2062[log\u2061p\u2062(xd,hd,zd|W,\u03b1)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(hd,zd|\u03b2d,\u03d5d)]absentsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dsubscript\ud835\udc65\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4a\ud835\udefcsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle\\geq\\mathbb{E}_{q}\\!\\left[\\log p(x_{d},h_{d},z_{d}|W,\\alpha)%\n\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(h_{d},z_{d}|\\beta_{d},\\phi_{d})\\right]\u2265 blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n\n\n=\ud835\udd3cq\u2062[log\u2061p\u2062(hd|\u03b1)]+\ud835\udd3cq\u2062[log\u2061p\u2062(xd,zd|W,hd)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(hd)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(zd)]absentsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dconditionalsubscript\u210e\ud835\udc51\ud835\udefcsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dsubscript\ud835\udc65\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\u210e\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\ud835\udc67\ud835\udc51\\displaystyle=\\mathbb{E}_{q}\\!\\left[\\log p(h_{d}|\\alpha)\\right]+\\mathbb{E}_{q}%\n\\!\\left[\\log p(x_{d},z_{d}|W,h_{d})\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(h_{d})%\n\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(z_{d})\\right]= blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b1 ) ] + blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n\n\n\u2255\u2112dDP\u2062(xd|W,\u03b1,\u03b2d,\u03d5d)\u2255absentsubscriptsuperscript\u2112DP\ud835\udc51conditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefcsubscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle\\eqqcolon\\mathcal{L}^{\\text{DP}}_{d}(x_{d}|W,\\alpha,\\beta_{d},%\n\\phi_{d})\u2255 caligraphic_L start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )\n\nof the Dirichlet\u2013Poisson model. Plugging in the density of the Dirichlet distribution, the first and third term are given by\n\n\ud835\udd3cq\u2062[log\u2061p\u2062(hd|\u03b1)]subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dconditionalsubscript\u210e\ud835\udc51\ud835\udefc\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log p(h_{d}|\\alpha)\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b1 ) ]\n=log\u2061\u0393\u2062(\u2211k\u03b1k)\u2212\u2211klog\u2061\u0393\u2062(\u03b1k)+\u2211k(\u03b1k\u22121)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d],absent\u0393subscript\ud835\udc58subscript\ud835\udefc\ud835\udc58subscript\ud835\udc58\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udc58subscript\ud835\udefc\ud835\udc581subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=\\log\\Gamma(\\textstyle\\sum_{k}\\alpha_{k})-\\displaystyle\\sum_{k}%\n\\log\\Gamma(\\alpha_{k})+\\sum_{k}(\\alpha_{k}-1)\\mathbb{E}_{q}\\!\\left[\\log h_{kd}%\n\\right],= roman_log roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] ,\n\n\n\ud835\udd3cq\u2062[log\u2061q\u2062(hd)]subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\u210e\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log q(h_{d})\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n=log\u2061\u0393\u2062(\u2211k\u03b2k\u2062d)\u2212\u2211klog\u2061\u0393\u2062(\u03b2k\u2062d)+\u2211k(\u03b2k\u2062d\u22121)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d]absent\u0393subscript\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc58\u0393subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc511subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=\\log\\Gamma(\\textstyle\\sum_{k}\\beta_{kd})-\\displaystyle\\sum_{k}%\n\\log\\Gamma(\\beta_{kd})+\\sum_{k}(\\beta_{kd}-1)\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]= roman_log roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log roman_\u0393 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) + \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT - 1 ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ]\n\nfor\n\n\ud835\udd3cq\u2062[log\u2061hk\u2062d]=\u03c8\u2062(\u03b2k\u2062d)\u2212\u03c8\u2062(\u2211k\u2032\u03b2k\u2032\u2062d)\u2255log\u2061(h~k\u2062d)subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\ud835\udf13subscript\ud835\udefd\ud835\udc58\ud835\udc51\ud835\udf13subscriptsuperscript\ud835\udc58\u2032subscript\ud835\udefdsuperscript\ud835\udc58\u2032\ud835\udc51\u2255subscript~\u210e\ud835\udc58\ud835\udc51\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]=\\psi(\\beta_{kd})-\\psi(\\textstyle\\sum_%\n{k^{\\prime}}\\beta_{k^{\\prime}d})\\eqqcolon\\log(\\widetilde{h}_{kd})blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] = italic_\u03c8 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - italic_\u03c8 ( \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_d end_POSTSUBSCRIPT ) \u2255 roman_log ( over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT )\n(30)\n\nand the digamma function \u03c8\ud835\udf13\\psiitalic_\u03c8, cf.\u00a0[4, A.1, eq.\u00a0(14), (15)]. Using the normalization constraints on the topic matrix and topic weights, the second term reduces to\n\n\ud835\udd3cq\u2062[log\u2061p\u2062(xd,zd|W,hd)]subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dsubscript\ud835\udc65\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log p(x_{d},z_{d}|W,h_{d})\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n=\u2211v,k\ud835\udd3cq\u2062[log\u2061p\u2062(zv\u2062k\u2062d|W,hd)]absentsubscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dconditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51\\displaystyle=\\sum_{v,k}\\mathbb{E}_{q}\\!\\left[\\log p(z_{vkd}|W,h_{d})\\right]= \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n\n\n=\u2211v,k\ud835\udd3cq\u2062[zv\u2062k\u2062d]\u2062log\u2061(wv\u2062k\u2062h~k\u2062d)\u2212\u2211v,kwv\u2062k\u2062\ud835\udd3cq\u2062[hk\u2062d]\u2212\u2211v,k\ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!]absentsubscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{v,k}\\mathbb{E}_{q}\\!\\left[z_{vkd}\\right]\\log(w_{vk}%\n\\widetilde{h}_{kd})-\\sum_{v,k}w_{vk}\\mathbb{E}_{q}\\!\\left[h_{kd}\\right]-\\sum_{%\nv,k}\\mathbb{E}_{q}\\!\\left[\\log z_{vkd}!\\right]= \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ] roman_log ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ]\n\n\n=\u2211v,k\u03d5v\u2062k\u2062d\u2062xv\u2062d\u2062log\u2061(wv\u2062k\u2062h~k\u2062d)\u22121\u2212\u2211v,k\ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!],absentsubscript\ud835\udc63\ud835\udc58subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc511subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{v,k}\\phi_{vkd}x_{vd}\\log(w_{vk}\\widetilde{h}_{kd})-1-\\sum_%\n{v,k}\\mathbb{E}_{q}\\!\\left[\\log z_{vkd}!\\right],= \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - 1 - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ] ,\n\nand the last term is given by\n\n\ud835\udd3cq\u2062[log\u2061q\u2062(zd)]subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\ud835\udc67\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log q(z_{d})\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n=\u2211v\ud835\udd3cq\u2062[log\u2061zv\u2062d]absentsubscript\ud835\udc63subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc51\\displaystyle=\\sum_{v}\\mathbb{E}_{q}\\!\\left[\\log z_{vd}\\right]= \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ]\n\n\n=\u2211vlog\u2061xv\u2062d!\u2212\u2211v,k\ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!]+\u2211v,k\ud835\udd3cq\u2062[zv\u2062k\u2062d]\u2062log\u2061\u03d5v\u2062k\u2062dabsentsubscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{v}\\log x_{vd}!-\\sum_{v,k}\\mathbb{E}_{q}\\!\\left[\\log z_{vkd%\n}!\\right]+\\sum_{v,k}\\mathbb{E}_{q}\\!\\left[z_{vkd}\\right]\\log\\phi_{vkd}= \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT roman_log italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ! - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ] + \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ] roman_log italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT\n\n\n=\u2211vlog\u2061xv\u2062d!\u2212\u2211v,k\ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!]+\u2211v,k\u03d5v\u2062k\u2062d\u2062xv\u2062d\u2062log\u2061\u03d5v\u2062k\u2062d.absentsubscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc58subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{v}\\log x_{vd}!-\\sum_{v,k}\\mathbb{E}_{q}\\!\\left[\\log z_{vkd%\n}!\\right]+\\sum_{v,k}\\phi_{vkd}x_{vd}\\log\\phi_{vkd}.= \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT roman_log italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ! - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ] + \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT .\n\nWhen combining all terms except data-dependent constants, the summands involving \ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!]subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\\mathbb{E}_{q}\\!\\left[\\log z_{vkd}!\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ] cancel each other and the per-document variational lower bound simplifies to\n\n\u2112dDP\u2062(xd|W,\u03b1,\u03b2d,\u03d5d)subscriptsuperscript\u2112DP\ud835\udc51conditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefcsubscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle\\mathcal{L}^{\\text{DP}}_{d}(x_{d}|W,\\alpha,\\beta_{d},\\phi_{d})caligraphic_L start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )\n=\u2211v,kxv\u2062d\u2062\u03d5v\u2062k\u2062d\u2062log\u2061wv\u2062k\u2062h~k\u2062d\u03d5v\u2062k\u2062d+log\u2061\u0393\u2062(\u2211k\u03b1k)\u2212log\u2061\u0393\u2062(\u2211k\u03b2k\u2062d)absentsubscript\ud835\udc63\ud835\udc58subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\u0393subscript\ud835\udc58subscript\ud835\udefc\ud835\udc58\u0393subscript\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{v,k}x_{vd}\\phi_{vkd}\\log\\frac{w_{vk}\\widetilde{h}_{kd}}{%\n\\phi_{vkd}}\\,+\\,\\log\\Gamma(\\textstyle{\\sum}_{k}\\alpha_{k})-\\log\\Gamma(%\n\\textstyle{\\sum}_{k}\\beta_{kd})= \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG + roman_log roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - roman_log roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT )\n\n\n+\u2211k(log\u2061\u0393\u2062(\u03b2k\u2062d)\u2212log\u2061\u0393\u2062(\u03b1k)+(\u03b1k\u2212\u03b2k\u2062d)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d]).subscript\ud835\udc58\u0393subscript\ud835\udefd\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udefc\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle\\phantom{=}+\\displaystyle\\sum_{k}\\Big{(}\\log\\Gamma(\\beta_{kd})-%\n\\log\\Gamma(\\alpha_{k})+(\\alpha_{k}-\\beta_{kd})\\mathbb{E}_{q}\\!\\left[\\log h_{kd%\n}\\right]\\Big{)}.+ \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( roman_log roman_\u0393 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - roman_log roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] ) .\n\nFinally, the variational lower bound of the Dirichlet\u2013Poisson model is\n\n\u2112DP\u2062(X|W,\u03b1,\u03b2,\u03d5)=\u2211d\u2112dDP\u2062(xd|W,\u03b1,\u03b2d,\u03d5d),superscript\u2112DPconditional\ud835\udc4b\ud835\udc4a\ud835\udefc\ud835\udefditalic-\u03d5subscript\ud835\udc51subscriptsuperscript\u2112DP\ud835\udc51conditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefcsubscript\ud835\udefd\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\mathcal{L}^{\\text{DP}}(X|W,\\alpha,\\beta,\\phi)=\\sum_{d}\\mathcal{L}^{\\text{DP}}%\n_{d}(x_{d}|W,\\alpha,\\beta_{d},\\phi_{d}),caligraphic_L start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT ( italic_X | italic_W , italic_\u03b1 , italic_\u03b2 , italic_\u03d5 ) = \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT DP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ,\n\nwhich is identical to the variational lower bound (17) of LDA.\n"
        },
        {
            "id": "A4",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix D Equivalent generative models\nThe connections between NMF and PLSA or LDA exist because of the close relationships of their underlying generative models, which we summarize in this section.\n\nWe say that a parameter of a model is trivial if the marginal distribution of the observed variables X\ud835\udc4bXitalic_X has a unique global optimum with respect to this parameter that can be immediately estimated from the observations X\ud835\udc4bXitalic_X. A model parameter that is not trivial is called non-trivial.\n\nWe call two models m1subscript\ud835\udc5a1m_{1}italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and m2subscript\ud835\udc5a2m_{2}italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT equivalent if they have the same non-trivial model parameters \u03b8\ud835\udf03\\thetaitalic_\u03b8 and the marginal distributions of the observed variables X\ud835\udc4bXitalic_X are identical up to multiplicative and additive constants after estimating the trivial parameters, i.e.,\n\np\u03b8m1\u2062(X)=c1\u2062p\u03b8m2\u2062(X)+c2subscriptsuperscript\ud835\udc5dsubscript\ud835\udc5a1\ud835\udf03\ud835\udc4bsubscript\ud835\udc501subscriptsuperscript\ud835\udc5dsubscript\ud835\udc5a2\ud835\udf03\ud835\udc4bsubscript\ud835\udc502p^{m_{1}}_{\\theta}(X)=c_{1}p^{m_{2}}_{\\theta}(X)+c_{2}italic_p start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_X ) = italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT ( italic_X ) + italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT\n\nfor some data-dependent constants c1\u2062(X),c2\u2062(X)\u2208\u211d.subscript\ud835\udc501\ud835\udc4bsubscript\ud835\udc502\ud835\udc4b\u211dc_{1}(X),c_{2}(X)\\in\\mathbb{R}.italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_X ) , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_X ) \u2208 blackboard_R . It is important to note that potentially existing latent variables are allowed to differ.\n\nLemma D.1.\n\nLet the model parameters be W\u2208\u211d+V\u00d7K\ud835\udc4asubscriptsuperscript\u211d\ud835\udc49\ud835\udc3eW\\in\\mathbb{R}^{V\\times K}_{+}italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and h\u2208\u211d+K\u210esubscriptsuperscript\u211d\ud835\udc3eh\\in\\mathbb{R}^{K}_{+}italic_h \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT. Then the following models are equivalent:\n\n(i)\nXv\u223cPoisson\u2061((W\u2062h)v)similar-tosubscript\ud835\udc4b\ud835\udc63Poissonsubscript\ud835\udc4a\u210e\ud835\udc63X_{v}\\sim\\operatorname{Poisson}((Wh)_{v})italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u223c roman_Poisson ( ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )\n\n(ii)\nzv\u2062k\u223cPoisson\u2061(wv\u2062k\u2062hk)similar-tosubscript\ud835\udc67\ud835\udc63\ud835\udc58Poissonsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58z_{vk}\\sim\\operatorname{Poisson}(w_{vk}h_{k})italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) independent \nXv=\u2211kzv\u2062ksubscript\ud835\udc4b\ud835\udc63subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58X_{v}=\\sum_{k}z_{vk}italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT\n\n\n\nProof.\nFor both models, it holds\n\np\u2062(X1=x1,\u2026,XV=xV|W,h)=e\u2212\u2211v(W\u2062h)v\u2062\u220fv1xv!\u2062(W\u2062h)vxv.\ud835\udc5dformulae-sequencesubscript\ud835\udc4b1subscript\ud835\udc651\u2026subscript\ud835\udc4b\ud835\udc49conditionalsubscript\ud835\udc65\ud835\udc49\ud835\udc4a\u210esuperscript\ud835\udc52subscript\ud835\udc63subscript\ud835\udc4a\u210e\ud835\udc63subscriptproduct\ud835\udc631subscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc4a\u210e\ud835\udc63subscript\ud835\udc65\ud835\udc63p(X_{1}=x_{1},\\ldots,X_{V}=x_{V}|W,h)=e^{-\\sum_{v}(Wh)_{v}}\\prod_{v}\\frac{1}{x%\n_{v}!}(Wh)_{v}^{x_{v}}.italic_p ( italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_X start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT | italic_W , italic_h ) = italic_e start_POSTSUPERSCRIPT - \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ! end_ARG ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT .\n(31)\n\n\u220e\n\n\nLemma D.2.\n\nLet the model parameters be W\u2208\u211d+V\u00d7K\ud835\udc4asubscriptsuperscript\u211d\ud835\udc49\ud835\udc3eW\\in\\mathbb{R}^{V\\times K}_{+}italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT, h\u2208\u211d+K\u210esubscriptsuperscript\u211d\ud835\udc3eh\\in\\mathbb{R}^{K}_{+}italic_h \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and the total number of observations N\ud835\udc41Nitalic_N. Then the following models are equivalent:\n\n(i)\n(Xv)v\u223cMultinomial\u2061(N,pv\u221d(W\u2062h)v)similar-tosubscriptsubscript\ud835\udc4b\ud835\udc63\ud835\udc63Multinomial\ud835\udc41proportional-tosubscript\ud835\udc5d\ud835\udc63subscript\ud835\udc4a\u210e\ud835\udc63(X_{v})_{v}\\sim\\operatorname{Multinomial}(N,p_{v}\\propto(Wh)_{v})( italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u223c roman_Multinomial ( italic_N , italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u221d ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )\n\n(ii)\n(zv\u2062k)v\u2062k\u223cMultinomial\u2061(N,pv\u2062k\u221dwv\u2062k\u2062hk)similar-tosubscriptsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc63\ud835\udc58Multinomial\ud835\udc41proportional-tosubscript\ud835\udc5d\ud835\udc63\ud835\udc58subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58(z_{vk})_{vk}\\sim\\operatorname{Multinomial}(N,p_{vk}\\propto w_{vk}h_{k})( italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u223c roman_Multinomial ( italic_N , italic_p start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u221d italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \nXv=\u2211kzv\u2062ksubscript\ud835\udc4b\ud835\udc63subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58X_{v}=\\sum_{k}z_{vk}italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT\n\nAdditionally, the total number of observations N\ud835\udc41Nitalic_N is a trivial model parameter. If the columns of W\ud835\udc4aWitalic_W are normalized, i.e., W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT, then (i) and (ii) are also equivalent to\n\n(iii)\n\u2200n=1,\u2026,Nfor-all\ud835\udc5b1\u2026\ud835\udc41\\forall n=1,\\ldots,N\u2200 italic_n = 1 , \u2026 , italic_N:\n\n\u2022\nzn\u223cCat\u2061(h)similar-tosubscript\ud835\udc67\ud835\udc5bCat\u210ez_{n}\\sim\\operatorname{Cat}(h)italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u223c roman_Cat ( italic_h )\n\n\u2022\n\u03bdn\u223cCat\u2061(wzn)similar-tosubscript\ud835\udf08\ud835\udc5bCatsubscript\ud835\udc64subscript\ud835\udc67\ud835\udc5b\\nu_{n}\\sim\\operatorname{Cat}(w_{z_{n}})italic_\u03bd start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u223c roman_Cat ( italic_w start_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT )\n\nXv=\u2211n\ud835\udfd9\u03bdn=vsubscript\ud835\udc4b\ud835\udc63subscript\ud835\udc5bsubscript1subscript\ud835\udf08\ud835\udc5b\ud835\udc63X_{v}=\\sum_{n}\\mathbbm{1}_{\\nu_{n}=v}italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_\u03bd start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_v end_POSTSUBSCRIPT\n\n\n\nProof.\nFor all three models, it holds\n\np\u2062(X1=x1,\u2026,XV=xV|W,h,N)=\ud835\udfd9\u2211vxv=N\u2062N!\u220fvxv!\u2062\u220fv((W\u2062h)v\u2211v\u2032(W\u2062h)v\u2032)xv.\ud835\udc5dformulae-sequencesubscript\ud835\udc4b1subscript\ud835\udc651\u2026subscript\ud835\udc4b\ud835\udc49conditionalsubscript\ud835\udc65\ud835\udc49\ud835\udc4a\u210e\ud835\udc41subscript1subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc41\ud835\udc41subscriptproduct\ud835\udc63subscript\ud835\udc65\ud835\udc63subscriptproduct\ud835\udc63superscriptsubscript\ud835\udc4a\u210e\ud835\udc63subscriptsuperscript\ud835\udc63\u2032subscript\ud835\udc4a\u210esuperscript\ud835\udc63\u2032subscript\ud835\udc65\ud835\udc63p(X_{1}=x_{1},\\ldots,X_{V}=x_{V}|W,h,N)=\\mathbbm{1}_{\\sum_{v}x_{v}=N}\\frac{N!}%\n{\\prod_{v}x_{v}!}\\prod_{v}\\Big{(}\\frac{(Wh)_{v}}{\\sum_{v^{\\prime}}(Wh)_{v^{%\n\\prime}}}\\Big{)}^{x_{v}}.italic_p ( italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_X start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT | italic_W , italic_h , italic_N ) = blackboard_1 start_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_N end_POSTSUBSCRIPT divide start_ARG italic_N ! end_ARG start_ARG \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ! end_ARG \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( divide start_ARG ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_ARG ) start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT .\n(32)\n\nNote that to see (32) for (ii), the trick is to apply the multinomial theorem\n\n\u2211z\u2208\u21150K\u2211kzk=xx!\u220fkzk!\u2062\u220fkykzk=(\u2211kyk)xsubscript\ud835\udc67superscriptsubscript\u21150\ud835\udc3esubscript\ud835\udc58subscript\ud835\udc67\ud835\udc58\ud835\udc65\ud835\udc65subscriptproduct\ud835\udc58subscript\ud835\udc67\ud835\udc58subscriptproduct\ud835\udc58superscriptsubscript\ud835\udc66\ud835\udc58subscript\ud835\udc67\ud835\udc58superscriptsubscript\ud835\udc58subscript\ud835\udc66\ud835\udc58\ud835\udc65\\sum_{\\begin{subarray}{c}z\\in\\mathbb{N}_{0}^{K}\\\\\n\\sum_{k}z_{k}=x\\end{subarray}}\\frac{x!}{\\prod_{k}z_{k}!}\\prod_{k}y_{k}^{z_{k}}%\n=\\Big{(}\\sum_{k}y_{k}\\Big{)}^{x}\u2211 start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_z \u2208 blackboard_N start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_x end_CELL end_ROW end_ARG end_POSTSUBSCRIPT divide start_ARG italic_x ! end_ARG start_ARG \u220f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ! end_ARG \u220f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT = ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT\n\nfor all x\u2208\u21150\ud835\udc65subscript\u21150x\\in\\mathbb{N}_{0}italic_x \u2208 blackboard_N start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and y\u2208\u211dK\ud835\udc66superscript\u211d\ud835\udc3ey\\in\\mathbb{R}^{K}italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT.\n\u220e\n\n\nLemma D.3.\n\nLet the model parameters be normalized W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT and h\u2208\u0394K\u22121\u210esubscript\u0394\ud835\udc3e1h\\in\\Delta_{K-1}italic_h \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT, or normalized W,h\ud835\udc4a\u210eW,hitalic_W , italic_h and the total number of observations N\ud835\udc41Nitalic_N. Then the following models are equivalent:\n\n(i)\nXv\u223cPoisson\u2061((W\u2062h)v)similar-tosubscript\ud835\udc4b\ud835\udc63Poissonsubscript\ud835\udc4a\u210e\ud835\udc63X_{v}\\sim\\operatorname{Poisson}((Wh)_{v})italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u223c roman_Poisson ( ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )\n\n(ii)\nzv\u2062k\u223cPoisson\u2061(wv\u2062k\u2062hk)similar-tosubscript\ud835\udc67\ud835\udc63\ud835\udc58Poissonsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58z_{vk}\\sim\\operatorname{Poisson}(w_{vk}h_{k})italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) independent \nXv=\u2211kzv\u2062ksubscript\ud835\udc4b\ud835\udc63subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58X_{v}=\\sum_{k}z_{vk}italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT\n\n(iii)\n(Xv)v\u223cMultinomial\u2061(N,pv=(W\u2062h)v)similar-tosubscriptsubscript\ud835\udc4b\ud835\udc63\ud835\udc63Multinomial\ud835\udc41subscript\ud835\udc5d\ud835\udc63subscript\ud835\udc4a\u210e\ud835\udc63(X_{v})_{v}\\sim\\operatorname{Multinomial}(N,p_{v}=(Wh)_{v})( italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u223c roman_Multinomial ( italic_N , italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )\n\n(iv)\n(zv\u2062k)v\u2062k\u223cMultinomial\u2061(N,pv\u2062k=wv\u2062k\u2062hk)similar-tosubscriptsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc63\ud835\udc58Multinomial\ud835\udc41subscript\ud835\udc5d\ud835\udc63\ud835\udc58subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58(z_{vk})_{vk}\\sim\\operatorname{Multinomial}(N,p_{vk}=w_{vk}h_{k})( italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u223c roman_Multinomial ( italic_N , italic_p start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT = italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \nXv=\u2211kzv\u2062ksubscript\ud835\udc4b\ud835\udc63subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58X_{v}=\\sum_{k}z_{vk}italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT\n\n(v)\n\u2200n=1,\u2026,Nfor-all\ud835\udc5b1\u2026\ud835\udc41\\forall n=1,\\ldots,N\u2200 italic_n = 1 , \u2026 , italic_N:\n\n\u2022\nzn\u223cCat\u2061(h)similar-tosubscript\ud835\udc67\ud835\udc5bCat\u210ez_{n}\\sim\\operatorname{Cat}(h)italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u223c roman_Cat ( italic_h )\n\n\u2022\n\u03bdn\u223cCat\u2061(wzn)similar-tosubscript\ud835\udf08\ud835\udc5bCatsubscript\ud835\udc64subscript\ud835\udc67\ud835\udc5b\\nu_{n}\\sim\\operatorname{Cat}(w_{z_{n}})italic_\u03bd start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT \u223c roman_Cat ( italic_w start_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT )\n\nXv=\u2211n\ud835\udfd9\u03bdn=vsubscript\ud835\udc4b\ud835\udc63subscript\ud835\udc5bsubscript1subscript\ud835\udf08\ud835\udc5b\ud835\udc63X_{v}=\\sum_{n}\\mathbbm{1}_{\\nu_{n}=v}italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_\u03bd start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_v end_POSTSUBSCRIPT\n\n\n\nProof.\nWith observed X\ud835\udc4bXitalic_X, trivially estimated N\ud835\udc41Nitalic_N and normalized model parameters, (31) and (32) are identical up to a constant due to \u2211v(W\u2062h)v=1.subscript\ud835\udc63subscript\ud835\udc4a\u210e\ud835\udc631\\sum_{v}(Wh)_{v}=1.\u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = 1 .\n\u220e\n\n"
        },
        {
            "id": "A5",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix E Gamma\u2013Poisson model\nIn this section, we extend the connections between NMF with a normalization constraint on W\ud835\udc4aWitalic_W (18) and NMF with a normalization constraint on both W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H (19) / PLSA presented in Appendix\u00a0B to their Bayesian counterparts: the Gamma\u2013Poisson and the Dirichlet\u2013Poisson / LDA models. We first introduce the Gamma\u2013Poisson model and derive its variational inference algorithm following [7, 6]. Then, we extend the connection between their generative models from [6, Lemma 1] to an algorithmic equivalence of their VI algorithms.\n\nIn the Gamma\u2013Poisson model we consider, the topics W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT are normalized model parameters. The topic weights hdsubscript\u210e\ud835\udc51h_{d}italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT are not normalized and Gamma priors hk\u2062d\u223c\u0393\u2062(\u03b1k,ak)similar-tosubscript\u210e\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58h_{kd}\\sim\\Gamma(\\alpha_{k},a_{k})italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT \u223c roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) for additional model parameters \u03b1,a\u2208\u211d>0K\ud835\udefc\ud835\udc4esubscriptsuperscript\u211d\ud835\udc3eabsent0\\alpha,a\\in\\mathbb{R}^{K}_{>0}italic_\u03b1 , italic_a \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT are introduced. The generative process of each document d\ud835\udc51ditalic_d then becomes\n\n1.\nSample the topic weights hk\u2062d\u223c\u0393\u2062(\u03b1k,ak)similar-tosubscript\u210e\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58h_{kd}\\sim\\Gamma(\\alpha_{k},a_{k})italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT \u223c roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ).\n\n2.\nSample the topic contributions zv\u2062k\u2062d\u223cPoisson\u2061(wv\u2062k\u2062hk\u2062d)similar-tosubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51Poissonsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51z_{vkd}\\sim\\operatorname{Poisson}(w_{vk}h_{kd})italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT \u223c roman_Poisson ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ).\n\n3.\nXv\u2062d=\u2211kzv\u2062k\u2062dsubscript\ud835\udc4b\ud835\udc63\ud835\udc51subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51X_{vd}=\\sum_{k}z_{vkd}italic_X start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT.\n\n\nHere and in the following, \u0393\u2062(\u03b1k,ak)\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58\\Gamma(\\alpha_{k},a_{k})roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) denotes the Gamma distribution with shape parameter \u03b1ksubscript\ud835\udefc\ud835\udc58\\alpha_{k}italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and rate parameter aksubscript\ud835\udc4e\ud835\udc58a_{k}italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. The joint likelihood of the observed counts xv\u2062dsubscript\ud835\udc65\ud835\udc63\ud835\udc51x_{vd}italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT and the latent variables (hd,zd)subscript\u210e\ud835\udc51subscript\ud835\udc67\ud835\udc51(h_{d},z_{d})( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is then given by\n\np\u2062(xd,hd,zd|W,\u03b1)=(\u220fv\ud835\udfd9\u2211kzv\u2062k\u2062d=xv\u2062d)\u2062\u220fkp\u2062(hk\u2062d|\u03b1k,ak)\u2062\u220fv,kp\u2062(zv\u2062k\u2062d|W,hd),\ud835\udc5dsubscript\ud835\udc65\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4a\ud835\udefcsubscriptproduct\ud835\udc63subscript1subscript\ud835\udc58subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptproduct\ud835\udc58\ud835\udc5dconditionalsubscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58subscriptproduct\ud835\udc63\ud835\udc58\ud835\udc5dconditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51p(x_{d},h_{d},z_{d}|W,\\alpha)=\\Big{(}\\prod_{v}\\mathbbm{1}_{\\sum_{k}z_{vkd}=x_{%\nvd}}\\Big{)}\\prod_{k}p(h_{kd}|\\alpha_{k},a_{k})\\prod_{v,k}p(z_{vkd}|W,h_{d}),italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 ) = ( \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) \u220f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_p ( italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT | italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \u220f start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_p ( italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ,\n\nwhere\n\np\u2062(hk\u2062d|\u03b1k,ak)\ud835\udc5dconditionalsubscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58\\displaystyle p(h_{kd}|\\alpha_{k},a_{k})italic_p ( italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT | italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )\n=ak\u03b1k\u0393\u2062(\u03b1k)\u2062hk\u2062d\u03b1k\u22121\u2062e\u2212ak\u2062hk\u2062d,absentsuperscriptsubscript\ud835\udc4e\ud835\udc58subscript\ud835\udefc\ud835\udc58\u0393subscript\ud835\udefc\ud835\udc58superscriptsubscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udefc\ud835\udc581superscript\ud835\udc52subscript\ud835\udc4e\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=\\frac{a_{k}^{\\alpha_{k}}}{\\Gamma(\\alpha_{k})}h_{kd}^{\\alpha_{k}-%\n1}e^{-a_{k}h_{kd}},= divide start_ARG italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ,\n\n\np\u2062(zv\u2062k\u2062d|W,hd)\ud835\udc5dconditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51\\displaystyle p(z_{vkd}|W,h_{d})italic_p ( italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )\n=(wv\u2062k\u2062hk\u2062d)zv\u2062k\u2062d\u2062e\u2212wv\u2062k\u2062hk\u2062dzv\u2062k\u2062d!absentsuperscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51superscript\ud835\udc52subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle=(w_{vk}h_{kd})^{z_{vkd}}\\frac{e^{-w_{vk}h_{kd}}}{z_{vkd}!}= ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG italic_e start_POSTSUPERSCRIPT - italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! end_ARG\n\ndenote the density of the Gamma distribution and probability mass function of the Poisson distribution, respectively. Similar to the Dirichlet\u2013Poisson / LDA model, the introduction of the Gamma prior on the topic weights results in an intractable posterior distribution of the latent variables. We apply variational inference and approximate the true posterior with a variational distribution q\ud835\udc5eqitalic_q given by\n\nq\u2062(h,z|\u03b2,b,\u03d5)\ud835\udc5e\u210econditional\ud835\udc67\ud835\udefd\ud835\udc4fitalic-\u03d5\\displaystyle q(h,z|\\beta,b,\\phi)italic_q ( italic_h , italic_z | italic_\u03b2 , italic_b , italic_\u03d5 )\n=\u220fdqd\u2062(hd,zd|\u03b2d,bd,\u03d5d),absentsubscriptproduct\ud835\udc51subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscript\ud835\udc4f\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle=\\prod_{d}q_{d}(h_{d},z_{d}|\\beta_{d},b_{d},\\phi_{d}),= \u220f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ,\n\n\nqd\u2062(hd,zd|\u03b2d,bd,\u03d5d)subscript\ud835\udc5e\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscript\ud835\udc4f\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle q_{d}(h_{d},z_{d}|\\beta_{d},b_{d},\\phi_{d})italic_q start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )\n=\u220fkq\u2062(hk\u2062d|\u03b2k\u2062d,bk\u2062d)\u2062\u220fvq\u2062(zv\u2062d|xv\u2062d,\u03d5v\u2062d),absentsubscriptproduct\ud835\udc58\ud835\udc5econditionalsubscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51subscriptproduct\ud835\udc63\ud835\udc5econditionalsubscript\ud835\udc67\ud835\udc63\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc51\\displaystyle=\\prod_{k}q(h_{kd}|\\beta_{kd},b_{kd})\\prod_{v}q(z_{vd}|x_{vd},%\n\\phi_{vd}),= \u220f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_q ( italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ) ,\n\nwhere (\u03b2k\u2062d,bk\u2062d)subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51(\\beta_{kd},b_{kd})( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) and \u03d5v\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc51\\phi_{vd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT are Gamma and multinomial parameters, respectively. For completeness, we will now compute the individual terms of the per-document variational lower bound\n\nlog\u2061p\u2062(xd|W,\u03b1,a)\ud835\udc5dconditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefc\ud835\udc4e\\displaystyle\\log p(x_{d}|W,\\alpha,a)roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_a )\n\u2265\ud835\udd3cq\u2062[log\u2061p\u2062(xd,hd,zd|W,\u03b1,a)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(hd,zd|\u03b2d,bd,\u03d5d)]absentsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dsubscript\ud835\udc65\ud835\udc51subscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4a\ud835\udefc\ud835\udc4esubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\u210e\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51subscript\ud835\udefd\ud835\udc51subscript\ud835\udc4f\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle\\geq\\mathbb{E}_{q}\\!\\left[\\log p(x_{d},h_{d},z_{d}|W,\\alpha,a)%\n\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(h_{d},z_{d}|\\beta_{d},b_{d},\\phi_{d})\\right]\u2265 blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_a ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n\n\n=\ud835\udd3cq\u2062[log\u2061p\u2062(hd|\u03b1,a)]+\ud835\udd3cq\u2062[log\u2061p\u2062(xd,zd|W,hd)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(hd)]\u2212\ud835\udd3cq\u2062[log\u2061q\u2062(zd)]absentsubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dconditionalsubscript\u210e\ud835\udc51\ud835\udefc\ud835\udc4esubscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dsubscript\ud835\udc65\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\u210e\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\ud835\udc67\ud835\udc51\\displaystyle=\\mathbb{E}_{q}\\!\\left[\\log p(h_{d}|\\alpha,a)\\right]+\\mathbb{E}_{%\nq}\\!\\left[\\log p(x_{d},z_{d}|W,h_{d})\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(h_{d%\n})\\right]-\\mathbb{E}_{q}\\!\\left[\\log q(z_{d})\\right]= blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b1 , italic_a ) ] + blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ] - blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n\n\n\u2255\u2112dGaP\u2062(xd|W,\u03b1,a,\u03b2d,bd,\u03d5d)\u2255absentsubscriptsuperscript\u2112GaP\ud835\udc51conditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefc\ud835\udc4esubscript\ud835\udefd\ud835\udc51subscript\ud835\udc4f\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle\\eqqcolon\\mathcal{L}^{\\text{GaP}}_{d}(x_{d}|W,\\alpha,a,\\beta_{d},%\nb_{d},\\phi_{d})\u2255 caligraphic_L start_POSTSUPERSCRIPT GaP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_a , italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )\n\nof the Gamma\u2013Poisson model. Plugging in the density of the Gamma distribution, the first and third term are given by\n\n\ud835\udd3cq\u2062[log\u2061p\u2062(hd|\u03b1,a)]subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dconditionalsubscript\u210e\ud835\udc51\ud835\udefc\ud835\udc4e\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log p(h_{d}|\\alpha,a)\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_\u03b1 , italic_a ) ]\n=\u2211k(\u03b1k\u2062log\u2061ak\u2212log\u2061\u0393\u2062(\u03b1k)+(\u03b1k\u22121)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d]\u2212ak\u2062\ud835\udd3cq\u2062[hk\u2062d]),absentsubscript\ud835\udc58subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udefc\ud835\udc581subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc4e\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{k}\\Big{(}\\alpha_{k}\\log a_{k}-\\log\\Gamma(\\alpha_{k})+(%\n\\alpha_{k}-1)\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]-a_{k}\\mathbb{E}_{q}\\!%\n\\left[h_{kd}\\right]\\Big{)},= \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - roman_log roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] - italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] ) ,\n\n\n\ud835\udd3cq\u2062[log\u2061q\u2062(hd)]subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\u210e\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log q(h_{d})\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ]\n=\u2211k(\u03b2k\u2062d\u2062log\u2061bk\u2062d\u2212log\u2061\u0393\u2062(\u03b2k\u2062d)+(\u03b2k\u2062d\u22121)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d]\u2212bk\u2062d\u2062\ud835\udd3cq\u2062[hk\u2062d])absentsubscript\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udefd\ud835\udc58\ud835\udc511subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{k}\\Big{(}\\beta_{kd}\\log b_{kd}-\\log\\Gamma(\\beta_{kd})+(%\n\\beta_{kd}-1)\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]-b_{kd}\\mathbb{E}_{q}\\!%\n\\left[h_{kd}\\right]\\Big{)}= \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT roman_log italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT - roman_log roman_\u0393 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) + ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT - 1 ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] - italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] )\n\nfor\n\n\ud835\udd3cq\u2062[log\u2061hk\u2062d]subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ]\n=\u03c8\u2062(\u03b2k\u2062d)\u2212log\u2061bk\u2062d\u2255log\u2061(h~k\u2062d),absent\ud835\udf13subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51\u2255subscript~\u210e\ud835\udc58\ud835\udc51\\displaystyle=\\psi(\\beta_{kd})-\\log b_{kd}\\eqqcolon\\log(\\widetilde{h}_{kd}),= italic_\u03c8 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - roman_log italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT \u2255 roman_log ( over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) ,\n(33)\n\n\ud835\udd3cq\u2062[hk\u2062d]subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[h_{kd}\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ]\n=\u03b2k\u2062dbk\u2062dabsentsubscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51\\displaystyle=\\frac{\\beta_{kd}}{b_{kd}}= divide start_ARG italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG\n\nand the digamma function \u03c8\ud835\udf13\\psiitalic_\u03c8, cf.\u00a0[6, eq.\u00a0(14)]. Identically to the Dirichlet\u2013Poisson model, the second term reduces to\n\n\ud835\udd3cq\u2062[log\u2061p\u2062(xd,zd|W,hd)]=\u2211v,k\u03d5v\u2062k\u2062d\u2062xv\u2062d\u2062log\u2061(wv\u2062k\u2062h~k\u2062d)\u2212\u2211k\ud835\udd3cq\u2062[hk\u2062d]\u2212\u2211v,k\ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!],subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5dsubscript\ud835\udc65\ud835\udc51conditionalsubscript\ud835\udc67\ud835\udc51\ud835\udc4asubscript\u210e\ud835\udc51subscript\ud835\udc63\ud835\udc58subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51\\displaystyle\\mathbb{E}_{q}\\!\\left[\\log p(x_{d},z_{d}|W,h_{d})\\right]=\\sum_{v,%\nk}\\phi_{vkd}x_{vd}\\log(w_{vk}\\widetilde{h}_{kd})-\\sum_{k}\\mathbb{E}_{q}\\!\\left%\n[h_{kd}\\right]-\\sum_{v,k}\\mathbb{E}_{q}\\!\\left[\\log z_{vkd}!\\right],blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ] = \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log ( italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ] ,\n\nand the last term is given by\n\n\ud835\udd3cq\u2062[log\u2061q\u2062(zd)]=\u2211vlog\u2061xv\u2062d!\u2212\u2211v,k\ud835\udd3cq\u2062[log\u2061zv\u2062k\u2062d!]+\u2211v,k\u03d5v\u2062k\u2062d\u2062xv\u2062d\u2062log\u2061\u03d5v\u2062k\u2062d.subscript\ud835\udd3c\ud835\udc5edelimited-[]\ud835\udc5esubscript\ud835\udc67\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscript\ud835\udc63\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\ud835\udc67\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc63\ud835\udc58subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\mathbb{E}_{q}\\!\\left[\\log q(z_{d})\\right]=\\sum_{v}\\log x_{vd}!-\\sum_{v,k}%\n\\mathbb{E}_{q}\\!\\left[\\log z_{vkd}!\\right]+\\sum_{v,k}\\phi_{vkd}x_{vd}\\log\\phi_%\n{vkd}.blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_q ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ] = \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT roman_log italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT ! - \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_z start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT ! ] + \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT roman_log italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT .\n\nNotice that in contrast to the Dirichlet\u2013Poisson model, the sum \u2211k\ud835\udd3cq\u2062[hk\u2062d]subscript\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\sum_{k}\\mathbb{E}_{q}\\!\\left[h_{kd}\\right]\u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] is not a constant because the topic weights are not normalized. When combining all terms except data-dependent constants, the per-document variational lower bound simplifies to\n\n\u2112dGaPsubscriptsuperscript\u2112GaP\ud835\udc51\\displaystyle\\mathcal{L}^{\\text{GaP}}_{d}caligraphic_L start_POSTSUPERSCRIPT GaP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT\n(xd|W,\u03b1,a,\u03b2d,bd,\u03d5d)conditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefc\ud835\udc4esubscript\ud835\udefd\ud835\udc51subscript\ud835\udc4f\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\displaystyle(x_{d}|W,\\alpha,a,\\beta_{d},b_{d},\\phi_{d})( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_a , italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )\n\n\n=\u2211v,kxv\u2062d\u2062\u03d5v\u2062k\u2062d\u2062log\u2061wv\u2062k\u2062h~k\u2062d\u03d5v\u2062k\u2062d\u2212\u2211k\ud835\udd3cq\u2062[hk\u2062d]+\u2211k(\u03b1k\u2062log\u2061ak\u2212\u03b2k\u2062d\u2062log\u2061bk\u2062d)absentsubscript\ud835\udc63\ud835\udc58subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc58subscript\ud835\udefc\ud835\udc58subscript\ud835\udc4e\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51\\displaystyle=\\sum_{v,k}x_{vd}\\phi_{vkd}\\log\\frac{w_{vk}\\widetilde{h}_{kd}}{%\n\\phi_{vkd}}-\\sum_{k}\\mathbb{E}_{q}\\!\\left[h_{kd}\\right]+\\sum_{k}\\Big{(}\\alpha_%\n{k}\\log a_{k}-\\beta_{kd}\\log b_{kd}\\Big{)}= \u2211 start_POSTSUBSCRIPT italic_v , italic_k end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT end_ARG - \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] + \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT roman_log italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT )\n\n\n+\u2211k(log\u2061\u0393\u2062(\u03b2k\u2062d)\u2212log\u2061\u0393\u2062(\u03b1k)+(\u03b1k\u2212\u03b2k\u2062d)\u2062\ud835\udd3cq\u2062[log\u2061hk\u2062d]+(bk\u2062d\u2212ak)\u2062\ud835\udd3cq\u2062[hk\u2062d]).subscript\ud835\udc58\u0393subscript\ud835\udefd\ud835\udc58\ud835\udc51\u0393subscript\ud835\udefc\ud835\udc58subscript\ud835\udefc\ud835\udc58subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51subscript\ud835\udc4f\ud835\udc58\ud835\udc51subscript\ud835\udc4e\ud835\udc58subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle\\phantom{=}+\\sum_{k}\\Big{(}\\log\\Gamma(\\beta_{kd})-\\log\\Gamma(%\n\\alpha_{k})+(\\alpha_{k}-\\beta_{kd})\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]+(b%\n_{kd}-a_{k})\\mathbb{E}_{q}\\!\\left[h_{kd}\\right]\\Big{)}.+ \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( roman_log roman_\u0393 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) - roman_log roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) + ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] + ( italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT - italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] ) .\n\nFinally, the variational lower bound of the Gamma\u2013Poisson model is\n\n\u2112GaP\u2062(X|W,\u03b1,a,\u03b2,b,\u03d5)=\u2211d\u2112dGaP\u2062(xd|W,\u03b1,a,\u03b2d,bd,\u03d5d).superscript\u2112GaPconditional\ud835\udc4b\ud835\udc4a\ud835\udefc\ud835\udc4e\ud835\udefd\ud835\udc4fitalic-\u03d5subscript\ud835\udc51subscriptsuperscript\u2112GaP\ud835\udc51conditionalsubscript\ud835\udc65\ud835\udc51\ud835\udc4a\ud835\udefc\ud835\udc4esubscript\ud835\udefd\ud835\udc51subscript\ud835\udc4f\ud835\udc51subscriptitalic-\u03d5\ud835\udc51\\mathcal{L}^{\\text{GaP}}(X|W,\\alpha,a,\\beta,b,\\phi)=\\sum_{d}\\mathcal{L}^{\\text%\n{GaP}}_{d}(x_{d}|W,\\alpha,a,\\beta_{d},b_{d},\\phi_{d}).caligraphic_L start_POSTSUPERSCRIPT GaP end_POSTSUPERSCRIPT ( italic_X | italic_W , italic_\u03b1 , italic_a , italic_\u03b2 , italic_b , italic_\u03d5 ) = \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT caligraphic_L start_POSTSUPERSCRIPT GaP end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_W , italic_\u03b1 , italic_a , italic_\u03b2 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_\u03d5 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) .\n\n\nWe again assume that the prior parameters (\u03b1,a)\ud835\udefc\ud835\udc4e(\\alpha,a)( italic_\u03b1 , italic_a ) are fixed during model training. Optimizing the variational lower bound with respect to the topics W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT and the variational parameters \u03d5v\u2062d\u2208\u0394K\u22121subscriptitalic-\u03d5\ud835\udc63\ud835\udc51subscript\u0394\ud835\udc3e1\\phi_{vd}\\in\\Delta_{K-1}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT works identically to the derivation of the VI algorithm of the Dirichlet\u2013Poisson / LDA model, and their update equations are given by wv\u2062k\u221d\u2211dxv\u2062d\u2062\u03d5v\u2062k\u2062dproportional-tosubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51w_{vk}\\propto\\sum_{d}x_{vd}\\phi_{vkd}italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u221d \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT and \u03d5v\u2062k\u2062d\u221dwv\u2062k\u2062h~k\u2062dproportional-tosubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc64\ud835\udc63\ud835\udc58subscript~\u210e\ud835\udc58\ud835\udc51\\phi_{vkd}\\propto w_{vk}\\widetilde{h}_{kd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT \u221d italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT. Moreover, setting the derivatives of the variational lower bound with respect to \u03b2k\u2062dsubscript\ud835\udefd\ud835\udc58\ud835\udc51\\beta_{kd}italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT and bk\u2062dsubscript\ud835\udc4f\ud835\udc58\ud835\udc51b_{kd}italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT to zero is equivalent to\n\n(\u03c8\u2032\u2062(\u03b2k\u2062d)\u22121bk\u2062d\u22121bk\u2062d\u03b2k\u2062dbk\u2062d2)\u2062(\u03b1k+\u2211vxv\u2062d\u2062\u03d5v\u2062k\u2062d\u2212\u03b2k\u2062dak+1\u2212bk\u2062d)=(00),matrixsuperscript\ud835\udf13\u2032subscript\ud835\udefd\ud835\udc58\ud835\udc511subscript\ud835\udc4f\ud835\udc58\ud835\udc511subscript\ud835\udc4f\ud835\udc58\ud835\udc51subscript\ud835\udefd\ud835\udc58\ud835\udc51superscriptsubscript\ud835\udc4f\ud835\udc58\ud835\udc512matrixsubscript\ud835\udefc\ud835\udc58subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udc4e\ud835\udc581subscript\ud835\udc4f\ud835\udc58\ud835\udc51matrix00\\begin{pmatrix}\\psi^{\\prime}(\\beta_{kd})&-\\frac{1}{b_{kd}}\\\\\n-\\frac{1}{b_{kd}}&\\frac{\\beta_{kd}}{b_{kd}^{2}}\\end{pmatrix}\\begin{pmatrix}%\n\\alpha_{k}+\\sum_{v}x_{vd}\\phi_{vkd}-\\beta_{kd}\\\\\na_{k}+1-b_{kd}\\end{pmatrix}=\\begin{pmatrix}0\\\\\n0\\end{pmatrix},( start_ARG start_ROW start_CELL italic_\u03c8 start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ) end_CELL start_CELL - divide start_ARG 1 end_ARG start_ARG italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG end_CELL end_ROW start_ROW start_CELL - divide start_ARG 1 end_ARG start_ARG italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG end_CELL start_CELL divide start_ARG italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_CELL end_ROW end_ARG ) ( start_ARG start_ROW start_CELL italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT - italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + 1 - italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ) = ( start_ARG start_ROW start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL end_ROW end_ARG ) ,\n\nwhich yields the update equations \u03b2k\u2062d=\u03b1k+\u2211vxv\u2062d\u2062\u03d5v\u2062k\u2062dsubscript\ud835\udefd\ud835\udc58\ud835\udc51subscript\ud835\udefc\ud835\udc58subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\beta_{kd}=\\alpha_{k}+\\sum_{v}x_{vd}\\phi_{vkd}italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT and bk\u2062d=1+aksubscript\ud835\udc4f\ud835\udc58\ud835\udc511subscript\ud835\udc4e\ud835\udc58b_{kd}=1+a_{k}italic_b start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = 1 + italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. Rewriting the updates involving \u03d5italic-\u03d5\\phiitalic_\u03d5 in the familiar multiplicative form, we obtain Algorithm\u00a07. Notice that for fixed prior parameters (\u03b1,a)\ud835\udefc\ud835\udc4e(\\alpha,a)( italic_\u03b1 , italic_a ) and aside from the trivially estimated variational parameters b\ud835\udc4fbitalic_b, the only difference between Algorithm\u00a07 and the Dirichlet\u2013Poisson / LDA Algorithm\u00a05 is the explicit formula for \ud835\udd3cq\u2062[log\u2061hk\u2062d].subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right].blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] .\n\nBefore concretizing the algorithmic equivalence of the Gamma\u2013Poisson and Dirichlet\u2013Poisson / LDA VI algorithms, let us recall that the Gamma\u2013Poisson model with rate parameters ak=asubscript\ud835\udc4e\ud835\udc58\ud835\udc4ea_{k}=aitalic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_a extends the generative model of LDA by also modeling the total counts of words in a document.\n\nLemma E.1 ([6, Lemma 1]).\n\nLet \u03b1\u2208\u211d>0K\ud835\udefcsubscriptsuperscript\u211d\ud835\udc3eabsent0\\alpha\\in\\mathbb{R}^{K}_{>0}italic_\u03b1 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT and ak=a>0subscript\ud835\udc4e\ud835\udc58\ud835\udc4e0a_{k}=a>0italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_a > 0 be fixed prior parameters. Let the model parameter be W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT. Then the following two models are equivalent:\n\n(i)\nhk\u223c\u0393\u2062(\u03b1k,a)similar-tosubscript\u210e\ud835\udc58\u0393subscript\ud835\udefc\ud835\udc58\ud835\udc4eh_{k}\\sim\\Gamma(\\alpha_{k},a)italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u223c roman_\u0393 ( italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a ) independent, \nXv=Poisson\u2061((W\u2062h)v)subscript\ud835\udc4b\ud835\udc63Poissonsubscript\ud835\udc4a\u210e\ud835\udc63X_{v}=\\operatorname{Poisson}((Wh)_{v})italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = roman_Poisson ( ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ).\n\n(ii)\nh\u223cDir\u2061(\u03b1)similar-to\u210eDir\ud835\udefch\\sim\\operatorname{Dir}(\\alpha)italic_h \u223c roman_Dir ( italic_\u03b1 ), \nN\u223cPoisson\u2212\u2212Gamma\u2061(\u2211k\u03b1k,a)N\\sim\\operatorname{Poisson--Gamma}(\\sum_{k}\\alpha_{k},a)italic_N \u223c start_OPFUNCTION roman_Poisson - - roman_Gamma end_OPFUNCTION ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a ), \nX\u223cMultinomial\u2061(N,p=W\u2062h)similar-to\ud835\udc4bMultinomial\ud835\udc41\ud835\udc5d\ud835\udc4a\u210eX\\sim\\operatorname{Multinomial}(N,p=Wh)italic_X \u223c roman_Multinomial ( italic_N , italic_p = italic_W italic_h ).\n\n\n\nProof.\nFor simplicity, we denote the value of the density of the Dirichlet distribution at h\u2208\u0394K\u22121\u210esubscript\u0394\ud835\udc3e1h\\in\\Delta_{K-1}italic_h \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT by Dir\u2061(\u03b1)\u2062(h)Dir\ud835\udefc\u210e\\operatorname{Dir}(\\alpha)(h)roman_Dir ( italic_\u03b1 ) ( italic_h ), and we use the analogous notation for the probability densities of the Gamma, Poisson, Poisson\u2013Gamma and multinomial distributions, respectively. We also use the abbreviations PoisPois\\operatorname{Pois}roman_Pois and \u2133\u2133\\mathcal{M}caligraphic_M to refer to the Poisson and multinomial distributions. With this notation, one can show\n\np\u2062(x|W,\u03b1,a)=(\u222b\u0394K\u22121Dir\u2061(\u03b1)\u2062(h)\u2062\u2133\u2062(\u2211vxv,p=W\u2062h)\u2062(x)\u2062\ud835\udc51h)\u2062(\u222b\u211d>0Pois\u2061(\u03bb)\u2062(\u2211vxv)\u2062\u0393\u2062(\u2211k\u03b1k,a)\u2062(\u03bb)\u2062\ud835\udc51\u03bb)=(\u222b\u0394K\u22121Dir\u2061(\u03b1)\u2062(h)\u2062\u2133\u2062(\u2211vxv,p=W\u2062h)\u2062(x)\u2062\ud835\udc51h)\u2062Poisson\u2212\u2212Gamma\u2061(\u2211k\u03b1k,a)\u2062(\u2211vxv)=c\u2062(x,\u2211k\u03b1k,a)\u2062(\u222b\u0394K\u22121Dir\u2061(\u03b1)\u2062(h)\u2062\u220fvPois\u2061((W\u2062h)v)\u2062(xv)\u2062d\u2062h)\\begin{split}&p(x|W,\\alpha,a)\\\\\n&=\\bigg{(}\\int_{\\Delta_{K-1}}\\operatorname{Dir}(\\alpha)(h)\\mathcal{M}(%\n\\textstyle\\sum_{v}x_{v},p=Wh)(x)dh\\bigg{)}\\bigg{(}\\displaystyle\\int_{\\mathbb{R%\n}_{>0}}\\operatorname{Pois}(\\lambda)(\\textstyle\\sum_{v}x_{v})\\Gamma(\\textstyle%\n\\sum_{k}\\alpha_{k},a)(\\lambda)d\\lambda\\bigg{)}\\\\\n&=\\bigg{(}\\int_{\\Delta_{K-1}}\\operatorname{Dir}(\\alpha)(h)\\mathcal{M}(%\n\\textstyle\\sum_{v}x_{v},p=Wh)(x)\\,dh\\bigg{)}\\operatorname{Poisson--Gamma}(%\n\\textstyle\\sum_{k}\\alpha_{k},a)(\\textstyle\\sum_{v}x_{v})\\\\\n&=c(x,\\textstyle\\sum_{k}\\alpha_{k},a)\\bigg{(}\\displaystyle\\int_{\\Delta_{K-1}}%\n\\operatorname{Dir}(\\alpha)(h)\\prod_{v}\\operatorname{Pois}((Wh)_{v})(x_{v})\\,dh%\n\\bigg{)}\\end{split}start_ROW start_CELL end_CELL start_CELL italic_p ( italic_x | italic_W , italic_\u03b1 , italic_a ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = ( \u222b start_POSTSUBSCRIPT roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Dir ( italic_\u03b1 ) ( italic_h ) caligraphic_M ( \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_p = italic_W italic_h ) ( italic_x ) italic_d italic_h ) ( \u222b start_POSTSUBSCRIPT blackboard_R start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Pois ( italic_\u03bb ) ( \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) roman_\u0393 ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a ) ( italic_\u03bb ) italic_d italic_\u03bb ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = ( \u222b start_POSTSUBSCRIPT roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Dir ( italic_\u03b1 ) ( italic_h ) caligraphic_M ( \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_p = italic_W italic_h ) ( italic_x ) italic_d italic_h ) start_OPFUNCTION roman_Poisson - - roman_Gamma end_OPFUNCTION ( \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a ) ( \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = italic_c ( italic_x , \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a ) ( \u222b start_POSTSUBSCRIPT roman_\u0394 start_POSTSUBSCRIPT italic_K - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_Dir ( italic_\u03b1 ) ( italic_h ) \u220f start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT roman_Pois ( ( italic_W italic_h ) start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) italic_d italic_h ) end_CELL end_ROW\n(34)\n\nfor both generative models. The details of the proof are based on the fact that normalized independent Gamma-distributed random variables with identical rate parameter follow a Dirichlet distribution, see [6, Lemma 1].\n\u220e\n\nThe Bayesian analogue of Lemma\u00a04.2 is the following result.\n\nCorollary E.2.\n\nLet \u03b1\u2208\u211d>0K\ud835\udefcsubscriptsuperscript\u211d\ud835\udc3eabsent0\\alpha\\in\\mathbb{R}^{K}_{>0}italic_\u03b1 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT and ak=a>0subscript\ud835\udc4e\ud835\udc58\ud835\udc4e0a_{k}=a>0italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_a > 0 be fixed prior parameters. Then the Gamma\u2013Poisson and Dirichlet\u2013Poisson / LDA likelihoods have the same optima W\u2208\u0394V\u22121K.\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}.italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT .\n\nProof.\nBy (34), the likelihoods are identical up to a constant c\u2062(X,\u2211k\u03b1k,a)\ud835\udc50\ud835\udc4bsubscript\ud835\udc58subscript\ud835\udefc\ud835\udc58\ud835\udc4ec(X,\\textstyle\\sum_{k}\\alpha_{k},a)italic_c ( italic_X , \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a ).\n\u220e\n\nDenoting the n\ud835\udc5bnitalic_n-th iterate of the Gamma\u2013Poisson Algorithm\u00a07 by (W(n),GaP,\u03b2(n),GaP)superscript\ud835\udc4a\ud835\udc5bGaPsuperscript\ud835\udefd\ud835\udc5bGaP(W^{(n),\\text{GaP}},\\beta^{(n),\\text{GaP}})( italic_W start_POSTSUPERSCRIPT ( italic_n ) , GaP end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) , GaP end_POSTSUPERSCRIPT ) and the n\ud835\udc5bnitalic_n-th iterate of Dirichlet\u2013Poisson / LDA Algorithm\u00a05 by (W(n),LDA,\u03b2(n),LDA)superscript\ud835\udc4a\ud835\udc5bLDAsuperscript\ud835\udefd\ud835\udc5bLDA(W^{(n),\\text{LDA}},\\beta^{(n),\\text{LDA}})( italic_W start_POSTSUPERSCRIPT ( italic_n ) , LDA end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) , LDA end_POSTSUPERSCRIPT ), the Bayesian analogue of Lemma\u00a0B.1 is the following result.\n\nLemma E.3.\n\nLet X\u2208\u211d+V\u00d7D\ud835\udc4bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}^{V\\times D}_{+}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and let \u03b1\u2208\u211d>0K,ak=a>0formulae-sequence\ud835\udefcsubscriptsuperscript\u211d\ud835\udc3eabsent0subscript\ud835\udc4e\ud835\udc58\ud835\udc4e0\\alpha\\in\\mathbb{R}^{K}_{>0},\\,a_{k}=a>0italic_\u03b1 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_a > 0 be fixed prior parameters. Let \u03bbd=\u2211k\u2032\u03b1k\u2032+\u2211vxv\u2062dsubscript\ud835\udf06\ud835\udc51subscriptsuperscript\ud835\udc58\u2032subscript\ud835\udefcsuperscript\ud835\udc58\u2032subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51\\lambda_{d}=\\sum_{k^{\\prime}}\\alpha_{k^{\\prime}}+\\sum_{v}x_{vd}italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_\u03b1 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT + \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT. Let W(0)\u2208\u0394V\u22121Ksuperscript\ud835\udc4a0superscriptsubscript\u0394\ud835\udc491\ud835\udc3eW^{(0)}\\in\\Delta_{V-1}^{K}italic_W start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT and \u03b2(0)\u2208\u211d+K\u00d7Dsuperscript\ud835\udefd0superscriptsubscript\u211d\ud835\udc3e\ud835\udc37\\beta^{(0)}\\in\\mathbb{R}_{+}^{K\\times D}italic_\u03b2 start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT be arbitrary initial values. Then for all n\u22651\ud835\udc5b1n\\geq 1italic_n \u2265 1\n\nW(n),GaPsuperscript\ud835\udc4a\ud835\udc5bGaP\\displaystyle W^{(n),\\text{GaP}}italic_W start_POSTSUPERSCRIPT ( italic_n ) , GaP end_POSTSUPERSCRIPT\n=W(n),LDA,absentsuperscript\ud835\udc4a\ud835\udc5bLDA\\displaystyle=W^{(n),\\text{LDA}},= italic_W start_POSTSUPERSCRIPT ( italic_n ) , LDA end_POSTSUPERSCRIPT ,\n\n\n\u03b2(n),GaPsuperscript\ud835\udefd\ud835\udc5bGaP\\displaystyle\\beta^{(n),\\text{GaP}}italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) , GaP end_POSTSUPERSCRIPT\n=\u03b2(n),LDA=\u03b2(n),absentsuperscript\ud835\udefd\ud835\udc5bLDAsuperscript\ud835\udefd\ud835\udc5b\\displaystyle=\\beta^{(n),\\text{LDA}}=\\beta^{(n)},= italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) , LDA end_POSTSUPERSCRIPT = italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ,\n\n\n\u2211k\u03b2k\u2062d(n)subscript\ud835\udc58subscriptsuperscript\ud835\udefd\ud835\udc5b\ud835\udc58\ud835\udc51\\displaystyle\\sum_{k}\\beta^{(n)}_{kd}\u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT\n=\u03bbd.absentsubscript\ud835\udf06\ud835\udc51\\displaystyle=\\lambda_{d}.= italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT .\n\n\n\nProof.\nThe key observation is that the expressions (30) and (33) of \ud835\udd3cq\u2062[log\u2061hk\u2062d]subscript\ud835\udd3c\ud835\udc5edelimited-[]subscript\u210e\ud835\udc58\ud835\udc51\\mathbb{E}_{q}\\!\\left[\\log h_{kd}\\right]blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ roman_log italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ] in the LDA and Gamma\u2013Poisson Algorithms\u00a07 and\u00a05 are identical up to a constant independent of k\ud835\udc58kitalic_k. More precisely, we have\n\nh~k\u2062d(0),GaPsuperscriptsubscript~\u210e\ud835\udc58\ud835\udc510GaP\\displaystyle\\widetilde{h}_{kd}^{(0),\\text{GaP}}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) , GaP end_POSTSUPERSCRIPT\n=exp\u2061(\u03c8\u2062(\u03b2k\u2062d(0)))1+a,absent\ud835\udf13superscriptsubscript\ud835\udefd\ud835\udc58\ud835\udc5101\ud835\udc4e\\displaystyle=\\frac{\\exp(\\psi(\\beta_{kd}^{(0)}))}{1+a},= divide start_ARG roman_exp ( italic_\u03c8 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) end_ARG start_ARG 1 + italic_a end_ARG ,\n\n\nh~k\u2062d(0),LDAsuperscriptsubscript~\u210e\ud835\udc58\ud835\udc510LDA\\displaystyle\\widetilde{h}_{kd}^{(0),\\text{LDA}}over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) , LDA end_POSTSUPERSCRIPT\n=exp\u2061(\u03c8\u2062(\u03b2k\u2062d(0)))exp\u2061(\u03c8\u2062(\u2211k\u2032\u03b2k\u2032\u2062d(0))).absent\ud835\udf13superscriptsubscript\ud835\udefd\ud835\udc58\ud835\udc510\ud835\udf13subscriptsuperscript\ud835\udc58\u2032superscriptsubscript\ud835\udefdsuperscript\ud835\udc58\u2032\ud835\udc510\\displaystyle=\\frac{\\exp(\\psi(\\beta_{kd}^{(0)}))}{\\exp(\\psi(\\sum_{k^{\\prime}}%\n\\beta_{k^{\\prime}d}^{(0)}))}.= divide start_ARG roman_exp ( italic_\u03c8 ( italic_\u03b2 start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) end_ARG start_ARG roman_exp ( italic_\u03c8 ( \u2211 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT ) ) end_ARG .\n\nConsequently, the first iterate of the variational parameters \u03d5v\u2062k\u2062dsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\\phi_{vkd}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT is identical for the GaP and LDA algorithm, which implies (W(1),GaP,\u03b2(1),GaP)=(W(1),LDA,\u03b2(1),LDA)superscript\ud835\udc4a1GaPsuperscript\ud835\udefd1GaPsuperscript\ud835\udc4a1LDAsuperscript\ud835\udefd1LDA(W^{(1),\\text{GaP}},\\beta^{(1),\\text{GaP}})=(W^{(1),\\text{LDA}},\\beta^{(1),%\n\\text{LDA}})( italic_W start_POSTSUPERSCRIPT ( 1 ) , GaP end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT ( 1 ) , GaP end_POSTSUPERSCRIPT ) = ( italic_W start_POSTSUPERSCRIPT ( 1 ) , LDA end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT ( 1 ) , LDA end_POSTSUPERSCRIPT ). The induction step and \u2211k\u03b2k\u2062d(n)=\u03bbdsubscript\ud835\udc58subscriptsuperscript\ud835\udefd\ud835\udc5b\ud835\udc58\ud835\udc51subscript\ud835\udf06\ud835\udc51\\sum_{k}\\beta^{(n)}_{kd}=\\lambda_{d}\u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_\u03b2 start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = italic_\u03bb start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT follow immediately.\n\u220e\n\nFinally, the Bayesian analogue of Corollary\u00a0B.2 is the following result.\n\nCorollary E.4.\n\nLet X\u2208\u211d+V\u00d7D\ud835\udc4bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}^{V\\times D}_{+}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and let \u03b1\u2208\u211d>0K,ak=a>0formulae-sequence\ud835\udefcsubscriptsuperscript\u211d\ud835\udc3eabsent0subscript\ud835\udc4e\ud835\udc58\ud835\udc4e0\\alpha\\in\\mathbb{R}^{K}_{>0},\\,a_{k}=a>0italic_\u03b1 \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT > 0 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_a > 0 be fixed prior parameters. Let (W\u2217,\u03b2\u2217,b\u2217,\u03d5\u2217)superscript\ud835\udc4asuperscript\ud835\udefdsuperscript\ud835\udc4fsuperscriptitalic-\u03d5(W^{*},\\beta^{*},b^{*},\\phi^{*})( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) be a fixed point the Gamma\u2013Poisson VI algorithm. Then bk\u2062d\u2217=1+asubscriptsuperscript\ud835\udc4f\ud835\udc58\ud835\udc511\ud835\udc4eb^{*}_{kd}=1+aitalic_b start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = 1 + italic_a for all k,d\ud835\udc58\ud835\udc51k,ditalic_k , italic_d and (W\u2217,\u03b2\u2217,\u03d5\u2217)superscript\ud835\udc4asuperscript\ud835\udefdsuperscriptitalic-\u03d5(W^{*},\\beta^{*},\\phi^{*})( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) is a fixed point of the Dirichlet\u2013Poisson / LDA VI algorithm. Conversely, let (W\u2217,\u03b2\u2217,\u03d5\u2217)superscript\ud835\udc4asuperscript\ud835\udefdsuperscriptitalic-\u03d5(W^{*},\\beta^{*},\\phi^{*})( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) be a fixed point of the Dirichlet\u2013Poisson / LDA VI algorithm. Then (W\u2217,\u03b2\u2217,b\u2217,\u03d5\u2217)superscript\ud835\udc4asuperscript\ud835\udefdsuperscript\ud835\udc4fsuperscriptitalic-\u03d5(W^{*},\\beta^{*},b^{*},\\phi^{*})( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03b2 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_b start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_\u03d5 start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) with bk\u2062d\u2217=1+asubscriptsuperscript\ud835\udc4f\ud835\udc58\ud835\udc511\ud835\udc4eb^{*}_{kd}=1+aitalic_b start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = 1 + italic_a for all k,d\ud835\udc58\ud835\udc51k,ditalic_k , italic_d is a fixed point of the Gamma\u2013Poisson VI algorithm.\n\nProof.\nFollows from Lemma\u00a0E.3 and the update equations of b\ud835\udc4fbitalic_b and \u03d5italic-\u03d5\\phiitalic_\u03d5.\n\u220e\n\n"
        },
        {
            "id": "A6",
            "type": "text",
            "title": "",
            "caption": "",
            "metadata": {},
            "text": "\nAppendix F Sparse NMF\nIn this section, we first provide details to the derivation of the joint update rules for Lasso-penalized NMF with the KL divergence and \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraints on columns of W\ud835\udc4aWitalic_W. We then prove that Lasso-penalized NMF with the KL divergence and an \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraint on W\ud835\udc4aWitalic_W (24) does not induce any sparsity. Finally, we generalize [34, Lemma 1 and 2] and show that any normalization constraints on the columns of W and penalty terms on H are competing regularizers on the two matrices.\n\nLet us denote the objective function of \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-penalized NMF with the KL divergence and hyperparameter \u03bb>0\ud835\udf060\\lambda>0italic_\u03bb > 0 by\n\nDKL\u03bb(X||WH)\u2254DKL(X||WH)+\u03bb\u2225H\u22251.D_{\\text{KL}}^{\\lambda}(X\\,||\\,WH)\\coloneqq D_{\\text{KL}}(X\\,||\\,WH)+\\lambda\\|%\nH\\|_{1}.italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W italic_H ) \u2254 italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .\n\n\nUsing Corollary\u00a0A.3 and W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT, a joint auxiliary function of DKL\u03bbsuperscriptsubscript\ud835\udc37KL\ud835\udf06D_{\\text{KL}}^{\\lambda}italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT is given by\n\n\u03d5v\u2062k\u2062d(n)superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5b\\displaystyle\\phi_{vkd}^{(n)}italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT\n=wv\u2062k(n)\u2062hk\u2062d(n)(W(n)\u2062H(n))v\u2062d,absentsuperscriptsubscript\ud835\udc64\ud835\udc63\ud835\udc58\ud835\udc5bsuperscriptsubscript\u210e\ud835\udc58\ud835\udc51\ud835\udc5bsubscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51\\displaystyle=\\frac{w_{vk}^{(n)}h_{kd}^{(n)}}{(W^{(n)}H^{(n)})_{vd}},= divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG ,\n\n\nG\u2062((W,H),(W(n),H(n)))\ud835\udc3a\ud835\udc4a\ud835\udc3bsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\\displaystyle G((W,H),(W^{(n)},H^{(n)}))italic_G ( ( italic_W , italic_H ) , ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) )\n=\u2212\u2211v,k,dxv\u2062d\u2062\u03d5v\u2062k\u2062d(n)\u2062log\u2061wv\u2062k\u2062hk\u2062d\u03d5v\u2062k\u2062d(n)+(1+\u03bb)\u2062\u2211k,dhk\u2062d.absentsubscript\ud835\udc63\ud835\udc58\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5bsubscript\ud835\udc64\ud835\udc63\ud835\udc58subscript\u210e\ud835\udc58\ud835\udc51superscriptsubscriptitalic-\u03d5\ud835\udc63\ud835\udc58\ud835\udc51\ud835\udc5b1\ud835\udf06subscript\ud835\udc58\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51\\displaystyle=-\\sum_{v,k,d}x_{vd}\\phi_{vkd}^{(n)}\\log\\frac{w_{vk}h_{kd}}{\\phi_%\n{vkd}^{(n)}}+(1+\\lambda)\\sum_{k,d}h_{kd}.= - \u2211 start_POSTSUBSCRIPT italic_v , italic_k , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT roman_log divide start_ARG italic_w start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG italic_\u03d5 start_POSTSUBSCRIPT italic_v italic_k italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT end_ARG + ( 1 + italic_\u03bb ) \u2211 start_POSTSUBSCRIPT italic_k , italic_d end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT .\n\nApplying the method of Lagrange multipliers, the auxiliary function can be jointly optimized in W\ud835\udc4aWitalic_W and H\ud835\udc3bHitalic_H, and the objective function value is non-increasing under the following joint updates:\n\nwv\u2062k(n+1)\u221dwv\u2062k(n)\u2062\u2211dxv\u2062d\u2062hk\u2062d(n)(W(n)\u2062H(n))v\u2062d,hk\u2062d(n+1)=11+\u03bb\u2062hk\u2062d(n)\u2062\u2211vxv\u2062d\u2062wv\u2062k(n)(W(n)\u2062H(n))v\u2062d.formulae-sequenceproportional-tosubscriptsuperscript\ud835\udc64\ud835\udc5b1\ud835\udc63\ud835\udc58subscriptsuperscript\ud835\udc64\ud835\udc5b\ud835\udc63\ud835\udc58subscript\ud835\udc51subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptsuperscript\u210e\ud835\udc5b\ud835\udc58\ud835\udc51subscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51subscriptsuperscript\u210e\ud835\udc5b1\ud835\udc58\ud835\udc5111\ud835\udf06subscriptsuperscript\u210e\ud835\udc5b\ud835\udc58\ud835\udc51subscript\ud835\udc63subscript\ud835\udc65\ud835\udc63\ud835\udc51subscriptsuperscript\ud835\udc64\ud835\udc5b\ud835\udc63\ud835\udc58subscriptsuperscript\ud835\udc4a\ud835\udc5bsuperscript\ud835\udc3b\ud835\udc5b\ud835\udc63\ud835\udc51\\displaystyle w^{(n+1)}_{vk}\\propto w^{(n)}_{vk}\\sum_{d}x_{vd}\\frac{h^{(n)}_{%\nkd}}{(W^{(n)}H^{(n)})_{vd}},\\qquad h^{(n+1)}_{kd}=\\frac{1}{1+\\lambda}h^{(n)}_{%\nkd}\\sum_{v}x_{vd}\\frac{w^{(n)}_{vk}}{(W^{(n)}H^{(n)})_{vd}}.italic_w start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u221d italic_w start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT divide start_ARG italic_h start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG , italic_h start_POSTSUPERSCRIPT ( italic_n + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_h start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT \u2211 start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT divide start_ARG italic_w start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v italic_k end_POSTSUBSCRIPT end_ARG start_ARG ( italic_W start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT end_ARG .\n\n\n\nLemma 6.1.\n\nLet (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of NMF with an \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraint on W\ud835\udc4aWitalic_W (18) and let \u03bb>0\ud835\udf060\\lambda>0italic_\u03bb > 0. Then (W,11+\u03bb\u2062H)\ud835\udc4a11\ud835\udf06\ud835\udc3b(W,\\tfrac{1}{1+\\lambda}H)( italic_W , divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_H ) is a solution of Lasso-penalized NMF with an \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraint (24). Conversely, let (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of (24). Then (W,(1+\u03bb)\u2062H)\ud835\udc4a1\ud835\udf06\ud835\udc3b(W,(1+\\lambda)H)( italic_W , ( 1 + italic_\u03bb ) italic_H ) is a solution of (18).\n\nProof.\nNotice that for any pair (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) with W\u2208\u0394V\u22121K\ud835\udc4asuperscriptsubscript\u0394\ud835\udc491\ud835\udc3eW\\in\\Delta_{V-1}^{K}italic_W \u2208 roman_\u0394 start_POSTSUBSCRIPT italic_V - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT, we have\n\nDKL\u03bb(X||W11+\u03bbH)=DKL(X||WH)+log(1+\u03bb)\u2211v,dxv\u2062d,DKL(X||W(1+\u03bb)H)=DKL\u03bb(X||WH)\u2212log(1+\u03bb)\u2211v,dxv\u2062d.\\begin{split}D_{\\text{KL}}^{\\lambda}(X\\,||\\,W\\tfrac{1}{1+\\lambda}H)&=D_{\\text{%\nKL}}(X\\,||\\,WH)+\\log(1+\\lambda)\\sum_{v,d}x_{vd},\\\\\nD_{\\text{KL}}(X\\,||\\,W(1+\\lambda)H)&=D_{\\text{KL}}^{\\lambda}(X\\,||\\,WH)-\\log(1%\n+\\lambda)\\sum_{v,d}x_{vd}.\\end{split}start_ROW start_CELL italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_H ) end_CELL start_CELL = italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) + roman_log ( 1 + italic_\u03bb ) \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W ( 1 + italic_\u03bb ) italic_H ) end_CELL start_CELL = italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W italic_H ) - roman_log ( 1 + italic_\u03bb ) \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT . end_CELL end_ROW\n(35)\n\nThe result can now be shown using a proof by contradiction. Let (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) be a solution of (18) and assume that (W,11+\u03bb\u2062H)\ud835\udc4a11\ud835\udf06\ud835\udc3b(W,\\tfrac{1}{1+\\lambda}H)( italic_W , divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_H ) is not a solution of \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-penalized NMF (24). Then there exists (W\u2217,H\u2217)superscript\ud835\udc4asuperscript\ud835\udc3b(W^{*},H^{*})( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) with DKL\u03bb(X||W\u2217H\u2217)<DKL\u03bb(X||W11+\u03bbH)D_{\\text{KL}}^{\\lambda}(X\\,||\\,W^{*}H^{*})<D_{\\text{KL}}^{\\lambda}(X\\,||\\,W%\n\\tfrac{1}{1+\\lambda}H)italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) < italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_H ). Applying (35), it follows\n\nDKL(X||W\u2217(1+\u03bb)H\u2217)\\displaystyle D_{\\text{KL}}(X\\,||\\,W^{*}(1+\\lambda)H^{*})italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( 1 + italic_\u03bb ) italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT )\n=DKL\u03bb(X||W\u2217H\u2217)\u2212log(1+\u03bb)\u2211v,dxv\u2062d\\displaystyle=D_{\\text{KL}}^{\\lambda}(X\\,||\\,W^{*}H^{*})-\\log(1+\\lambda)\\sum_{%\nv,d}x_{vd}= italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) - roman_log ( 1 + italic_\u03bb ) \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT\n\n\n<DKL\u03bb(X||W11+\u03bbH)\u2212log(1+\u03bb)\u2211v,dxv\u2062d\\displaystyle<D_{\\text{KL}}^{\\lambda}(X\\,||\\,W\\tfrac{1}{1+\\lambda}H)-\\log(1+%\n\\lambda)\\sum_{v,d}x_{vd}< italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03bb end_POSTSUPERSCRIPT ( italic_X | | italic_W divide start_ARG 1 end_ARG start_ARG 1 + italic_\u03bb end_ARG italic_H ) - roman_log ( 1 + italic_\u03bb ) \u2211 start_POSTSUBSCRIPT italic_v , italic_d end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_v italic_d end_POSTSUBSCRIPT\n\n\n=DKL(X||WH),\\displaystyle=D_{\\text{KL}}(X\\,||\\,WH),= italic_D start_POSTSUBSCRIPT KL end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) ,\n\nwhich is a contradiction to (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) being a solution of (18). The proof of the second statement is identical.\n\u220e\n\nMore generally, let us now consider any reconstruction error D(X||WH)D(X||WH)italic_D ( italic_X | | italic_W italic_H ) with D:\u211d+V\u00d7D\u00d7\u211d+V\u00d7D\u2192\u211d+:\ud835\udc37\u2192subscriptsuperscript\u211d\ud835\udc49\ud835\udc37subscriptsuperscript\u211d\ud835\udc49\ud835\udc37subscript\u211dD:\\mathbb{R}^{V\\times D}_{+}\\times\\mathbb{R}^{V\\times D}_{+}\\to\\mathbb{R}_{+}italic_D : blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT \u2192 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT, any regularizer R\u2062(H)\ud835\udc45\ud835\udc3bR(H)italic_R ( italic_H ) with R:\u211d+K\u00d7D\u2192\u211d+:\ud835\udc45\u2192subscriptsuperscript\u211d\ud835\udc3e\ud835\udc37subscript\u211dR:\\mathbb{R}^{K\\times D}_{+}\\to\\mathbb{R}_{+}italic_R : blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT \u2192 blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT, and any normalization constraint W\u2208\ud835\udd38p\ud835\udc4asubscript\ud835\udd38\ud835\udc5dW\\in\\mathbb{A}_{p}italic_W \u2208 blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT, where 0<p<\u221e0\ud835\udc5d0<p<\\infty0 < italic_p < \u221e and\n\n\ud835\udd38p={W\u2208\u211d+V\u00d7K|\u2200k:\u2016wk\u2016p=1}subscript\ud835\udd38\ud835\udc5dconditional-set\ud835\udc4asubscriptsuperscript\u211d\ud835\udc49\ud835\udc3e:for-all\ud835\udc58subscriptnormsubscript\ud835\udc64\ud835\udc58\ud835\udc5d1\\mathbb{A}_{p}=\\{W\\in\\mathbb{R}^{V\\times K}_{+}\\,|\\,\\forall k:\\|w_{k}\\|_{p}=1\\}blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = { italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT | \u2200 italic_k : \u2225 italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 1 }\n\ndenotes the set non-negative matrices with \u2113psubscript\u2113\ud835\udc5d\\ell_{p}roman_\u2113 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-normalized columns. The goal is to solve the following sparse NMF problem\n\nminW\u2208\ud835\udd38p,H\u2208\u211d+K\u00d7DD(X||WH)+R(H).\\min_{W\\in\\mathbb{A}_{p},\\,H\\in\\mathbb{R}^{K\\times D}_{+}}D(X\\,||\\,WH)+R(H).roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_H \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_D ( italic_X | | italic_W italic_H ) + italic_R ( italic_H ) .\n(36)\n\nIn the special cases of beta divergence reconstruction errors, Lasso- or log-regularizers R\u2062(H)=\u03bb\u2062\u2016H\u20161\ud835\udc45\ud835\udc3b\ud835\udf06subscriptnorm\ud835\udc3b1R(H)=\\lambda\\|H\\|_{1}italic_R ( italic_H ) = italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or R\u2062(H)=\u03bb\u2062\u2211k,dlog\u2061(hk\u2062d)\ud835\udc45\ud835\udc3b\ud835\udf06subscript\ud835\udc58\ud835\udc51subscript\u210e\ud835\udc58\ud835\udc51R(H)=\\lambda\\sum_{k,d}\\log(h_{kd})italic_R ( italic_H ) = italic_\u03bb \u2211 start_POSTSUBSCRIPT italic_k , italic_d end_POSTSUBSCRIPT roman_log ( italic_h start_POSTSUBSCRIPT italic_k italic_d end_POSTSUBSCRIPT ), and \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-normalized W\u2208\ud835\udd381\ud835\udc4asubscript\ud835\udd381W\\in\\mathbb{A}_{1}italic_W \u2208 blackboard_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, it has been shown that (36) is equivalent to an optimization problem without any normalization constraints on W\ud835\udc4aWitalic_W [34, Lemma 1 and 2]. We now generalize this result to (36).\n\nLet us introduce the normalization matrix Dp\u2062(W)\u2254diag\u2062(\u2016w1\u2016p,\u2026,\u2016wK\u2016p)\u2208\u211d+K\u00d7K\u2254subscript\ud835\udc37\ud835\udc5d\ud835\udc4adiagsubscriptnormsubscript\ud835\udc641\ud835\udc5d\u2026subscriptnormsubscript\ud835\udc64\ud835\udc3e\ud835\udc5dsubscriptsuperscript\u211d\ud835\udc3e\ud835\udc3eD_{p}(W)\\coloneqq\\text{diag}(\\|w_{1}\\|_{p},\\ldots,\\|w_{K}\\|_{p})\\in\\mathbb{R}^%\n{K\\times K}_{+}italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W ) \u2254 diag ( \u2225 italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , \u2026 , \u2225 italic_w start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and the optimization problem\n\nminW\u2208\u211d+V\u00d7K,H\u2208\u211d+K\u00d7DD(X||WH)+R(Dp(W)H).\\min_{W\\in\\mathbb{R}^{V\\times K}_{+},\\,H\\in\\mathbb{R}^{K\\times D}_{+}}D(X\\,||%\n\\,WH)+R(D_{p}(W)H).roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT , italic_H \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_D ( italic_X | | italic_W italic_H ) + italic_R ( italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W ) italic_H ) .\n(37)\n\nThen the optimization problems (36) and (37) are equivalent in the sense that every solution of (36) is a solution of (37) and normalized solutions of (37) are solutions of (36).\n\nTheorem F.1 (cf.\u00a0[34, Lemma 1 and 2]).\n\nLet X\u2208\u211d+V\u00d7D\ud835\udc4bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc37X\\in\\mathbb{R}^{V\\times D}_{+}italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT. Let (W,H)\u2208\ud835\udd38p\u00d7\u211d+K\u00d7D\ud835\udc4a\ud835\udc3bsubscript\ud835\udd38\ud835\udc5dsubscriptsuperscript\u211d\ud835\udc3e\ud835\udc37(W,H)\\in\\mathbb{A}_{p}\\times\\mathbb{R}^{K\\times D}_{+}( italic_W , italic_H ) \u2208 blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT be a solution of (36). Then (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) is a solution of (37). Conversely, let (W,H)\u2208\u211d+V\u00d7K\u00d7\u211d+K\u00d7D\ud835\udc4a\ud835\udc3bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc3esubscriptsuperscript\u211d\ud835\udc3e\ud835\udc37(W,H)\\in\\mathbb{R}^{V\\times K}_{+}\\times\\mathbb{R}^{K\\times D}_{+}( italic_W , italic_H ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT be a solution of (37). Then (W\u2062Dp\u2062(W)\u22121,Dp\u2062(W)\u2062H)\ud835\udc4asubscript\ud835\udc37\ud835\udc5dsuperscript\ud835\udc4a1subscript\ud835\udc37\ud835\udc5d\ud835\udc4a\ud835\udc3b(WD_{p}(W)^{-1},D_{p}(W)H)( italic_W italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W ) italic_H ) is a solution of (36).\n\nProof.\nLet (W,H)\u2208\ud835\udd38p\u00d7\u211d+K\u00d7D\ud835\udc4a\ud835\udc3bsubscript\ud835\udd38\ud835\udc5dsubscriptsuperscript\u211d\ud835\udc3e\ud835\udc37(W,H)\\in\\mathbb{A}_{p}\\times\\mathbb{R}^{K\\times D}_{+}( italic_W , italic_H ) \u2208 blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT be a solution of (36). Assume there exists (W\u2217,H\u2217)\u2208\u211d+V\u00d7K\u00d7\u211d+K\u00d7Dsuperscript\ud835\udc4asuperscript\ud835\udc3bsubscriptsuperscript\u211d\ud835\udc49\ud835\udc3esubscriptsuperscript\u211d\ud835\udc3e\ud835\udc37(W^{*},H^{*})\\in\\mathbb{R}^{V\\times K}_{+}\\times\\mathbb{R}^{K\\times D}_{+}( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT , italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT with\n\nD(X||W\u2217H\u2217)+R(Dp(W\u2217)H\u2217)<D(X||WH)+R(Dp(W)H)=D(X||WH)+R(H).D(X\\,||\\,W^{*}H^{*})+R(D_{p}(W^{*})H^{*})<D(X\\,||\\,WH)+R(D_{p}(W)H)=D(X\\,||\\,%\nWH)+R(H).italic_D ( italic_X | | italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) + italic_R ( italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) < italic_D ( italic_X | | italic_W italic_H ) + italic_R ( italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W ) italic_H ) = italic_D ( italic_X | | italic_W italic_H ) + italic_R ( italic_H ) .\n\nThen the normalized matrices (W~,H~)~\ud835\udc4a~\ud835\udc3b(\\widetilde{W},\\widetilde{H})( over~ start_ARG italic_W end_ARG , over~ start_ARG italic_H end_ARG ) with W~=W\u2217\u2062Dp\u22121\u2062(W\u2217)~\ud835\udc4asuperscript\ud835\udc4asuperscriptsubscript\ud835\udc37\ud835\udc5d1superscript\ud835\udc4a\\widetilde{W}=W^{*}D_{p}^{-1}(W^{*})over~ start_ARG italic_W end_ARG = italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) and H~=Dp\u2062(W\u2217)\u2062H\u2217~\ud835\udc3bsubscript\ud835\udc37\ud835\udc5dsuperscript\ud835\udc4asuperscript\ud835\udc3b\\widetilde{H}=D_{p}(W^{*})H^{*}over~ start_ARG italic_H end_ARG = italic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_W start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ) italic_H start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT fulfill\n\nD(X||W~H~)+R(H~)<D(X||WH)+R(H),D(X\\,||\\,\\widetilde{W}\\widetilde{H})+R(\\widetilde{H})<D(X\\,||WH)+R(H),italic_D ( italic_X | | over~ start_ARG italic_W end_ARG over~ start_ARG italic_H end_ARG ) + italic_R ( over~ start_ARG italic_H end_ARG ) < italic_D ( italic_X | | italic_W italic_H ) + italic_R ( italic_H ) ,\n\ncontradicting that (W,H)\ud835\udc4a\ud835\udc3b(W,H)( italic_W , italic_H ) is a solution of (36). The proof of the second statement is analogous.\n\u220e\n\n\nExample.\n\nLet 0<p,q<\u221eformulae-sequence0\ud835\udc5d\ud835\udc5e0<p,q<\\infty0 < italic_p , italic_q < \u221e and consider (36) with W\u2208\ud835\udd38p\ud835\udc4asubscript\ud835\udd38\ud835\udc5dW\\in\\mathbb{A}_{p}italic_W \u2208 blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT and R\u2062(H)=\u03bb\u2062\u2016H\u2016qq\ud835\udc45\ud835\udc3b\ud835\udf06superscriptsubscriptnorm\ud835\udc3b\ud835\udc5e\ud835\udc5eR(H)=\\lambda\\|H\\|_{q}^{q}italic_R ( italic_H ) = italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT for some weight \u03bb>0\ud835\udf060\\lambda>0italic_\u03bb > 0. Then\n\nminW\u2208\ud835\udd38p,H\u2208\u211d+K\u00d7DD(X||WH)+\u03bb\u2225H\u2225qq\\min_{W\\in\\mathbb{A}_{p},\\,H\\in\\mathbb{R}^{K\\times D}_{+}}D(X\\,||\\,WH)+\\lambda%\n\\|H\\|_{q}^{q}roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_A start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_H \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_D ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT\n\nis equivalent to\n\nminW\u2208\u211d+V\u00d7K,H\u2208\u211d+K\u00d7DD(X||WH)+\u03bb\u2211k\u2225wk\u2225pq\u2225hkT\u2225qq.\\min_{W\\in\\mathbb{R}^{V\\times K}_{+},\\,H\\in\\mathbb{R}^{K\\times D}_{+}}D(X\\,||%\n\\,WH)+\\lambda\\sum_{k}\\|w_{k}\\|_{p}^{q}\\|h_{k}^{T}\\|_{q}^{q}.roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT , italic_H \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_D ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT \u2225 italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2225 start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT .\n\nThis result demonstrates that the normalization constraint on W\ud835\udc4aWitalic_W and the regularizer R\u2062(H)\ud835\udc45\ud835\udc3bR(H)italic_R ( italic_H ) are competing penalties.\n\n\nExample ([34]).\n\nLet p=1\ud835\udc5d1p=1italic_p = 1 and consider (36) with the beta divergence D=D\u03b2\ud835\udc37subscript\ud835\udc37\ud835\udefdD=D_{\\beta}italic_D = italic_D start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT for \u03b2\u2208\u211d\ud835\udefd\u211d\\beta\\in\\mathbb{R}italic_\u03b2 \u2208 blackboard_R, W\u2208\ud835\udd381\ud835\udc4asubscript\ud835\udd381W\\in\\mathbb{A}_{1}italic_W \u2208 blackboard_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, and R\u2062(H)=\u03bb\u2062\u2016H\u20161\ud835\udc45\ud835\udc3b\ud835\udf06subscriptnorm\ud835\udc3b1R(H)=\\lambda\\|H\\|_{1}italic_R ( italic_H ) = italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for some weight \u03bb>0\ud835\udf060\\lambda>0italic_\u03bb > 0.\nThen\n\nminW\u2208\ud835\udd381,H\u2208\u211d+K\u00d7DD\u03b2(X||WH)+\u03bb\u2225H\u22251\\min_{W\\in\\mathbb{A}_{1},\\,H\\in\\mathbb{R}^{K\\times D}_{+}}D_{\\beta}(X\\,||\\,WH)%\n+\\lambda\\|H\\|_{1}roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_H \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2225 italic_H \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT\n(38)\n\nis equivalent to\n\nminW\u2208\u211d+V\u00d7K,H\u2208\u211d+K\u00d7DD\u03b2(X||WH)+\u03bb\u2211k\u2225wk\u22251\u2225hkT\u22251,\\min_{W\\in\\mathbb{R}^{V\\times K}_{+},\\,H\\in\\mathbb{R}^{K\\times D}_{+}}D_{\\beta%\n}(X\\,||\\,WH)+\\lambda\\sum_{k}\\|w_{k}\\|_{1}\\|h_{k}^{T}\\|_{1},roman_min start_POSTSUBSCRIPT italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_V \u00d7 italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT , italic_H \u2208 blackboard_R start_POSTSUPERSCRIPT italic_K \u00d7 italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT ( italic_X | | italic_W italic_H ) + italic_\u03bb \u2211 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2225 italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT \u2225 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,\n\nsuggesting that a Lasso penalty on H\ud835\udc3bHitalic_H together with an \u21131subscript\u21131\\ell_{1}roman_\u2113 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT normalization constraint on W\ud835\udc4aWitalic_W induces mutual sparsity [34]. However, Lemma\u00a06.1 proves that there is no effect on the sparsity in the case \u03b2=1\ud835\udefd1\\beta=1italic_\u03b2 = 1, demonstrating that the effect on the sparsity also depends on the reconstruction error.\n\n"
        }
    ],
    "figure_chunks": [],
    "table_chunks": [],
    "metadata": {},
    "pdf_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\pdfs\\2405.20542v1.pdf",
    "HTML_path": "D:\\PaperIgnition\\PaperIgnition\\orchestrator\\htmls\\2405.20542v1.html"
}