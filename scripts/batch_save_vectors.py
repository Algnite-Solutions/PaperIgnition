#!/usr/bin/env python3
"""
æ‰¹é‡ä¿å­˜è®ºæ–‡å‘é‡åˆ°FAISSæ•°æ®åº“çš„è„šæœ¬

è¯¥è„šæœ¬ä¼šï¼š
1. è¯»å–é…ç½®æ–‡ä»¶è·å–APIæœåŠ¡å™¨åœ°å€
2. è°ƒç”¨ /get_all_metadata_doc_ids/ API è·å–æ‰€æœ‰è®ºæ–‡çš„doc_id
3. è°ƒç”¨ /get_all_vector_doc_ids/ API è·å–å·²å­˜å‚¨å‘é‡çš„doc_id
4. è®¡ç®—å·®é›†ï¼Œæ‰¾å‡ºæœªå­˜å‚¨å‘é‡çš„è®ºæ–‡
5. è°ƒç”¨ /get_metadata/{doc_id} API æ‰¹é‡è·å–è¿™äº›è®ºæ–‡çš„å…ƒæ•°æ®
6. è°ƒç”¨ /save_vectors/ API ç«¯ç‚¹å°†å‘é‡ä¿å­˜åˆ°FAISSæ•°æ®åº“
7. æä¾›è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºå’Œé”™è¯¯æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python batch_save_vectors.py [--api-url URL] [--config CONFIG] [--batch-size SIZE] [--dry-run]

å‚æ•°ï¼š
    --api-url: APIæœåŠ¡å™¨åœ°å€ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)
    --config: é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ../backend/configs/app_config.yaml)
    --batch-size: æ¯æ‰¹å¤„ç†çš„è®ºæ–‡æ•°é‡ (é»˜è®¤: 50)
    --dry-run: ä»…æ˜¾ç¤ºå°†è¦å¤„ç†çš„è®ºæ–‡ï¼Œä¸å®é™…ä¿å­˜

é…ç½®æ–‡ä»¶æ ¼å¼ï¼š
    è„šæœ¬ä¼šè‡ªåŠ¨ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–INDEX_SERVICE.hosté…ç½®
    
æ³¨æ„ï¼š
    æ­¤è„šæœ¬å®Œå…¨é€šè¿‡ HTTP API å·¥ä½œï¼Œä¸éœ€è¦ç›´æ¥çš„æ•°æ®åº“è¿æ¥
"""

import os
import sys
import argparse
import asyncio
import httpx
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Set
import logging
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

from backend.index_service.db_utils import load_config
from AIgnite.data.docset import DocSet, DocSetList

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('batch_save_vectors.log')
    ]
)

# ç¦ç”¨ç¬¬ä¸‰æ–¹åº“çš„ DEBUG æ—¥å¿—ï¼Œåªä¿ç•™ WARNING åŠä»¥ä¸Šçº§åˆ«
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('httpcore').setLevel(logging.WARNING)
logging.getLogger('urllib3').setLevel(logging.WARNING)
logging.getLogger('asyncio').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


def get_api_url_from_config(config: Dict) -> str:
    """ä»é…ç½®ä¸­è·å–API URL"""
    try:
        return config['INDEX_SERVICE']['host']
    except KeyError:
        logger.warning("é…ç½®ä¸­æœªæ‰¾åˆ°INDEX_SERVICE.hostï¼Œä½¿ç”¨é»˜è®¤å€¼")
        return "http://localhost:8002"


class BatchVectorSaver:
    def __init__(self, api_url: str = None, config_path: str = None, batch_size: int = 50):
        """åˆå§‹åŒ–æ‰¹é‡å‘é‡ä¿å­˜å™¨
        
        Args:
            api_url: APIæœåŠ¡å™¨åœ°å€ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åœ°å€ï¼‰
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            batch_size: æ¯æ‰¹å¤„ç†çš„è®ºæ–‡æ•°é‡
        """
        # ä¼˜å…ˆçº§ï¼šé…ç½®æ–‡ä»¶ > å‘½ä»¤è¡Œå‚æ•° > é»˜è®¤å€¼
        
        # åŠ è½½é…ç½®
        if config_path:
            try:
                self.config = load_config(config_path)
                logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                
                # ä¼˜å…ˆä»é…ç½®æ–‡ä»¶è·å– API URL
                config_api_url = get_api_url_from_config(self.config)
                if config_api_url:
                    # å¦‚æœå‘½ä»¤è¡Œä¹Ÿæä¾›äº† api_urlï¼Œæ˜¾ç¤ºè­¦å‘Š
                    if api_url is not None and api_url != config_api_url:
                        logger.warning(f"âš ï¸  å‘½ä»¤è¡Œ API URL ({api_url}) å°†è¢«é…ç½®æ–‡ä»¶ä¸­çš„åœ°å€è¦†ç›–")
                    api_url = config_api_url
                    logger.info(f"âœ… ä»é…ç½®æ–‡ä»¶è·å– API URL: {api_url}")
                elif api_url is None:
                    # é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰ï¼Œä¸”å‘½ä»¤è¡Œä¹Ÿæ²¡æä¾›
                    api_url = "http://localhost:8002"
                    logger.warning(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° API URLï¼Œä½¿ç”¨é»˜è®¤å€¼: {api_url}")
                    
            except Exception as e:
                logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
                self.config = None
                if api_url is None:
                    api_url = "http://localhost:8002"
                    logger.warning(f"âš ï¸  ä½¿ç”¨é»˜è®¤ API URL: {api_url}")
        else:
            self.config = None
            if api_url is None:
                api_url = "http://localhost:8002"
                logger.warning(f"âš ï¸  æœªæä¾›é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ API URL: {api_url}")
            else:
                logger.info(f"âœ… ä½¿ç”¨å‘½ä»¤è¡Œæä¾›çš„ API URL: {api_url}")
        
        self.api_url = api_url.rstrip('/')
        self.batch_size = batch_size
        
        # API ç«¯ç‚¹é…ç½®
        self.save_vectors_endpoint = f"{self.api_url}/save_vectors/"
        self.health_endpoint = f"{self.api_url}/health"
        self.get_all_metadata_doc_ids_endpoint = f"{self.api_url}/get_all_metadata_doc_ids/"
        self.get_all_vector_doc_ids_endpoint = f"{self.api_url}/get_all_vector_doc_ids/"
        self.get_metadata_endpoint = f"{self.api_url}/get_metadata"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_papers = 0
        self.successful_saves = 0
        self.failed_saves = 0
        self.skipped_papers = 0
        self.results = []
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        logger.info(f"ğŸ”§ API URL: {self.api_url}")
        logger.info(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
    
    async def check_server_health(self) -> bool:
        """æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(self.health_endpoint)
                if response.status_code == 200:
                    data = response.json()
                    logger.info(f"âœ… æœåŠ¡å™¨å¥åº·æ£€æŸ¥é€šè¿‡: {data}")
                    return data.get("indexer_ready", False)
                else:
                    logger.error(f"âŒ æœåŠ¡å™¨å¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
                    return False
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ {self.api_url}: {e}")
            return False
    
    async def get_missing_doc_ids(self, client: httpx.AsyncClient) -> Set[str]:
        """é€šè¿‡ API è·å–æœªå­˜å‚¨å‘é‡çš„doc_ids
        
        Args:
            client: HTTP å®¢æˆ·ç«¯
            
        Returns:
            æœªå­˜å‚¨å‘é‡çš„doc_idé›†åˆ
        """
        try:
            # è°ƒç”¨ API è·å–æ‰€æœ‰ metadata doc_ids
            logger.info("ğŸ“Š æ­£åœ¨ä» MetadataDB è·å–æ‰€æœ‰è®ºæ–‡ID...")
            metadata_response = await client.get(
                self.get_all_metadata_doc_ids_endpoint,
                timeout=30.0
            )
            
            if metadata_response.status_code != 200:
                logger.error(f"âŒ è·å–metadata doc_idså¤±è´¥: {metadata_response.status_code}")
                return set()
            
            metadata_result = metadata_response.json()
            all_doc_ids = set(metadata_result.get('doc_ids', []))
            logger.info(f"   æ‰¾åˆ° {len(all_doc_ids)} ç¯‡è®ºæ–‡")
            
            # è°ƒç”¨ API è·å–æ‰€æœ‰ vector doc_ids
            logger.info("ğŸ“Š æ­£åœ¨ä» VectorDB è·å–å·²å­˜å‚¨å‘é‡çš„è®ºæ–‡ID...")
            vector_response = await client.get(
                self.get_all_vector_doc_ids_endpoint,
                timeout=30.0
            )
            
            if vector_response.status_code != 200:
                logger.error(f"âŒ è·å–vector doc_idså¤±è´¥: {vector_response.status_code}")
                return set()
            
            vector_result = vector_response.json()
            vector_doc_ids = set(vector_result.get('doc_ids', []))
            logger.info(f"   æ‰¾åˆ° {len(vector_doc_ids)} ç¯‡å·²å­˜å‚¨å‘é‡çš„è®ºæ–‡")
            
            # è®¡ç®—å·®é›†
            missing_doc_ids = all_doc_ids - vector_doc_ids
            logger.info(f"ğŸ“Š éœ€è¦å­˜å‚¨å‘é‡çš„è®ºæ–‡æ•°: {len(missing_doc_ids)}")
            
            return missing_doc_ids
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼ºå¤±doc_idså¤±è´¥: {e}")
            return set()
    
    async def fetch_papers_metadata(self, client: httpx.AsyncClient, doc_ids: Set[str]) -> List[Dict]:
        """é€šè¿‡ API æ‰¹é‡è·å–è®ºæ–‡å…ƒæ•°æ®
        
        Args:
            client: HTTP å®¢æˆ·ç«¯
            doc_ids: éœ€è¦è·å–å…ƒæ•°æ®çš„doc_idé›†åˆ
            
        Returns:
            è®ºæ–‡å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
        """
        papers_metadata = []
        failed_count = 0
        
        logger.info(f"ğŸ“Š æ­£åœ¨è·å– {len(doc_ids)} ç¯‡è®ºæ–‡çš„å…ƒæ•°æ®...")
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        with tqdm(total=len(doc_ids), desc="ğŸ“¥ è·å–å…ƒæ•°æ®", unit="ç¯‡", ncols=100) as pbar:
            for doc_id in doc_ids:
                try:
                    response = await client.get(
                        f"{self.get_metadata_endpoint}/{doc_id}",
                        timeout=10.0
                    )
                    
                    if response.status_code == 200:
                        metadata = response.json()
                        papers_metadata.append(metadata)
                        pbar.set_postfix({"æˆåŠŸ": len(papers_metadata), "å¤±è´¥": failed_count})
                    elif response.status_code == 404:
                        failed_count += 1
                        pbar.set_postfix({"æˆåŠŸ": len(papers_metadata), "å¤±è´¥": failed_count})
                    else:
                        failed_count += 1
                        pbar.set_postfix({"æˆåŠŸ": len(papers_metadata), "å¤±è´¥": failed_count})
                        
                except Exception as e:
                    failed_count += 1
                    pbar.set_postfix({"æˆåŠŸ": len(papers_metadata), "å¤±è´¥": failed_count})
                finally:
                    pbar.update(1)  # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ›´æ–°è¿›åº¦
        
        logger.info(f"ğŸ“Š æˆåŠŸè·å– {len(papers_metadata)} ç¯‡è®ºæ–‡çš„å…ƒæ•°æ®")
        if failed_count > 0:
            logger.warning(f"âš ï¸  å¤±è´¥ {failed_count} ç¯‡")
        
        return papers_metadata
    
    def build_docsets(self, papers_metadata: List[Dict]) -> List[DocSet]:
        """ä»å…ƒæ•°æ®æ„å»ºDocSetå¯¹è±¡åˆ—è¡¨
        
        Args:
            papers_metadata: è®ºæ–‡å…ƒæ•°æ®å­—å…¸åˆ—è¡¨
            
        Returns:
            DocSetå¯¹è±¡åˆ—è¡¨
        """
        docsets = []
        
        for metadata in papers_metadata:
            try:
                docset = DocSet(
                    doc_id=metadata.get('doc_id'),
                    title=metadata.get('title', ''),
                    abstract=metadata.get('abstract', ''),
                    authors=metadata.get('authors', []),
                    categories=metadata.get('categories', []),
                    published_date=metadata.get('published_date', ''),
                    pdf_path=metadata.get('pdf_path', ''),
                    HTML_path=metadata.get('HTML_path'),
                    text_chunks=[],  # save_vectorsä¸éœ€è¦text_chunks
                    figure_chunks=[],  # save_vectorsä¸éœ€è¦figure_chunks
                    table_chunks=[],  # save_vectorsä¸éœ€è¦table_chunks
                    metadata=metadata.get('metadata', {}),
                    comments=metadata.get('comments')
                )
                docsets.append(docset)
            except Exception as e:
                logger.error(f"âŒ æ„å»ºDocSetå¤±è´¥ {metadata.get('doc_id', 'unknown')}: {e}")
        
        logger.info(f"ğŸ“¦ æˆåŠŸæ„å»º {len(docsets)} ä¸ªDocSetå¯¹è±¡")
        return docsets
    
    async def save_vectors_batch(self, client: httpx.AsyncClient, docsets: List[DocSet]) -> Dict:
        """è°ƒç”¨APIæ‰¹é‡ä¿å­˜å‘é‡
        
        Args:
            client: HTTPå®¢æˆ·ç«¯
            docsets: DocSetå¯¹è±¡åˆ—è¡¨
            
        Returns:
            ä¿å­˜ç»“æœå­—å…¸
        """
        request_data = {
            "docsets": {
                "docsets": [docset.dict() for docset in docsets]
            },
            "indexing_status": None
        }
        
        try:
            logger.info(f"ğŸ“¤ æ­£åœ¨ä¿å­˜ {len(docsets)} ç¯‡è®ºæ–‡çš„å‘é‡...")
            response = await client.post(
                self.save_vectors_endpoint,
                json=request_data,
                timeout=120.0  # å‘é‡è®¡ç®—å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get("success", False):
                    logger.info(f"âœ… æ‰¹æ¬¡ä¿å­˜æˆåŠŸ: {result.get('message', '')}")
                    return {
                        "status": "success",
                        "papers_processed": result.get("papers_processed", len(docsets)),
                        "message": result.get("message", "ä¿å­˜æˆåŠŸ")
                    }
                else:
                    logger.error(f"âŒ æ‰¹æ¬¡ä¿å­˜å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return {
                        "status": "failed",
                        "papers_processed": 0,
                        "message": result.get("message", "ä¿å­˜å¤±è´¥")
                    }
            else:
                error_msg = f"HTTP {response.status_code}: {response.text}"
                logger.error(f"âŒ æ‰¹æ¬¡ä¿å­˜å¤±è´¥: {error_msg}")
                return {
                    "status": "failed",
                    "papers_processed": 0,
                    "message": error_msg
                }
                
        except httpx.TimeoutException:
            error_msg = "è¯·æ±‚è¶…æ—¶"
            logger.error(f"âŒ æ‰¹æ¬¡ä¿å­˜å¤±è´¥: {error_msg}")
            return {
                "status": "failed",
                "papers_processed": 0,
                "message": error_msg
            }
        except Exception as e:
            error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
            logger.error(f"âŒ æ‰¹æ¬¡ä¿å­˜å¤±è´¥: {error_msg}")
            return {
                "status": "failed",
                "papers_processed": 0,
                "message": error_msg
            }
    
    async def batch_save_vectors(self, dry_run: bool = False) -> Dict:
        """æ‰¹é‡ä¿å­˜å‘é‡ä¸»æµç¨‹
        
        Args:
            dry_run: æ˜¯å¦ä¸ºæ¼”ç»ƒæ¨¡å¼
            
        Returns:
            å¤„ç†ç»“æœæ‘˜è¦
        """
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡ä¿å­˜å‘é‡...")
        
        # åˆ›å»º HTTP client
        async with httpx.AsyncClient(timeout=120.0) as client:
            # 1. è·å–ç¼ºå¤±çš„doc_ids
            missing_doc_ids = await self.get_missing_doc_ids(client)
            self.total_papers = len(missing_doc_ids)
            
            if self.total_papers == 0:
                logger.info("âœ… æ‰€æœ‰è®ºæ–‡çš„å‘é‡å·²å­˜å‚¨ï¼Œæ— éœ€å¤„ç†")
                return self.get_summary()
            
            logger.info(f"ğŸ“‹ å°†è¦å¤„ç† {self.total_papers} ç¯‡è®ºæ–‡")
            
            missing_doc_ids = list(missing_doc_ids)
            # 2. è·å–è®ºæ–‡å…ƒæ•°æ®
            papers_metadata = await self.fetch_papers_metadata(client, missing_doc_ids)
            if not papers_metadata:
                logger.error("âŒ æœªèƒ½è·å–ä»»ä½•è®ºæ–‡å…ƒæ•°æ®")
                return self.get_summary()
            
            # 3. æ„å»ºDocSetå¯¹è±¡
            all_docsets = self.build_docsets(papers_metadata)
            if not all_docsets:
                logger.error("âŒ æœªèƒ½æ„å»ºä»»ä½•DocSetå¯¹è±¡")
                return self.get_summary()
            
            if dry_run:
                logger.info("ğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…ä¿å­˜å‘é‡")
                logger.info(f"ğŸ“‹ å°†è¦ä¿å­˜å‘é‡çš„è®ºæ–‡:")
                for i, docset in enumerate(all_docsets[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                    logger.info(f"  {i}. {docset.doc_id}: {docset.title[:60]}...")
                if len(all_docsets) > 10:
                    logger.info(f"  ... è¿˜æœ‰ {len(all_docsets) - 10} ç¯‡è®ºæ–‡")
                return self.get_summary()
            
            # 4. æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
            logger.info("ğŸ” æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€...")
            if not await self.check_server_health():
                logger.error("âŒ æœåŠ¡å™¨æœªå°±ç»ªæˆ–indexeræœªåˆå§‹åŒ–")
                return self.get_summary()
            
            # 5. åˆ†æ‰¹å¤„ç†
            logger.info(f"ğŸ’¾ å¼€å§‹æ‰¹é‡ä¿å­˜å‘é‡ï¼ˆæ¯æ‰¹ {self.batch_size} ç¯‡ï¼‰...")
            start_time = time.time()
            
            total_batches = (len(all_docsets) + self.batch_size - 1) // self.batch_size
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ‰¹å¤„ç†è¿›åº¦
            with tqdm(total=total_batches, desc="ğŸ’¾ æ‰¹é‡ä¿å­˜", unit="æ‰¹æ¬¡", ncols=100) as pbar:
                for batch_idx in range(0, len(all_docsets), self.batch_size):
                    batch_docsets = all_docsets[batch_idx:batch_idx + self.batch_size]
                    current_batch = batch_idx // self.batch_size + 1
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_description(f"ğŸ’¾ æ‰¹æ¬¡ {current_batch}/{total_batches}")
                    
                    result = await self.save_vectors_batch(client, batch_docsets)
                    self.results.append(result)
                    
                    if result["status"] == "success":
                        self.successful_saves += result["papers_processed"]
                        pbar.set_postfix({"æˆåŠŸ": self.successful_saves, "å¤±è´¥": self.failed_saves})
                    else:
                        self.failed_saves += len(batch_docsets)
                        pbar.set_postfix({"æˆåŠŸ": self.successful_saves, "å¤±è´¥": self.failed_saves})
                    
                    pbar.update(1)  # æ›´æ–°è¿›åº¦
                    
                    # æ·»åŠ å°å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    await asyncio.sleep(0.5)
        
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        if self.total_papers > 0:
            logger.info(f"ğŸ“Š å¹³å‡æ¯ç¯‡è®ºæ–‡: {duration/self.total_papers:.2f} ç§’")
        
        return self.get_summary()
    
    def get_summary(self) -> Dict:
        """è·å–å¤„ç†ç»“æœæ‘˜è¦"""
        return {
            "total_papers": self.total_papers,
            "successful_saves": self.successful_saves,
            "failed_saves": self.failed_saves,
            "skipped_papers": self.skipped_papers,
            "success_rate": (self.successful_saves / self.total_papers * 100) if self.total_papers > 0 else 0,
            "results": self.results
        }
    
    def print_summary(self):
        """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
        summary = self.get_summary()
        
        print("\n" + "="*60)
        print("ğŸ“Š æ‰¹é‡å‘é‡ä¿å­˜ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"ğŸ“ æ€»è®ºæ–‡æ•°: {summary['total_papers']}")
        print(f"âœ… æˆåŠŸä¿å­˜: {summary['successful_saves']}")
        print(f"âŒ ä¿å­˜å¤±è´¥: {summary['failed_saves']}")
        print(f"â­ï¸  è·³è¿‡è®ºæ–‡: {summary['skipped_papers']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        if summary['failed_saves'] > 0:
            print("\nâŒ å¤±è´¥çš„æ‰¹æ¬¡:")
            for i, result in enumerate(summary['results'], 1):
                if result['status'] == 'failed':
                    print(f"  æ‰¹æ¬¡ {i}: {result['message']}")
        
        print("="*60)


async def main():
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡ä¿å­˜è®ºæ–‡å‘é‡åˆ°FAISSæ•°æ®åº“",
        epilog="ä¼˜å…ˆçº§: é…ç½®æ–‡ä»¶ä¸­çš„API URL > --api-urlå‚æ•° > é»˜è®¤å€¼(http://localhost:8002)"
    )
    parser.add_argument("--api-url", 
                       help="APIæœåŠ¡å™¨åœ°å€ (å¯é€‰ï¼Œç”¨äºè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„åœ°å€)")
    parser.add_argument("--config", 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ../backend/configs/app_config.yaml)",
                       default=str(Path(__file__).parent.parent / "backend" / "configs" / "app_config.yaml"))
    parser.add_argument("--batch-size", 
                       type=int,
                       default=50,
                       help="æ¯æ‰¹å¤„ç†çš„è®ºæ–‡æ•°é‡ (é»˜è®¤: 50)")
    parser.add_argument("--dry-run", action="store_true", 
                       help="ä»…æ˜¾ç¤ºå°†è¦å¤„ç†çš„è®ºæ–‡ï¼Œä¸å®é™…ä¿å­˜")
    
    args = parser.parse_args()

    args.config = "/data3/guofang/AIgnite-Solutions/PaperIgnition/backend/configs/app_config.yaml"
    args.dry_run = False

    try:
        saver = BatchVectorSaver(
            api_url=args.api_url,
            config_path=args.config,
            batch_size=args.batch_size
        )
        
        await saver.batch_save_vectors(dry_run=args.dry_run)
        saver.print_summary()
        
        # å¦‚æœæœ‰å¤±è´¥çš„ä¿å­˜ï¼Œé€€å‡ºç ä¸º1
        if saver.failed_saves > 0:
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())


