"""
å°† Paper Abstract å’Œ User Interest çš„ Embedding è¿ç§»åˆ° pgvector

åŠŸèƒ½:
1. ä» paperignition æ•°æ®åº“è¯»å–è®ºæ–‡ Abstract
2. ä» paperignition_user æ•°æ®åº“è¯»å–ç”¨æˆ· Interest
3. ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ Embedding API ç”Ÿæˆå‘é‡
4. å­˜å‚¨åˆ°é˜¿é‡Œäº‘ RDS PostgreSQL çš„ pgvector è¡¨

è¿è¡Œæ–¹å¼:
    python scripts/migrate_embeddings_to_pgvector.py
    python scripts/migrate_embeddings_to_pgvector.py --config scripts/migration_config.yaml

ç¯å¢ƒå˜é‡:
    DASHSCOPE_API_KEY: é˜¿é‡Œäº‘ç™¾ç‚¼ API Key (å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶)
"""

import os
import sys
import json
import time
import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================
# é…ç½®åŠ è½½
# ============================================

def expand_env_vars(value):
    """å±•å¼€é…ç½®å€¼ä¸­çš„ç¯å¢ƒå˜é‡

    æ”¯æŒæ ¼å¼: ${ENV_VAR} æˆ– ${ENV_VAR:default_value}
    """
    if not isinstance(value, str):
        return value

    pattern = r'\$\{([^}:]+)(?::([^}]*))?\}'

    def replace(match):
        env_var = match.group(1)
        default = match.group(2) if match.group(2) is not None else ""
        return os.getenv(env_var, default)

    return re.sub(pattern, replace, value)


def load_config(config_path: str = None) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if config_path is None:
        # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
        config_path = Path(__file__).parent / "migration_config.yaml"

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # é€’å½’å±•å¼€ç¯å¢ƒå˜é‡
    def expand_config(obj):
        if isinstance(obj, dict):
            return {k: expand_config(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [expand_config(item) for item in obj]
        else:
            return expand_env_vars(obj)

    return expand_config(config)


# å…¨å±€é…ç½®å˜é‡ï¼ˆå°†åœ¨ load_all_config() ä¸­åˆå§‹åŒ–ï¼‰
DASHSCOPE_API_KEY = None
DASHSCOPE_BASE_URL = None
EMBEDDING_MODEL = None
EMBEDDING_DIMENSION = None
BATCH_SIZE = None
PAPER_DB_CONFIG = None
USER_DB_CONFIG = None
PAPER_EMBEDDING_TABLE = None
USER_EMBEDDING_TABLE = None
MAX_PAPERS = None
MAX_USERS = None
DELAY_BETWEEN_BATCHES = None
SKIP_EXISTING = None


def load_all_config(config_path: str = None):
    """åŠ è½½æ‰€æœ‰é…ç½®åˆ°å…¨å±€å˜é‡"""
    global DASHSCOPE_API_KEY, DASHSCOPE_BASE_URL, EMBEDDING_MODEL, EMBEDDING_DIMENSION, BATCH_SIZE
    global PAPER_DB_CONFIG, USER_DB_CONFIG
    global PAPER_EMBEDDING_TABLE, USER_EMBEDDING_TABLE
    global MAX_PAPERS, MAX_USERS, DELAY_BETWEEN_BATCHES, SKIP_EXISTING

    config = load_config(config_path)

    # é˜¿é‡Œäº‘ç™¾ç‚¼ Embedding é…ç½®
    dashscope_cfg = config.get("dashscope", {})
    DASHSCOPE_API_KEY = dashscope_cfg.get("api_key", os.getenv("DASHSCOPE_API_KEY", ""))
    DASHSCOPE_BASE_URL = dashscope_cfg.get("base_url", "https://dashscope.aliyuncs.com/compatible-mode/v1")
    EMBEDDING_MODEL = dashscope_cfg.get("embedding_model", "text-embedding-v4")
    EMBEDDING_DIMENSION = dashscope_cfg.get("embedding_dimension", 2048)
    BATCH_SIZE = dashscope_cfg.get("batch_size", 10)

    # é˜¿é‡Œäº‘ RDS æ•°æ®åº“é…ç½®
    aliyun_rds = config.get("aliyun_rds", {})
    db_host = aliyun_rds.get("db_host", "localhost")
    db_port = int(aliyun_rds.get("db_port", 5432))
    db_user = aliyun_rds.get("db_user", "postgres")
    db_password = aliyun_rds.get("db_password", "")

    PAPER_DB_CONFIG = {
        "host": db_host,
        "port": db_port,
        "database": aliyun_rds.get("db_name_paper", "paperignition"),
        "user": db_user,
        "password": db_password
    }

    USER_DB_CONFIG = {
        "host": db_host,
        "port": db_port,
        "database": aliyun_rds.get("db_name_user", "paperignition_user"),
        "user": db_user,
        "password": db_password
    }

    # pgvector è¿ç§»é…ç½®
    pgvector_cfg = config.get("pgvector_migration", {})
    PAPER_EMBEDDING_TABLE = pgvector_cfg.get("paper_embedding_table", "paper_embeddings")
    USER_EMBEDDING_TABLE = pgvector_cfg.get("user_embedding_table", "user_interest_embeddings")
    MAX_PAPERS = pgvector_cfg.get("max_papers")
    MAX_USERS = pgvector_cfg.get("max_users")
    DELAY_BETWEEN_BATCHES = pgvector_cfg.get("delay_between_batches", 0.5)
    SKIP_EXISTING = pgvector_cfg.get("skip_existing", True)

    logger.info(f"âœ… é…ç½®åŠ è½½å®Œæˆ:")
    logger.info(f"   Embedding æ¨¡å‹: {EMBEDDING_MODEL} (ç»´åº¦: {EMBEDDING_DIMENSION})")
    logger.info(f"   Paper DB: {PAPER_DB_CONFIG['host']}/{PAPER_DB_CONFIG['database']}")
    logger.info(f"   User DB: {USER_DB_CONFIG['host']}/{USER_DB_CONFIG['database']}")


# ============================================
# æ•°æ®åº“è¿æ¥
# ============================================

def get_db_connection(config: Dict[str, Any]):
    """åˆ›å»ºæ•°æ®åº“è¿æ¥"""
    try:
        import psycopg2
        conn = psycopg2.connect(
            host=config["host"],
            port=config["port"],
            dbname=config["database"],
            user=config["user"],
            password=config["password"]
        )
        conn.autocommit = True
        return conn
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise


def ensure_pgvector_extension(conn):
    """ç¡®ä¿ pgvector æ‰©å±•å·²å®‰è£…"""
    cur = conn.cursor()
    try:
        cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
        logger.info("âœ… pgvector æ‰©å±•å·²å°±ç»ª")
    except Exception as e:
        logger.error(f"å®‰è£… pgvector æ‰©å±•å¤±è´¥: {e}")
        raise
    finally:
        cur.close()


def create_paper_embedding_table(conn):
    """åœ¨ paperignition æ•°æ®åº“åˆ›å»ºè®ºæ–‡å‘é‡è¡¨"""
    cur = conn.cursor()
    try:
        # åˆ›å»ºè®ºæ–‡ Abstract å‘é‡è¡¨
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {PAPER_EMBEDDING_TABLE} (
                id SERIAL PRIMARY KEY,
                doc_id VARCHAR(255) UNIQUE NOT NULL,
                title TEXT,
                abstract TEXT,
                embedding vector({EMBEDDING_DIMENSION}),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info(f"âœ… åˆ›å»º/ç¡®è®¤è¡¨: {PAPER_EMBEDDING_TABLE}")

        # åˆ›å»ºå‘é‡ç´¢å¼• (HNSW)
        try:
            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{PAPER_EMBEDDING_TABLE}_embedding
                ON {PAPER_EMBEDDING_TABLE}
                USING hnsw (embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64);
            """)
            logger.info(f"âœ… åˆ›å»ºç´¢å¼•: idx_{PAPER_EMBEDDING_TABLE}_embedding")
        except Exception as e:
            logger.warning(f"åˆ›å»ºè®ºæ–‡å‘é‡ç´¢å¼•å¤±è´¥ (å¯èƒ½æ•°æ®é‡ä¸è¶³): {e}")

    except Exception as e:
        logger.error(f"åˆ›å»ºè®ºæ–‡å‘é‡è¡¨å¤±è´¥: {e}")
        raise
    finally:
        cur.close()


def create_user_embedding_table(conn):
    """åœ¨ paperignition_user æ•°æ®åº“åˆ›å»ºç”¨æˆ·å‘é‡è¡¨"""
    cur = conn.cursor()
    try:
        # åˆ›å»ºç”¨æˆ· Interest å‘é‡è¡¨
        # user_id ä½œä¸ºå¤–é”®å…³è”åˆ°åŒæ•°æ®åº“çš„ users è¡¨
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {USER_EMBEDDING_TABLE} (
                id SERIAL PRIMARY KEY,
                user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
                username VARCHAR(255) UNIQUE NOT NULL,
                interest_text TEXT,
                embedding vector({EMBEDDING_DIMENSION}),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        """)
        logger.info(f"âœ… åˆ›å»º/ç¡®è®¤è¡¨: {USER_EMBEDDING_TABLE}")

        # åˆ›å»ºå‘é‡ç´¢å¼• (HNSW)
        try:
            cur.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{USER_EMBEDDING_TABLE}_embedding
                ON {USER_EMBEDDING_TABLE}
                USING hnsw (embedding vector_cosine_ops)
                WITH (m = 16, ef_construction = 64);
            """)
            logger.info(f"âœ… åˆ›å»ºç´¢å¼•: idx_{USER_EMBEDDING_TABLE}_embedding")
        except Exception as e:
            logger.warning(f"åˆ›å»ºç”¨æˆ·å‘é‡ç´¢å¼•å¤±è´¥ (å¯èƒ½æ•°æ®é‡ä¸è¶³): {e}")

    except Exception as e:
        logger.error(f"åˆ›å»ºç”¨æˆ·å‘é‡è¡¨å¤±è´¥: {e}")
        raise
    finally:
        cur.close()


# ============================================
# Embedding API è°ƒç”¨
# ============================================

class EmbeddingClient:
    """é˜¿é‡Œäº‘ç™¾ç‚¼ Embedding å®¢æˆ·ç«¯"""

    def __init__(self, api_key: str, base_url: str, model: str, dimension: int):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.dimension = dimension

        try:
            from openai import OpenAI
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
            logger.info(f"âœ… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ (base_url: {base_url})")
        except ImportError:
            raise ImportError("è¯·å®‰è£… openai: pip install openai")

    def get_embeddings(self, texts: List[str]) -> Optional[List[List[float]]]:
        """æ‰¹é‡è·å–æ–‡æœ¬çš„ embedding"""
        if not texts:
            return None

        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimension
            )
            embeddings = [item.embedding for item in response.data]
            return embeddings
        except Exception as e:
            logger.error(f"Embedding API è°ƒç”¨å¤±è´¥: {e}")
            return None

    def get_embedding(self, text: str) -> Optional[List[float]]:
        """è·å–å•ä¸ªæ–‡æœ¬çš„ embedding"""
        embeddings = self.get_embeddings([text])
        return embeddings[0] if embeddings else None


def batch_process_embeddings(
    embedding_client: EmbeddingClient,
    texts: List[str],
    batch_size: int = BATCH_SIZE,
    delay: float = DELAY_BETWEEN_BATCHES
) -> List[Optional[List[float]]]:
    """æ‰¹é‡å¤„ç†æ–‡æœ¬çš„ embeddingï¼Œå¤„ç† API é™åˆ¶"""
    all_embeddings = []

    total_batches = (len(texts) + batch_size - 1) // batch_size
    logger.info(f"å¼€å§‹å¤„ç† {len(texts)} æ¡æ–‡æœ¬ï¼Œå…± {total_batches} æ‰¹æ¬¡")

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_num = i // batch_size + 1

        logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¡æ–‡æœ¬)")

        embeddings = embedding_client.get_embeddings(batch)

        if embeddings:
            all_embeddings.extend(embeddings)
        else:
            # å¦‚æœå¤±è´¥ï¼Œå¡«å…… None
            all_embeddings.extend([None] * len(batch))
            logger.warning(f"æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥")

        # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å… API é™æµ
        if i + batch_size < len(texts):
            time.sleep(delay)

    return all_embeddings


# ============================================
# æ•°æ®è¯»å–
# ============================================

def fetch_paper_abstracts(conn, target_conn=None, limit: Optional[int] = None, skip_existing: bool = True) -> List[Dict[str, Any]]:
    """ä»æ•°æ®åº“è¯»å–è®ºæ–‡ Abstract

    Args:
        conn: æºæ•°æ®åº“è¿æ¥ (paperignition)
        target_conn: ç›®æ ‡æ•°æ®åº“è¿æ¥ (ç”¨äºæ£€æŸ¥å·²å­˜åœ¨çš„è®°å½•)
        limit: é™åˆ¶è¯»å–æ•°é‡
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„è®°å½•
    """
    cur = conn.cursor()
    try:
        # è·å–å·²å­˜åœ¨çš„ doc_id åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        existing_doc_ids = set()
        if skip_existing and target_conn:
            target_cur = target_conn.cursor()
            target_cur.execute(f"SELECT doc_id FROM {PAPER_EMBEDDING_TABLE}")
            existing_doc_ids = {row[0] for row in target_cur.fetchall()}
            target_cur.close()
            logger.info(f"ğŸ“‹ å·²å­˜åœ¨ {len(existing_doc_ids)} æ¡è®ºæ–‡ embedding è®°å½•")

        limit_clause = f"LIMIT {limit * 3 if limit else ''}" if limit else ""  # å¤šå–ä¸€äº›ï¼Œå› ä¸ºä¼šè¿‡æ»¤æ‰å·²å­˜åœ¨çš„

        # æŸ¥è¯¢ papers è¡¨
        cur.execute(f"""
            SELECT doc_id, title, abstract
            FROM papers
            WHERE abstract IS NOT NULL AND abstract != ''
            {limit_clause}
        """)

        papers = []
        for row in cur.fetchall():
            doc_id, title, abstract = row

            # å»é‡ï¼šè·³è¿‡å·²å­˜åœ¨çš„
            if skip_existing and doc_id in existing_doc_ids:
                continue

            if abstract and abstract.strip():
                papers.append({
                    "doc_id": doc_id,
                    "title": title or "",
                    "abstract": abstract.strip()
                })

            # è¾¾åˆ°é™åˆ¶æ•°é‡å°±åœæ­¢
            if limit and len(papers) >= limit:
                break

        logger.info(f"ğŸ“š è¯»å–åˆ° {len(papers)} ç¯‡æ–°è®ºæ–‡ (å¾…å¤„ç†)")
        return papers

    except Exception as e:
        logger.error(f"è¯»å–è®ºæ–‡æ•°æ®å¤±è´¥: {e}")
        return []
    finally:
        cur.close()


def fetch_user_interests(conn, skip_existing: bool = True) -> List[Dict[str, Any]]:
    """ä»æ•°æ®åº“è¯»å–ç”¨æˆ· Interest

    Args:
        conn: æ•°æ®åº“è¿æ¥ (paperignition_userï¼ŒåŒæ—¶ç”¨äºè¯»å–å’Œå»é‡æ£€æŸ¥)
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„è®°å½•
    """
    cur = conn.cursor()
    try:
        # è·å–å·²å­˜åœ¨çš„ username åˆ—è¡¨ï¼ˆå»é‡ï¼‰- ç°åœ¨åœ¨åŒä¸€ä¸ªæ•°æ®åº“
        existing_usernames = set()
        if skip_existing:
            cur.execute(f"SELECT username FROM {USER_EMBEDDING_TABLE}")
            existing_usernames = {row[0] for row in cur.fetchall()}
            logger.info(f"ğŸ“‹ å·²å­˜åœ¨ {len(existing_usernames)} æ¡ç”¨æˆ· embedding è®°å½•")

        # æŸ¥è¯¢ users è¡¨çš„ interests_description
        # interests_description æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œéœ€è¦å±•å¼€å¤„ç†
        cur.execute(f"""
            SELECT id, username, interests_description, rewrite_interest
            FROM users
            WHERE interests_description IS NOT NULL
            AND array_length(interests_description, 1) > 0
        """)

        users = []
        for row in cur.fetchall():
            user_id, username, interests_description, rewrite_interest = row

            # å»é‡ï¼šè·³è¿‡å·²å­˜åœ¨çš„
            if skip_existing and username in existing_usernames:
                continue

            # ä¼˜å…ˆä½¿ç”¨ rewrite_interest (ç¿»è¯‘åçš„è‹±æ–‡ç‰ˆæœ¬)
            if rewrite_interest and rewrite_interest.strip():
                interest_text = rewrite_interest.strip()
            elif interests_description:
                # interests_description æ˜¯æ•°ç»„ï¼Œåˆå¹¶ä¸ºå­—ç¬¦ä¸²
                interest_text = " ".join([i for i in interests_description if i])
            else:
                continue

            if interest_text.strip():
                users.append({
                    "user_id": user_id,
                    "username": username,
                    "interest_text": interest_text.strip()
                })

        logger.info(f"ğŸ‘¥ è¯»å–åˆ° {len(users)} ä¸ªæ–°ç”¨æˆ· (å¾…å¤„ç†)")
        return users

    except Exception as e:
        logger.error(f"è¯»å–ç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
        return []
    finally:
        cur.close()


# ============================================
# æ•°æ®å­˜å‚¨
# ============================================

def insert_paper_embeddings(conn, papers: List[Dict[str, Any]], embeddings: List[Optional[List[float]]]):
    """å°†è®ºæ–‡ embedding æ’å…¥æ•°æ®åº“"""
    cur = conn.cursor()
    success_count = 0
    error_count = 0

    try:
        for paper, embedding in zip(papers, embeddings):
            if embedding is None:
                logger.warning(f"è·³è¿‡è®ºæ–‡ {paper['doc_id']} (embedding ä¸ºç©º)")
                error_count += 1
                continue

            try:
                # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                emb_str = json.dumps(embedding)

                # ä½¿ç”¨ UPSERT (INSERT ... ON CONFLICT)
                cur.execute(f"""
                    INSERT INTO {PAPER_EMBEDDING_TABLE} (doc_id, title, abstract, embedding, updated_at)
                    VALUES (%s, %s, %s, %s::vector, CURRENT_TIMESTAMP)
                    ON CONFLICT (doc_id)
                    DO UPDATE SET
                        title = EXCLUDED.title,
                        abstract = EXCLUDED.abstract,
                        embedding = EXCLUDED.embedding,
                        updated_at = CURRENT_TIMESTAMP
                """, (paper["doc_id"], paper["title"], paper["abstract"], emb_str))

                success_count += 1

            except Exception as e:
                logger.error(f"æ’å…¥è®ºæ–‡ {paper['doc_id']} å¤±è´¥: {e}")
                error_count += 1

        logger.info(f"ğŸ“„ è®ºæ–‡ embedding æ’å…¥å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")

    finally:
        cur.close()

    return success_count, error_count


def insert_user_embeddings(conn, users: List[Dict[str, Any]], embeddings: List[Optional[List[float]]]):
    """å°†ç”¨æˆ· embedding æ’å…¥æ•°æ®åº“"""
    cur = conn.cursor()
    success_count = 0
    error_count = 0

    try:
        for user, embedding in zip(users, embeddings):
            if embedding is None:
                logger.warning(f"è·³è¿‡ç”¨æˆ· {user['username']} (embedding ä¸ºç©º)")
                error_count += 1
                continue

            try:
                # è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                emb_str = json.dumps(embedding)

                # ä½¿ç”¨ UPSERT (INSERT ... ON CONFLICT)
                cur.execute(f"""
                    INSERT INTO {USER_EMBEDDING_TABLE} (user_id, username, interest_text, embedding, updated_at)
                    VALUES (%s, %s, %s, %s::vector, CURRENT_TIMESTAMP)
                    ON CONFLICT (username)
                    DO UPDATE SET
                        user_id = EXCLUDED.user_id,
                        interest_text = EXCLUDED.interest_text,
                        embedding = EXCLUDED.embedding,
                        updated_at = CURRENT_TIMESTAMP
                """, (user["user_id"], user["username"], user["interest_text"], emb_str))

                success_count += 1

            except Exception as e:
                logger.error(f"æ’å…¥ç”¨æˆ· {user['username']} å¤±è´¥: {e}")
                error_count += 1

        logger.info(f"ğŸ‘¥ ç”¨æˆ· embedding æ’å…¥å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {error_count}")

    finally:
        cur.close()

    return success_count, error_count


# ============================================
# ç›¸ä¼¼åº¦æ£€ç´¢å‡½æ•°
# ============================================

def search_similar_papers(conn, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:
    """æœç´¢ç›¸ä¼¼çš„è®ºæ–‡"""
    cur = conn.cursor()
    try:
        emb_str = json.dumps(query_embedding)

        cur.execute(f"""
            SELECT doc_id, title, abstract,
                   1 - (embedding <=> %s::vector) as similarity
            FROM {PAPER_EMBEDDING_TABLE}
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (emb_str, emb_str, top_k))

        results = []
        for row in cur.fetchall():
            results.append({
                "doc_id": row[0],
                "title": row[1],
                "abstract": row[2],
                "similarity": float(row[3])
            })

        return results

    finally:
        cur.close()


def search_similar_users(conn, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:
    """æœç´¢å…´è¶£ç›¸ä¼¼çš„ç”¨æˆ·"""
    cur = conn.cursor()
    try:
        emb_str = json.dumps(query_embedding)

        cur.execute(f"""
            SELECT user_id, username, interest_text,
                   1 - (embedding <=> %s::vector) as similarity
            FROM {USER_EMBEDDING_TABLE}
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """, (emb_str, emb_str, top_k))

        results = []
        for row in cur.fetchall():
            results.append({
                "user_id": row[0],
                "username": row[1],
                "interest_text": row[2],
                "similarity": float(row[3])
            })

        return results

    finally:
        cur.close()


# ============================================
# ä¸»æµç¨‹
# ============================================

def main(config_path: str = None):
    """ä¸»è¿ç§»æµç¨‹"""
    # åŠ è½½é…ç½®
    load_all_config(config_path)

    logger.info("=" * 60)
    logger.info("å¼€å§‹ Embedding è¿ç§»åˆ° pgvector")
    logger.info("=" * 60)

    start_time = datetime.now()

    # 1. åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯
    logger.info("\nğŸ“Œ æ­¥éª¤ 1: åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯")
    embedding_client = EmbeddingClient(
        api_key=DASHSCOPE_API_KEY,
        base_url=DASHSCOPE_BASE_URL,
        model=EMBEDDING_MODEL,
        dimension=EMBEDDING_DIMENSION
    )

    # 2. è¿æ¥æ•°æ®åº“å¹¶åˆ›å»ºè¡¨
    logger.info("\nğŸ“Œ æ­¥éª¤ 2: è¿æ¥æ•°æ®åº“å¹¶åˆ›å»ºè¡¨")

    # è¿æ¥åˆ° Paper æ•°æ®åº“ (é˜¿é‡Œäº‘ RDS)
    paper_conn = get_db_connection(PAPER_DB_CONFIG)
    logger.info(f"âœ… è¿æ¥åˆ° Paper æ•°æ®åº“: {PAPER_DB_CONFIG['database']} ({PAPER_DB_CONFIG['host']})")
    ensure_pgvector_extension(paper_conn)
    create_paper_embedding_table(paper_conn)

    # è¿æ¥åˆ° User æ•°æ®åº“ (æœ¬åœ°)
    user_conn = get_db_connection(USER_DB_CONFIG)
    logger.info(f"âœ… è¿æ¥åˆ° User æ•°æ®åº“: {USER_DB_CONFIG['database']} ({USER_DB_CONFIG['host']})")
    ensure_pgvector_extension(user_conn)
    create_user_embedding_table(user_conn)

    # 3. å¤„ç†è®ºæ–‡ Abstract
    logger.info("\nğŸ“Œ æ­¥éª¤ 3: å¤„ç†è®ºæ–‡ Abstract Embedding")
    papers = fetch_paper_abstracts(
        paper_conn,
        target_conn=paper_conn,
        limit=MAX_PAPERS,
        skip_existing=SKIP_EXISTING
    )

    if papers:
        # å‡†å¤‡ embedding æ–‡æœ¬ (title + abstract)
        paper_texts = []
        for paper in papers:
            # ç»„åˆ title å’Œ abstract è¿›è¡Œ embedding
            text = f"{paper['title']}. {paper['abstract']}" if paper['title'] else paper['abstract']
            paper_texts.append(text)

        # æ‰¹é‡è·å– embedding
        paper_embeddings = batch_process_embeddings(embedding_client, paper_texts)

        # æ’å…¥åˆ° Paper æ•°æ®åº“
        insert_paper_embeddings(paper_conn, papers, paper_embeddings)
    else:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡æ•°æ®")

    # 4. å¤„ç†ç”¨æˆ· Interest
    logger.info("\nğŸ“Œ æ­¥éª¤ 4: å¤„ç†ç”¨æˆ· Interest Embedding")
    users = fetch_user_interests(user_conn, skip_existing=SKIP_EXISTING)

    if users:
        # å‡†å¤‡ embedding æ–‡æœ¬
        user_texts = [user["interest_text"] for user in users]

        # æ‰¹é‡è·å– embedding
        user_embeddings = batch_process_embeddings(embedding_client, user_texts)

        # æ’å…¥åˆ° User æ•°æ®åº“ (paperignition_user)
        insert_user_embeddings(user_conn, users, user_embeddings)
    else:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ•°æ®")

    # 5. éªŒè¯ç»“æœ
    logger.info("\nğŸ“Œ æ­¥éª¤ 5: éªŒè¯è¿ç§»ç»“æœ")

    # æ£€æŸ¥è®ºæ–‡ embedding æ•°é‡
    cur = paper_conn.cursor()
    cur.execute(f"SELECT COUNT(*) FROM {PAPER_EMBEDDING_TABLE}")
    paper_count = cur.fetchone()[0]
    cur.close()
    logger.info(f"ğŸ“Š Paper Embedding è®°å½•æ•° (paperignition): {paper_count}")

    # æ£€æŸ¥ç”¨æˆ· embedding æ•°é‡
    cur = user_conn.cursor()
    cur.execute(f"SELECT COUNT(*) FROM {USER_EMBEDDING_TABLE}")
    user_count = cur.fetchone()[0]
    cur.close()
    logger.info(f"ğŸ“Š User Embedding è®°å½•æ•° (paperignition_user): {user_count}")

    # 6. æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
    logger.info("\nğŸ“Œ æ­¥éª¤ 6: æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢")

    # æµ‹è¯•æœç´¢ï¼šç”¨ç¬¬ä¸€ä¸ªç”¨æˆ·çš„å…´è¶£æœç´¢ç›¸ä¼¼è®ºæ–‡
    if users and papers:
        test_user = users[0]
        logger.info(f"ğŸ” ä½¿ç”¨ç”¨æˆ· '{test_user['username']}' çš„å…´è¶£æµ‹è¯•æœç´¢")

        # è·å–è¯¥ç”¨æˆ·çš„ embedding (ä» user_conn / paperignition_user æ•°æ®åº“)
        cur = user_conn.cursor()
        cur.execute(f"""
            SELECT embedding FROM {USER_EMBEDDING_TABLE}
            WHERE username = %s
        """, (test_user['username'],))
        result = cur.fetchone()
        cur.close()

        if result:
            user_emb = json.loads(result[0])
            # æœç´¢ç›¸ä¼¼è®ºæ–‡ (åœ¨ paper_conn / paperignition æ•°æ®åº“ä¸­)
            similar_papers = search_similar_papers(paper_conn, user_emb, top_k=3)

            logger.info(f"   ç”¨æˆ·å…´è¶£: {test_user['interest_text'][:100]}...")
            logger.info("   ç›¸ä¼¼è®ºæ–‡:")
            for i, paper in enumerate(similar_papers, 1):
                logger.info(f"   {i}. [{paper['similarity']:.4f}] {paper['title'][:60]}...")

    # æ¸…ç†è¿æ¥
    paper_conn.close()
    user_conn.close()

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    logger.info("\n" + "=" * 60)
    logger.info(f"âœ… è¿ç§»å®Œæˆ!")
    logger.info(f"   æ€»è€—æ—¶: {duration:.2f} ç§’")
    logger.info(f"   è®ºæ–‡ Embedding (paperignition): {paper_count} æ¡")
    logger.info(f"   ç”¨æˆ· Embedding (paperignition_user): {user_count} æ¡")
    logger.info("=" * 60)


def test_search(config_path: str = None):
    """æµ‹è¯•å‘é‡æœç´¢åŠŸèƒ½"""
    # åŠ è½½é…ç½®
    load_all_config(config_path)

    logger.info("\n" + "=" * 60)
    logger.info("æµ‹è¯•å‘é‡æœç´¢åŠŸèƒ½")
    logger.info("=" * 60)

    # åˆå§‹åŒ–
    embedding_client = EmbeddingClient(
        api_key=DASHSCOPE_API_KEY,
        base_url=DASHSCOPE_BASE_URL,
        model=EMBEDDING_MODEL,
        dimension=EMBEDDING_DIMENSION
    )

    paper_conn = get_db_connection(PAPER_DB_CONFIG)

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "æœºå™¨å­¦ä¹ ",
        "è‡ªç„¶è¯­è¨€å¤„ç†",
        "è®¡ç®—æœºè§†è§‰"
    ]

    for query in test_queries:
        logger.info(f"\nğŸ” æŸ¥è¯¢: {query}")

        # è·å–æŸ¥è¯¢çš„ embedding
        query_embedding = embedding_client.get_embedding(query)

        if query_embedding:
            # æœç´¢ç›¸ä¼¼è®ºæ–‡
            similar_papers = search_similar_papers(paper_conn, query_embedding, top_k=3)

            logger.info("   ç›¸ä¼¼è®ºæ–‡:")
            for i, paper in enumerate(similar_papers, 1):
                logger.info(f"   {i}. [{paper['similarity']:.4f}] {paper['title'][:60]}...")

    paper_conn.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="è¿ç§» Embedding åˆ° pgvector")
    parser.add_argument("--config", type=str, default=None,
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: scripts/migration_config.yaml)")
    parser.add_argument("--test", action="store_true", help="åªè¿è¡Œæœç´¢æµ‹è¯•")
    parser.add_argument("--max-papers", type=int, default=None, help="æœ€å¤§å¤„ç†è®ºæ–‡æ•°é‡")
    parser.add_argument("--max-users", type=int, default=None, help="æœ€å¤§å¤„ç†ç”¨æˆ·æ•°é‡")

    args = parser.parse_args()

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.max_papers is not None:
        MAX_PAPERS = args.max_papers
    if args.max_users is not None:
        MAX_USERS = args.max_users

    if args.test:
        test_search(args.config)
    else:
        main(args.config)
