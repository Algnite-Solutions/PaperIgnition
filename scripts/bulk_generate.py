#!/usr/bin/env python3
"""
åŸºäºæ•°æ®åº“çŠ¶æ€æ‰¹é‡æ›´æ–°è®ºæ–‡åšå®¢è„šæœ¬

é€»è¾‘å˜æ›´ï¼š
1. è¿æ¥ Metadata æ•°æ®åº“
2. æŸ¥è¯¢ blog å­—æ®µä¸ºç©ºçš„ paper_id
3. æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨å¯¹åº”çš„ .md æ–‡ä»¶
4. è¯»å–æ–‡ä»¶å¹¶è°ƒç”¨ API æ›´æ–°

ä½¿ç”¨æ–¹æ³•ï¼š
    python batch_update_blogs_db_driven.py
"""

import os
import sys
import asyncio
import httpx
import time
import asyncpg
from pathlib import Path
from typing import List, Dict, Optional, Set
import logging
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "backend"))

from backend.index_service.db_utils import load_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('batch_update_blogs.log')
    ]
)

# ç¦ç”¨ç¬¬ä¸‰æ–¹åº“çš„ DEBUG æ—¥å¿—
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('httpcore').setLevel(logging.WARNING)
logging.getLogger('asyncio').setLevel(logging.WARNING)
logging.getLogger('asyncpg').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


class BatchBlogUpdater:
    def __init__(self, api_url: str = None, config_path: str = None, batch_size: int = 50):
        # --- é…ç½®åŠ è½½é€»è¾‘ ---
        if config_path:
            try:
                self.config = load_config(config_path)
                logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                
                # è·å– API URL
                if not api_url:
                    api_url = self.config.get('index_service', {}).get('host', "http://10.0.1.226:8002")
                
                # è·å–æ•°æ®åº“è¿æ¥ URL
                self.db_url = self.config.get('index_service', {}).get('metadata_db', {}).get('db_url')
                if not self.db_url:
                    raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ index_service.metadata_db.db_url")
                    
            except Exception as e:
                logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼")
                self.config = load_config(config_path)
                api_url = "http://10.0.1.226:8002"
                self.db_url = "postgresql://postgres:11111@localhost:5432/paperignition"

        else:
            logger.error("âŒ å¿…é¡»æä¾›é…ç½®æ–‡ä»¶è·¯å¾„")
            sys.exit(1)

        self.api_url = api_url.rstrip('/')
        self.batch_size = batch_size
        
        # åšå®¢æ–‡ä»¶ç›®å½• (ä»é…ç½®ä¸­è¯»å–ï¼Œå¦‚æœé…ç½®æ²¡æœ‰åˆ™ä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼)
        config_blog_path = self.config.get('blog_generation', {}).get('output_path')
        self.blogs_dir = Path(config_blog_path) if config_blog_path else Path("/data3/guofang/peirongcan/PaperIgnition/orchestrator/blogs")
        
        self.update_blogs_endpoint = f"{self.api_url}/update_papers_blog/"
        self.health_endpoint = f"{self.api_url}/health"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_target_papers = 0
        self.found_local_files = 0
        self.missing_local_files = 0
        self.successful_updates = 0
        self.failed_updates = 0
        self.results = []

        logger.info(f"ğŸ”§ API URL: {self.api_url}")
        logger.info(f"ğŸ—„ï¸  DB URL: {self.db_url.split('@')[-1]}") # éšè—å¯†ç åªæ˜¾ç¤ºä¸»æœº
        logger.info(f"ğŸ“ æœ¬åœ°åšå®¢ç›®å½•: {self.blogs_dir}")

    async def check_server_health(self) -> bool:
        """æ£€æŸ¥APIæœåŠ¡å™¨æ˜¯å¦è¿è¡Œ"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(self.health_endpoint)
                if response.status_code == 200:
                    data = response.json()
                    return data.get("indexer_ready", False)
                return False
        except Exception as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ {self.api_url}: {e}")
            return False

    async def fetch_missing_blog_ids(self) -> List[str]:
        """
        è¿æ¥æ•°æ®åº“ï¼ŒæŸ¥è¯¢æ‰€æœ‰ blog å­—æ®µä¸ºç©ºæˆ–NULLçš„ paper_id
        """
        logger.info("ğŸ” æ­£åœ¨è¿æ¥æ•°æ®åº“æŸ¥è¯¢ç¼ºå¤±åšå®¢çš„è®ºæ–‡...")
        conn = None
        try:
            conn = await asyncpg.connect(self.db_url)
            
            # æŸ¥è¯¢ blog ä¸º NULL æˆ– ç©ºå­—ç¬¦ä¸² æˆ– åªæœ‰ç©ºç™½å­—ç¬¦ çš„è®°å½•
            query = """
                SELECT doc_id 
                FROM papers 
                WHERE blog IS NULL 
                   OR trim(blog) = ''
            """
            
            rows = await conn.fetch(query)
            paper_ids = [row['doc_id'] for row in rows]
            
            logger.info(f"ğŸ“‹ æ•°æ®åº“ä¸­å…±æœ‰ {len(paper_ids)} ç¯‡è®ºæ–‡ç¼ºå°‘åšå®¢å†…å®¹")
            return paper_ids
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
        finally:
            if conn:
                await conn.close()

    def prepare_update_data(self, target_paper_ids: List[str]) -> List[Dict[str, str]]:
        """
        æ ¹æ®ç›®æ ‡IDåˆ—è¡¨ï¼Œå»æœ¬åœ°æŸ¥æ‰¾å¯¹åº”çš„æ–‡ä»¶
        """
        papers_data = []
        self.total_target_papers = len(target_paper_ids)
        
        if not self.blogs_dir.exists():
            logger.error(f"âŒ æœ¬åœ°åšå®¢ç›®å½•ä¸å­˜åœ¨: {self.blogs_dir}")
            return []

        logger.info("ğŸ“‚ å¼€å§‹åŒ¹é…æœ¬åœ°æ–‡ä»¶...")
        
        # ä¸ºäº†æé«˜æ•ˆç‡ï¼Œå…ˆè·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶çš„é›†åˆ
        local_files_map = {f.stem: f for f in self.blogs_dir.glob("*.md")}
        
        with tqdm(total=len(target_paper_ids), desc="ğŸ“– åŒ¹é…å¹¶è¯»å–", unit="ç¯‡", ncols=100) as pbar:
            for paper_id in target_paper_ids:
                # å°è¯•å¤šç§æ–‡ä»¶ååŒ¹é… (æœ‰æ—¶å€™æ–‡ä»¶åå¯èƒ½æœ‰ç‰ˆæœ¬å·å·®å¼‚ï¼Œè¿™é‡Œå‡è®¾ä¸¥æ ¼åŒ¹é…æˆ–åŸºæœ¬åŒ¹é…)
                # ä¼˜å…ˆç›´æ¥åŒ¹é… paper_id.md
                md_file = local_files_map.get(paper_id)
                
                if md_file and md_file.exists():
                    try:
                        with open(md_file, 'r', encoding='utf-8') as f:
                            content = f.read().strip()
                        
                        if content:
                            papers_data.append({
                                "paper_id": paper_id,
                                "blog_content": content
                            })
                            self.found_local_files += 1
                        else:
                            # æ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©º
                            pass
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å‡ºé”™ {md_file}: {e}")
                else:
                    self.missing_local_files += 1
                
                pbar.update(1)

        logger.info(f"âœ… åŒ¹é…å®Œæˆ: éœ€æ›´æ–° {len(target_paper_ids)} ç¯‡ -> æ‰¾åˆ°æœ¬åœ°æ–‡ä»¶ {len(papers_data)} ç¯‡")
        if self.missing_local_files > 0:
            logger.warning(f"âš ï¸  æœ‰ {self.missing_local_files} ç¯‡è®ºæ–‡åœ¨æ•°æ®åº“ä¸­ç¼ºå°‘åšå®¢ï¼Œä¸”æœ¬åœ°æœªæ‰¾åˆ°å¯¹åº”æ–‡ä»¶")
            
        return papers_data

    async def update_blogs_batch(self, client: httpx.AsyncClient, papers_data: List[Dict[str, str]]) -> Dict:
        """è°ƒç”¨APIæ‰¹é‡æ›´æ–°åšå®¢ (é€»è¾‘ä¿æŒä¸å˜)"""
        request_data = {"papers": papers_data}
        try:
            response = await client.put(
                self.update_blogs_endpoint,
                json=request_data,
                timeout=120.0
            )
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "status": "success",
                    "updated_count": result.get("updated_count", 0),
                    "total_requested": len(papers_data),
                    "message": result.get("message", "æ›´æ–°æˆåŠŸ")
                }
            else:
                return {
                    "status": "failed",
                    "updated_count": 0,
                    "total_requested": len(papers_data),
                    "message": f"HTTP {response.status_code}: {response.text}"
                }
        except Exception as e:
            return {
                "status": "failed",
                "updated_count": 0,
                "total_requested": len(papers_data),
                "message": str(e)
            }

    async def run(self, dry_run: bool = False):
        """ä¸»æµç¨‹"""
        # 1. æ£€æŸ¥ API å¥åº·
        if not await self.check_server_health():
            logger.error("âŒ æœåŠ¡å™¨æœªå°±ç»ªï¼Œç»ˆæ­¢æ“ä½œ")
            return

        # 2. ä»æ•°æ®åº“è·å–ç›®æ ‡åˆ—è¡¨
        target_ids = await self.fetch_missing_blog_ids()
        if not target_ids:
            logger.info("âœ¨ æ•°æ®åº“ä¸­æ‰€æœ‰è®ºæ–‡å‡å·²æœ‰åšå®¢ï¼Œæ— éœ€æ›´æ–°")
            return

        # 3. å‡†å¤‡æ•°æ®ï¼ˆè¯»å–æœ¬åœ°æ–‡ä»¶ï¼‰
        papers_to_update = self.prepare_update_data(target_ids)
        if not papers_to_update:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å¯æ›´æ–°çš„æœ¬åœ°åšå®¢æ–‡ä»¶")
            return

        if dry_run:
            logger.info("ğŸ” Dry Run æ¨¡å¼ç»“æŸï¼Œä¸å‘é€è¯·æ±‚")
            return

        # 4. æ‰¹é‡æ›´æ–°
        logger.info(f"ğŸš€ å¼€å§‹é€šè¿‡ API æ›´æ–° {len(papers_to_update)} ç¯‡è®ºæ–‡...")
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            total_batches = (len(papers_to_update) + self.batch_size - 1) // self.batch_size
            
            with tqdm(total=total_batches, desc="ğŸ’¾ æäº¤æ›´æ–°", unit="æ‰¹æ¬¡") as pbar:
                for i in range(0, len(papers_to_update), self.batch_size):
                    batch = papers_to_update[i : i + self.batch_size]
                    result = await self.update_blogs_batch(client, batch)
                    
                    self.results.append(result)
                    if result["status"] == "success":
                        self.successful_updates += result["updated_count"]
                    else:
                        self.failed_updates += result["total_requested"]
                        logger.error(f"æ‰¹æ¬¡å¤±è´¥: {result['message']}")
                    
                    pbar.update(1)
                    await asyncio.sleep(0.2)

        self.print_summary()

    def print_summary(self):
        print("\n" + "="*60)
        print("ğŸ“Š æ›´æ–°ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"ğŸ“‹ DBä¸­ç¼ºå°‘åšå®¢æ€»æ•°: {self.total_target_papers}")
        print(f"ğŸ“‚ æœ¬åœ°æ‰¾åˆ°å¯¹åº”æ–‡ä»¶: {self.found_local_files}")
        print(f"ğŸ‘» æœ¬åœ°ç¼ºå¤±å¯¹åº”æ–‡ä»¶: {self.missing_local_files}")
        print("-" * 30)
        print(f"âœ… æˆåŠŸå†™å…¥æ•°æ®åº“:   {self.successful_updates}")
        print(f"âŒ å†™å…¥å¤±è´¥:         {self.failed_updates}")
        print("="*60)


async def main():
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = "/data3/guofang/peirongcan/PaperIgnition/orchestrator/production_config.yaml"
    
    try:
        updater = BatchBlogUpdater(
            config_path=config_path,
            batch_size=50
        )
        await updater.run(dry_run= True)
        
    except KeyboardInterrupt:
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())